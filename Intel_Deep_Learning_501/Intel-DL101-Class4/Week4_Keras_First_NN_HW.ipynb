{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Keras to Build and Train Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we will use a neural network to predict diabetes using the Pima Diabetes Dataset.  We will start by training a Random Forest to get a performance baseline.  Then we will use the Keras package to quickly build and train a neural network and compare the performance.  We will see how different network structures affect the performance, training time, and level of overfitting (or underfitting).\n",
    "\n",
    "## UCI Pima Diabetes Dataset\n",
    "\n",
    "* UCI ML Repositiory (http://archive.ics.uci.edu/ml/datasets/Pima+Indians+Diabetes)\n",
    "\n",
    "\n",
    "### Attributes: (all numeric-valued)\n",
    "   1. Number of times pregnant\n",
    "   2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
    "   3. Diastolic blood pressure (mm Hg)\n",
    "   4. Triceps skin fold thickness (mm)\n",
    "   5. 2-Hour serum insulin (mu U/ml)\n",
    "   6. Body mass index (weight in kg/(height in m)^2)\n",
    "   7. Diabetes pedigree function\n",
    "   8. Age (years)\n",
    "   9. Class variable (0 or 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The UCI Pima Diabetes Dataset which has 8 numerical predictors and a binary outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preliminaries\n",
    "\n",
    "from __future__ import absolute_import, division, print_function  # Python 2/3 compatibility\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve, roc_auc_score, roc_curve, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "## Import Keras objects for Deep Learning\n",
    "\n",
    "from keras.models  import Sequential, K\n",
    "from keras.layers import Input, Dense, Flatten, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam, SGD, RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load in the data set (Internet Access needed)\n",
    "\n",
    "#url not working\n",
    "#url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data\"\n",
    "names = [\"times_pregnant\", \"glucose_tolerance_test\", \"blood_pressure\", \"skin_thickness\", \"insulin\", \n",
    "         \"bmi\", \"pedigree_function\", \"age\", \"has_diabetes\"]\n",
    "#diabetes_df = pd.read_csv(url, names=names)\n",
    "\n",
    "diabetes_df = pd.read_csv(\"pima-indians-diabetes.csv\",names=names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>times_pregnant</th>\n",
       "      <th>glucose_tolerance_test</th>\n",
       "      <th>blood_pressure</th>\n",
       "      <th>skin_thickness</th>\n",
       "      <th>insulin</th>\n",
       "      <th>bmi</th>\n",
       "      <th>pedigree_function</th>\n",
       "      <th>age</th>\n",
       "      <th>has_diabetes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>8</td>\n",
       "      <td>109</td>\n",
       "      <td>76</td>\n",
       "      <td>39</td>\n",
       "      <td>114</td>\n",
       "      <td>27.9</td>\n",
       "      <td>0.640</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586</th>\n",
       "      <td>8</td>\n",
       "      <td>143</td>\n",
       "      <td>66</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>34.9</td>\n",
       "      <td>0.129</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>683</th>\n",
       "      <td>4</td>\n",
       "      <td>125</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32.3</td>\n",
       "      <td>0.536</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>1</td>\n",
       "      <td>111</td>\n",
       "      <td>86</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.143</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>693</th>\n",
       "      <td>7</td>\n",
       "      <td>129</td>\n",
       "      <td>68</td>\n",
       "      <td>49</td>\n",
       "      <td>125</td>\n",
       "      <td>38.5</td>\n",
       "      <td>0.439</td>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     times_pregnant  glucose_tolerance_test  blood_pressure  skin_thickness  \\\n",
       "188               8                     109              76              39   \n",
       "586               8                     143              66               0   \n",
       "683               4                     125              80               0   \n",
       "249               1                     111              86              19   \n",
       "693               7                     129              68              49   \n",
       "\n",
       "     insulin   bmi  pedigree_function  age  has_diabetes  \n",
       "188      114  27.9              0.640   31             1  \n",
       "586        0  34.9              0.129   41             1  \n",
       "683        0  32.3              0.536   27             1  \n",
       "249        0  30.1              0.143   23             0  \n",
       "693      125  38.5              0.439   43             1  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a peek at the data -- if there are lots of \"NaN\" you may have internet connectivity issues\n",
    "print(diabetes_df.shape)\n",
    "diabetes_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = diabetes_df.iloc[:, :-1].values\n",
    "y = diabetes_df[\"has_diabetes\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data to Train, and Test (75%, 25%)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=11111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3489583333333333, 0.6510416666666666)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y), np.mean(1-y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we see that about 35% of the patients in this dataset have diabetes, while 65% do not.  This means we can get an accuracy of 65% without any model - just declare that no one has diabetes. We will calculate the ROC-AUC score to evaluate performance of our model, and also look at the accuracy as well to see if we improved upon the 65% accuracy.\n",
    "## Exercise: Get a baseline performance using Random Forest\n",
    "To begin, and get a baseline for classifier performance:\n",
    "1. Train a Random Forest model with 200 trees on the training data.\n",
    "2. Calculate the accuracy and roc_auc_score of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Train the RF Model\n",
    "rf_model = RandomForestClassifier(n_estimators=200)\n",
    "rf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 0.771\n",
      "roc-auc is 0.833\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test set - both \"hard\" predictions, and the scores (percent of trees voting yes)\n",
    "y_pred_class_rf = rf_model.predict(X_test)\n",
    "y_pred_prob_rf = rf_model.predict_proba(X_test)\n",
    "\n",
    "\n",
    "print('accuracy is {:.3f}'.format(accuracy_score(y_test,y_pred_class_rf)))\n",
    "print('roc-auc is {:.3f}'.format(roc_auc_score(y_test,y_pred_prob_rf[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_roc(y_test, y_pred, model_name):\n",
    "    fpr, tpr, thr = roc_curve(y_test, y_pred)\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    ax.plot(fpr, tpr, 'k-')\n",
    "    ax.plot([0, 1], [0, 1], 'k--', linewidth=.5)  # roc curve for random model\n",
    "    ax.grid(True)\n",
    "    ax.set(title='ROC Curve for {} on PIMA diabetes problem'.format(model_name),\n",
    "           xlim=[-0.01, 1.01], ylim=[-0.01, 1.01])\n",
    "\n",
    "\n",
    "plot_roc(y_test, y_pred_prob_rf[:, 1], 'RF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Single Hidden Layer Neural Network\n",
    "\n",
    "We will use the Sequential model to quickly build a neural network.  Our first network will be a single layer network.  We have 8 variables, so we set the input shape to 8.  Let's start by having a single hidden layer with 12 nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First let's normalize the data\n",
    "## This aids the training of neural nets by providing numerical stability\n",
    "## Random Forest does not need this as it finds a split only, as opposed to performing matrix multiplications\n",
    "\n",
    "\n",
    "normalizer = StandardScaler()\n",
    "X_train_norm = normalizer.fit_transform(X_train)\n",
    "X_test_norm = normalizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Model \n",
    "# Input size is 8-dimensional\n",
    "# 1 hidden layer, 12 hidden nodes, sigmoid activation\n",
    "# Final layer has just one node with a sigmoid activation (standard for binary classification)\n",
    "\n",
    "model_1 = Sequential([\n",
    "    Dense(12, input_shape=(8,), activation=\"relu\"),\n",
    "    Dense(1, activation=\"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 12)                108       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 121\n",
      "Trainable params: 121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#  This is a nice tool to view the model you have created and count the parameters\n",
    "\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comprehension question:\n",
    "Why do we have 121 parameters?  Does that make sense?\n",
    "\n",
    "\n",
    "Let's fit our model for 200 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 576 samples, validate on 192 samples\n",
      "Epoch 1/200\n",
      "576/576 [==============================] - 0s 702us/step - loss: 1.0712 - acc: 0.3576 - val_loss: 1.1019 - val_acc: 0.3750\n",
      "Epoch 2/200\n",
      "576/576 [==============================] - 0s 91us/step - loss: 1.0335 - acc: 0.3594 - val_loss: 1.0630 - val_acc: 0.3750\n",
      "Epoch 3/200\n",
      "576/576 [==============================] - 0s 84us/step - loss: 0.9988 - acc: 0.3663 - val_loss: 1.0272 - val_acc: 0.3750\n",
      "Epoch 4/200\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.9668 - acc: 0.3646 - val_loss: 0.9941 - val_acc: 0.3750\n",
      "Epoch 5/200\n",
      "576/576 [==============================] - 0s 79us/step - loss: 0.9373 - acc: 0.3698 - val_loss: 0.9637 - val_acc: 0.3802\n",
      "Epoch 6/200\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.9102 - acc: 0.3802 - val_loss: 0.9357 - val_acc: 0.3854\n",
      "Epoch 7/200\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.8853 - acc: 0.3889 - val_loss: 0.9099 - val_acc: 0.3906\n",
      "Epoch 8/200\n",
      "576/576 [==============================] - 0s 88us/step - loss: 0.8624 - acc: 0.3906 - val_loss: 0.8861 - val_acc: 0.3906\n",
      "Epoch 9/200\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.8414 - acc: 0.3941 - val_loss: 0.8642 - val_acc: 0.4062\n",
      "Epoch 10/200\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.8220 - acc: 0.4028 - val_loss: 0.8440 - val_acc: 0.4115\n",
      "Epoch 11/200\n",
      "576/576 [==============================] - 0s 85us/step - loss: 0.8041 - acc: 0.4097 - val_loss: 0.8253 - val_acc: 0.4115\n",
      "Epoch 12/200\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.7877 - acc: 0.4219 - val_loss: 0.8082 - val_acc: 0.4219\n",
      "Epoch 13/200\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.7725 - acc: 0.4375 - val_loss: 0.7923 - val_acc: 0.4531\n",
      "Epoch 14/200\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.7585 - acc: 0.4566 - val_loss: 0.7777 - val_acc: 0.4688\n",
      "Epoch 15/200\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.7457 - acc: 0.4705 - val_loss: 0.7641 - val_acc: 0.4792\n",
      "Epoch 16/200\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.7338 - acc: 0.4826 - val_loss: 0.7516 - val_acc: 0.4896\n",
      "Epoch 17/200\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.7228 - acc: 0.5174 - val_loss: 0.7400 - val_acc: 0.5000\n",
      "Epoch 18/200\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.7127 - acc: 0.5434 - val_loss: 0.7293 - val_acc: 0.4948\n",
      "Epoch 19/200\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.7033 - acc: 0.5625 - val_loss: 0.7193 - val_acc: 0.5052\n",
      "Epoch 20/200\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.6946 - acc: 0.5799 - val_loss: 0.7101 - val_acc: 0.5417\n",
      "Epoch 21/200\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.6866 - acc: 0.5885 - val_loss: 0.7016 - val_acc: 0.5417\n",
      "Epoch 22/200\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.6792 - acc: 0.6076 - val_loss: 0.6936 - val_acc: 0.5312\n",
      "Epoch 23/200\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.6724 - acc: 0.6181 - val_loss: 0.6862 - val_acc: 0.5417\n",
      "Epoch 24/200\n",
      "576/576 [==============================] - 0s 83us/step - loss: 0.6659 - acc: 0.6250 - val_loss: 0.6794 - val_acc: 0.5469\n",
      "Epoch 25/200\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.6599 - acc: 0.6372 - val_loss: 0.6729 - val_acc: 0.5521\n",
      "Epoch 26/200\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.6544 - acc: 0.6545 - val_loss: 0.6669 - val_acc: 0.5625\n",
      "Epoch 27/200\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.6491 - acc: 0.6597 - val_loss: 0.6613 - val_acc: 0.5625\n",
      "Epoch 28/200\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.6442 - acc: 0.6719 - val_loss: 0.6560 - val_acc: 0.5521\n",
      "Epoch 29/200\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.6397 - acc: 0.6771 - val_loss: 0.6511 - val_acc: 0.5677\n",
      "Epoch 30/200\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.6353 - acc: 0.6788 - val_loss: 0.6465 - val_acc: 0.5729\n",
      "Epoch 31/200\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.6313 - acc: 0.6858 - val_loss: 0.6421 - val_acc: 0.5885\n",
      "Epoch 32/200\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.6275 - acc: 0.6910 - val_loss: 0.6380 - val_acc: 0.6042\n",
      "Epoch 33/200\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.6239 - acc: 0.6962 - val_loss: 0.6341 - val_acc: 0.6198\n",
      "Epoch 34/200\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.6205 - acc: 0.6997 - val_loss: 0.6304 - val_acc: 0.6354\n",
      "Epoch 35/200\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.6172 - acc: 0.7031 - val_loss: 0.6269 - val_acc: 0.6458\n",
      "Epoch 36/200\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.6142 - acc: 0.7118 - val_loss: 0.6236 - val_acc: 0.6510\n",
      "Epoch 37/200\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.6113 - acc: 0.7135 - val_loss: 0.6205 - val_acc: 0.6510\n",
      "Epoch 38/200\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.6085 - acc: 0.7118 - val_loss: 0.6175 - val_acc: 0.6615\n",
      "Epoch 39/200\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.6059 - acc: 0.7222 - val_loss: 0.6147 - val_acc: 0.6719\n",
      "Epoch 40/200\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.6033 - acc: 0.7274 - val_loss: 0.6120 - val_acc: 0.6719\n",
      "Epoch 41/200\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.6010 - acc: 0.7274 - val_loss: 0.6094 - val_acc: 0.6823\n",
      "Epoch 42/200\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.5987 - acc: 0.7396 - val_loss: 0.6070 - val_acc: 0.6823\n",
      "Epoch 43/200\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.5966 - acc: 0.7431 - val_loss: 0.6047 - val_acc: 0.6875\n",
      "Epoch 44/200\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.5945 - acc: 0.7396 - val_loss: 0.6025 - val_acc: 0.6875\n",
      "Epoch 45/200\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.5925 - acc: 0.7344 - val_loss: 0.6003 - val_acc: 0.6875\n",
      "Epoch 46/200\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.5905 - acc: 0.7361 - val_loss: 0.5983 - val_acc: 0.7031\n",
      "Epoch 47/200\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.5887 - acc: 0.7344 - val_loss: 0.5963 - val_acc: 0.7031\n",
      "Epoch 48/200\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.5869 - acc: 0.7361 - val_loss: 0.5944 - val_acc: 0.7083\n",
      "Epoch 49/200\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.5852 - acc: 0.7361 - val_loss: 0.5926 - val_acc: 0.7083\n",
      "Epoch 50/200\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.5835 - acc: 0.7396 - val_loss: 0.5908 - val_acc: 0.7031\n",
      "Epoch 51/200\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.5819 - acc: 0.7413 - val_loss: 0.5891 - val_acc: 0.7031\n",
      "Epoch 52/200\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.5803 - acc: 0.7431 - val_loss: 0.5874 - val_acc: 0.7031\n",
      "Epoch 53/200\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.5788 - acc: 0.7431 - val_loss: 0.5858 - val_acc: 0.7031\n",
      "Epoch 54/200\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.5773 - acc: 0.7413 - val_loss: 0.5843 - val_acc: 0.7031\n",
      "Epoch 55/200\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.5760 - acc: 0.7413 - val_loss: 0.5827 - val_acc: 0.7083\n",
      "Epoch 56/200\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.5746 - acc: 0.7413 - val_loss: 0.5813 - val_acc: 0.7083\n",
      "Epoch 57/200\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.5733 - acc: 0.7431 - val_loss: 0.5799 - val_acc: 0.7031\n",
      "Epoch 58/200\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.5719 - acc: 0.7431 - val_loss: 0.5785 - val_acc: 0.6979\n",
      "Epoch 59/200\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.5707 - acc: 0.7413 - val_loss: 0.5771 - val_acc: 0.6979\n",
      "Epoch 60/200\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.5694 - acc: 0.7396 - val_loss: 0.5758 - val_acc: 0.6979\n",
      "Epoch 61/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 63us/step - loss: 0.5682 - acc: 0.7396 - val_loss: 0.5746 - val_acc: 0.7083\n",
      "Epoch 62/200\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.5670 - acc: 0.7413 - val_loss: 0.5733 - val_acc: 0.7083\n",
      "Epoch 63/200\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.5659 - acc: 0.7431 - val_loss: 0.5721 - val_acc: 0.7135\n",
      "Epoch 64/200\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.5648 - acc: 0.7431 - val_loss: 0.5709 - val_acc: 0.7135\n",
      "Epoch 65/200\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.5636 - acc: 0.7431 - val_loss: 0.5698 - val_acc: 0.7135\n",
      "Epoch 66/200\n",
      "576/576 [==============================] - 0s 92us/step - loss: 0.5626 - acc: 0.7431 - val_loss: 0.5686 - val_acc: 0.7188\n",
      "Epoch 67/200\n",
      "576/576 [==============================] - 0s 107us/step - loss: 0.5615 - acc: 0.7431 - val_loss: 0.5675 - val_acc: 0.7188\n",
      "Epoch 68/200\n",
      "576/576 [==============================] - 0s 89us/step - loss: 0.5604 - acc: 0.7448 - val_loss: 0.5664 - val_acc: 0.7240\n",
      "Epoch 69/200\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.5594 - acc: 0.7448 - val_loss: 0.5654 - val_acc: 0.7240\n",
      "Epoch 70/200\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.5584 - acc: 0.7448 - val_loss: 0.5643 - val_acc: 0.7240\n",
      "Epoch 71/200\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.5574 - acc: 0.7448 - val_loss: 0.5633 - val_acc: 0.7240\n",
      "Epoch 72/200\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.5565 - acc: 0.7483 - val_loss: 0.5623 - val_acc: 0.7240\n",
      "Epoch 73/200\n",
      "576/576 [==============================] - 0s 96us/step - loss: 0.5555 - acc: 0.7483 - val_loss: 0.5613 - val_acc: 0.7240\n",
      "Epoch 74/200\n",
      "576/576 [==============================] - 0s 94us/step - loss: 0.5546 - acc: 0.7465 - val_loss: 0.5604 - val_acc: 0.7240\n",
      "Epoch 75/200\n",
      "576/576 [==============================] - 0s 87us/step - loss: 0.5536 - acc: 0.7483 - val_loss: 0.5594 - val_acc: 0.7240\n",
      "Epoch 76/200\n",
      "576/576 [==============================] - 0s 84us/step - loss: 0.5527 - acc: 0.7448 - val_loss: 0.5585 - val_acc: 0.7240\n",
      "Epoch 77/200\n",
      "576/576 [==============================] - 0s 93us/step - loss: 0.5518 - acc: 0.7483 - val_loss: 0.5575 - val_acc: 0.7240\n",
      "Epoch 78/200\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.5509 - acc: 0.7500 - val_loss: 0.5566 - val_acc: 0.7240\n",
      "Epoch 79/200\n",
      "576/576 [==============================] - 0s 85us/step - loss: 0.5500 - acc: 0.7535 - val_loss: 0.5557 - val_acc: 0.7240\n",
      "Epoch 80/200\n",
      "576/576 [==============================] - 0s 90us/step - loss: 0.5492 - acc: 0.7535 - val_loss: 0.5548 - val_acc: 0.7292\n",
      "Epoch 81/200\n",
      "576/576 [==============================] - 0s 79us/step - loss: 0.5483 - acc: 0.7517 - val_loss: 0.5539 - val_acc: 0.7292\n",
      "Epoch 82/200\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.6214 - acc: 0.625 - 0s 79us/step - loss: 0.5474 - acc: 0.7517 - val_loss: 0.5531 - val_acc: 0.7292\n",
      "Epoch 83/200\n",
      "576/576 [==============================] - 0s 94us/step - loss: 0.5466 - acc: 0.7517 - val_loss: 0.5522 - val_acc: 0.7292\n",
      "Epoch 84/200\n",
      "576/576 [==============================] - 0s 92us/step - loss: 0.5458 - acc: 0.7500 - val_loss: 0.5514 - val_acc: 0.7240\n",
      "Epoch 85/200\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.5450 - acc: 0.7483 - val_loss: 0.5506 - val_acc: 0.7240\n",
      "Epoch 86/200\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.5442 - acc: 0.7483 - val_loss: 0.5497 - val_acc: 0.7240\n",
      "Epoch 87/200\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.5434 - acc: 0.7465 - val_loss: 0.5489 - val_acc: 0.7240\n",
      "Epoch 88/200\n",
      "576/576 [==============================] - 0s 90us/step - loss: 0.5425 - acc: 0.7448 - val_loss: 0.5482 - val_acc: 0.7292\n",
      "Epoch 89/200\n",
      "576/576 [==============================] - 0s 90us/step - loss: 0.5417 - acc: 0.7448 - val_loss: 0.5474 - val_acc: 0.7292\n",
      "Epoch 90/200\n",
      "576/576 [==============================] - 0s 86us/step - loss: 0.5410 - acc: 0.7465 - val_loss: 0.5466 - val_acc: 0.7292\n",
      "Epoch 91/200\n",
      "576/576 [==============================] - 0s 93us/step - loss: 0.5402 - acc: 0.7465 - val_loss: 0.5459 - val_acc: 0.7292\n",
      "Epoch 92/200\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.5394 - acc: 0.7500 - val_loss: 0.5451 - val_acc: 0.7292\n",
      "Epoch 93/200\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.5387 - acc: 0.7517 - val_loss: 0.5444 - val_acc: 0.7344\n",
      "Epoch 94/200\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.5379 - acc: 0.7535 - val_loss: 0.5436 - val_acc: 0.7344\n",
      "Epoch 95/200\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.5371 - acc: 0.7535 - val_loss: 0.5429 - val_acc: 0.7344\n",
      "Epoch 96/200\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.5364 - acc: 0.7535 - val_loss: 0.5422 - val_acc: 0.7344\n",
      "Epoch 97/200\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.5357 - acc: 0.7535 - val_loss: 0.5415 - val_acc: 0.7396\n",
      "Epoch 98/200\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.5350 - acc: 0.7535 - val_loss: 0.5408 - val_acc: 0.7396\n",
      "Epoch 99/200\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.5343 - acc: 0.7517 - val_loss: 0.5401 - val_acc: 0.7396\n",
      "Epoch 100/200\n",
      "576/576 [==============================] - 0s 85us/step - loss: 0.5336 - acc: 0.7535 - val_loss: 0.5394 - val_acc: 0.7396\n",
      "Epoch 101/200\n",
      "576/576 [==============================] - 0s 98us/step - loss: 0.5329 - acc: 0.7552 - val_loss: 0.5387 - val_acc: 0.7396\n",
      "Epoch 102/200\n",
      "576/576 [==============================] - 0s 85us/step - loss: 0.5322 - acc: 0.7535 - val_loss: 0.5381 - val_acc: 0.7396\n",
      "Epoch 103/200\n",
      "576/576 [==============================] - 0s 79us/step - loss: 0.5315 - acc: 0.7569 - val_loss: 0.5374 - val_acc: 0.7396\n",
      "Epoch 104/200\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.5309 - acc: 0.7552 - val_loss: 0.5368 - val_acc: 0.7396\n",
      "Epoch 105/200\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.5302 - acc: 0.7569 - val_loss: 0.5362 - val_acc: 0.7396\n",
      "Epoch 106/200\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.5295 - acc: 0.7587 - val_loss: 0.5356 - val_acc: 0.7448\n",
      "Epoch 107/200\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.5289 - acc: 0.7569 - val_loss: 0.5350 - val_acc: 0.7448\n",
      "Epoch 108/200\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.5283 - acc: 0.7569 - val_loss: 0.5344 - val_acc: 0.7448\n",
      "Epoch 109/200\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.5277 - acc: 0.7569 - val_loss: 0.5338 - val_acc: 0.7448\n",
      "Epoch 110/200\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.5271 - acc: 0.7569 - val_loss: 0.5333 - val_acc: 0.7448\n",
      "Epoch 111/200\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.5265 - acc: 0.7569 - val_loss: 0.5327 - val_acc: 0.7448\n",
      "Epoch 112/200\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.5259 - acc: 0.7552 - val_loss: 0.5321 - val_acc: 0.7448\n",
      "Epoch 113/200\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.5253 - acc: 0.7569 - val_loss: 0.5316 - val_acc: 0.7448\n",
      "Epoch 114/200\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.5247 - acc: 0.7552 - val_loss: 0.5311 - val_acc: 0.7448\n",
      "Epoch 115/200\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.5241 - acc: 0.7535 - val_loss: 0.5305 - val_acc: 0.7448\n",
      "Epoch 116/200\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.5236 - acc: 0.7517 - val_loss: 0.5300 - val_acc: 0.7448\n",
      "Epoch 117/200\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.5230 - acc: 0.7517 - val_loss: 0.5295 - val_acc: 0.7396\n",
      "Epoch 118/200\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.5224 - acc: 0.7535 - val_loss: 0.5290 - val_acc: 0.7396\n",
      "Epoch 119/200\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.5219 - acc: 0.7535 - val_loss: 0.5284 - val_acc: 0.7344\n",
      "Epoch 120/200\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.5214 - acc: 0.7535 - val_loss: 0.5279 - val_acc: 0.7344\n",
      "Epoch 121/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 67us/step - loss: 0.5208 - acc: 0.7535 - val_loss: 0.5274 - val_acc: 0.7344\n",
      "Epoch 122/200\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.5203 - acc: 0.7535 - val_loss: 0.5269 - val_acc: 0.7344\n",
      "Epoch 123/200\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.5197 - acc: 0.7517 - val_loss: 0.5265 - val_acc: 0.7344\n",
      "Epoch 124/200\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.5192 - acc: 0.7552 - val_loss: 0.5260 - val_acc: 0.7396\n",
      "Epoch 125/200\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.5187 - acc: 0.7517 - val_loss: 0.5255 - val_acc: 0.7396\n",
      "Epoch 126/200\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.5182 - acc: 0.7500 - val_loss: 0.5250 - val_acc: 0.7396\n",
      "Epoch 127/200\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.5177 - acc: 0.7517 - val_loss: 0.5246 - val_acc: 0.7396\n",
      "Epoch 128/200\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.5173 - acc: 0.7500 - val_loss: 0.5241 - val_acc: 0.7396\n",
      "Epoch 129/200\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.5167 - acc: 0.7500 - val_loss: 0.5237 - val_acc: 0.7396\n",
      "Epoch 130/200\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.5162 - acc: 0.7483 - val_loss: 0.5232 - val_acc: 0.7396\n",
      "Epoch 131/200\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.5158 - acc: 0.7483 - val_loss: 0.5228 - val_acc: 0.7396\n",
      "Epoch 132/200\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.5153 - acc: 0.7500 - val_loss: 0.5224 - val_acc: 0.7396\n",
      "Epoch 133/200\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.5148 - acc: 0.7517 - val_loss: 0.5219 - val_acc: 0.7396\n",
      "Epoch 134/200\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.5144 - acc: 0.7500 - val_loss: 0.5215 - val_acc: 0.7396\n",
      "Epoch 135/200\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.5139 - acc: 0.7517 - val_loss: 0.5211 - val_acc: 0.7396\n",
      "Epoch 136/200\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.5135 - acc: 0.7535 - val_loss: 0.5207 - val_acc: 0.7396\n",
      "Epoch 137/200\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.5130 - acc: 0.7517 - val_loss: 0.5203 - val_acc: 0.7396\n",
      "Epoch 138/200\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.5126 - acc: 0.7517 - val_loss: 0.5199 - val_acc: 0.7396\n",
      "Epoch 139/200\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.5122 - acc: 0.7500 - val_loss: 0.5195 - val_acc: 0.7396\n",
      "Epoch 140/200\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.5118 - acc: 0.7500 - val_loss: 0.5191 - val_acc: 0.7396\n",
      "Epoch 141/200\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.5113 - acc: 0.7517 - val_loss: 0.5187 - val_acc: 0.7396\n",
      "Epoch 142/200\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.5109 - acc: 0.7535 - val_loss: 0.5184 - val_acc: 0.7396\n",
      "Epoch 143/200\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.5104 - acc: 0.7535 - val_loss: 0.5180 - val_acc: 0.7396\n",
      "Epoch 144/200\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.5100 - acc: 0.7535 - val_loss: 0.5176 - val_acc: 0.7396\n",
      "Epoch 145/200\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.5096 - acc: 0.7535 - val_loss: 0.5172 - val_acc: 0.7396\n",
      "Epoch 146/200\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.5092 - acc: 0.7535 - val_loss: 0.5169 - val_acc: 0.7396\n",
      "Epoch 147/200\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.5088 - acc: 0.7535 - val_loss: 0.5165 - val_acc: 0.7396\n",
      "Epoch 148/200\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.5085 - acc: 0.7535 - val_loss: 0.5162 - val_acc: 0.7396\n",
      "Epoch 149/200\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.5081 - acc: 0.7535 - val_loss: 0.5158 - val_acc: 0.7396\n",
      "Epoch 150/200\n",
      "576/576 [==============================] - 0s 84us/step - loss: 0.5077 - acc: 0.7552 - val_loss: 0.5155 - val_acc: 0.7396\n",
      "Epoch 151/200\n",
      "576/576 [==============================] - 0s 93us/step - loss: 0.5073 - acc: 0.7569 - val_loss: 0.5152 - val_acc: 0.7396\n",
      "Epoch 152/200\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.5069 - acc: 0.7569 - val_loss: 0.5148 - val_acc: 0.7396\n",
      "Epoch 153/200\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.5066 - acc: 0.7587 - val_loss: 0.5145 - val_acc: 0.7396\n",
      "Epoch 154/200\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.5062 - acc: 0.7587 - val_loss: 0.5142 - val_acc: 0.7396\n",
      "Epoch 155/200\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.5058 - acc: 0.7587 - val_loss: 0.5138 - val_acc: 0.7396\n",
      "Epoch 156/200\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.5054 - acc: 0.7587 - val_loss: 0.5135 - val_acc: 0.7448\n",
      "Epoch 157/200\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.5051 - acc: 0.7587 - val_loss: 0.5132 - val_acc: 0.7448\n",
      "Epoch 158/200\n",
      "576/576 [==============================] - 0s 85us/step - loss: 0.5047 - acc: 0.7587 - val_loss: 0.5129 - val_acc: 0.7448\n",
      "Epoch 159/200\n",
      "576/576 [==============================] - 0s 91us/step - loss: 0.5043 - acc: 0.7587 - val_loss: 0.5125 - val_acc: 0.7448\n",
      "Epoch 160/200\n",
      "576/576 [==============================] - 0s 87us/step - loss: 0.5040 - acc: 0.7587 - val_loss: 0.5122 - val_acc: 0.7448\n",
      "Epoch 161/200\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.5037 - acc: 0.7587 - val_loss: 0.5119 - val_acc: 0.7448\n",
      "Epoch 162/200\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.5033 - acc: 0.7587 - val_loss: 0.5116 - val_acc: 0.7448\n",
      "Epoch 163/200\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.5029 - acc: 0.7587 - val_loss: 0.5113 - val_acc: 0.7500\n",
      "Epoch 164/200\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.5026 - acc: 0.7604 - val_loss: 0.5110 - val_acc: 0.7500\n",
      "Epoch 165/200\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.5023 - acc: 0.7604 - val_loss: 0.5107 - val_acc: 0.7500\n",
      "Epoch 166/200\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.5019 - acc: 0.7604 - val_loss: 0.5105 - val_acc: 0.7500\n",
      "Epoch 167/200\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.5015 - acc: 0.7604 - val_loss: 0.5102 - val_acc: 0.7500\n",
      "Epoch 168/200\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.5012 - acc: 0.7604 - val_loss: 0.5099 - val_acc: 0.7500\n",
      "Epoch 169/200\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.5008 - acc: 0.7604 - val_loss: 0.5096 - val_acc: 0.7500\n",
      "Epoch 170/200\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.5005 - acc: 0.7604 - val_loss: 0.5093 - val_acc: 0.7500\n",
      "Epoch 171/200\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.5002 - acc: 0.7587 - val_loss: 0.5091 - val_acc: 0.7500\n",
      "Epoch 172/200\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4999 - acc: 0.7604 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 173/200\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4995 - acc: 0.7587 - val_loss: 0.5085 - val_acc: 0.7552\n",
      "Epoch 174/200\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4992 - acc: 0.7587 - val_loss: 0.5082 - val_acc: 0.7552\n",
      "Epoch 175/200\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4989 - acc: 0.7587 - val_loss: 0.5080 - val_acc: 0.7552\n",
      "Epoch 176/200\n",
      "576/576 [==============================] - 0s 87us/step - loss: 0.4986 - acc: 0.7604 - val_loss: 0.5077 - val_acc: 0.7552\n",
      "Epoch 177/200\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.4983 - acc: 0.7622 - val_loss: 0.5074 - val_acc: 0.7552\n",
      "Epoch 178/200\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.4979 - acc: 0.7587 - val_loss: 0.5072 - val_acc: 0.7604\n",
      "Epoch 179/200\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.4976 - acc: 0.7604 - val_loss: 0.5069 - val_acc: 0.7604\n",
      "Epoch 180/200\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.4973 - acc: 0.7604 - val_loss: 0.5066 - val_acc: 0.7604\n",
      "Epoch 181/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 70us/step - loss: 0.4970 - acc: 0.7622 - val_loss: 0.5064 - val_acc: 0.7604\n",
      "Epoch 182/200\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4967 - acc: 0.7622 - val_loss: 0.5061 - val_acc: 0.7604\n",
      "Epoch 183/200\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4964 - acc: 0.7622 - val_loss: 0.5058 - val_acc: 0.7604\n",
      "Epoch 184/200\n",
      "576/576 [==============================] - 0s 79us/step - loss: 0.4961 - acc: 0.7622 - val_loss: 0.5056 - val_acc: 0.7604\n",
      "Epoch 185/200\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4958 - acc: 0.7622 - val_loss: 0.5053 - val_acc: 0.7604\n",
      "Epoch 186/200\n",
      "576/576 [==============================] - 0s 88us/step - loss: 0.4955 - acc: 0.7622 - val_loss: 0.5051 - val_acc: 0.7656\n",
      "Epoch 187/200\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.4479 - acc: 0.812 - 0s 84us/step - loss: 0.4952 - acc: 0.7622 - val_loss: 0.5048 - val_acc: 0.7656\n",
      "Epoch 188/200\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.4949 - acc: 0.7622 - val_loss: 0.5046 - val_acc: 0.7656\n",
      "Epoch 189/200\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4946 - acc: 0.7622 - val_loss: 0.5043 - val_acc: 0.7656\n",
      "Epoch 190/200\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4943 - acc: 0.7639 - val_loss: 0.5041 - val_acc: 0.7656\n",
      "Epoch 191/200\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4940 - acc: 0.7622 - val_loss: 0.5038 - val_acc: 0.7656\n",
      "Epoch 192/200\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4937 - acc: 0.7622 - val_loss: 0.5036 - val_acc: 0.7656\n",
      "Epoch 193/200\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4935 - acc: 0.7656 - val_loss: 0.5033 - val_acc: 0.7656\n",
      "Epoch 194/200\n",
      "576/576 [==============================] - 0s 96us/step - loss: 0.4932 - acc: 0.7656 - val_loss: 0.5031 - val_acc: 0.7656\n",
      "Epoch 195/200\n",
      "576/576 [==============================] - 0s 95us/step - loss: 0.4929 - acc: 0.7639 - val_loss: 0.5029 - val_acc: 0.7656\n",
      "Epoch 196/200\n",
      "576/576 [==============================] - 0s 83us/step - loss: 0.4926 - acc: 0.7639 - val_loss: 0.5026 - val_acc: 0.7656\n",
      "Epoch 197/200\n",
      "576/576 [==============================] - 0s 106us/step - loss: 0.4923 - acc: 0.7656 - val_loss: 0.5024 - val_acc: 0.7656\n",
      "Epoch 198/200\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4921 - acc: 0.7674 - val_loss: 0.5022 - val_acc: 0.7656\n",
      "Epoch 199/200\n",
      "576/576 [==============================] - 0s 79us/step - loss: 0.4918 - acc: 0.7674 - val_loss: 0.5020 - val_acc: 0.7656\n",
      "Epoch 200/200\n",
      "576/576 [==============================] - 0s 88us/step - loss: 0.4915 - acc: 0.7674 - val_loss: 0.5018 - val_acc: 0.7656\n"
     ]
    }
   ],
   "source": [
    "# Fit(Train) the Model\n",
    "\n",
    "# Compile the model with Optimizer, Loss Function and Metrics\n",
    "# Roc-Auc is not available in Keras as an off the shelf metric yet, so we will skip it here.\n",
    "\n",
    "model_1.compile(SGD(lr = .003), \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "run_hist_1 = model_1.fit(X_train_norm, y_train, validation_data=(X_test_norm, y_test), epochs=200)\n",
    "# the fit function returns the run history. \n",
    "# It is very convenient, as it contains information about the model fit, iterations etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Like we did for the Random Forest, we generate two kinds of predictions\n",
    "#  One is a hard decision, the other is a probabilitistic score.\n",
    "\n",
    "y_pred_class_nn_1 = model_1.predict_classes(X_test_norm)\n",
    "y_pred_prob_nn_1 = model_1.predict(X_test_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check out the outputs to get a feel for how keras apis work.\n",
    "y_pred_class_nn_1[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob_nn_1[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model performance and plot the roc curve\n",
    "print('accuracy is {:.3f}'.format(accuracy_score(y_test,y_pred_class_nn_1)))\n",
    "print('roc-auc is {:.3f}'.format(roc_auc_score(y_test,y_pred_prob_nn_1)))\n",
    "\n",
    "plot_roc(y_test, y_pred_prob_nn_1, 'NN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There may be some variation in exact numbers due to randomness, but you should get results in the same ballpark as the Random Forest - between 75% and 85% accuracy, between .8 and .9 for AUC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the `run_hist_1` object that was created, specifically its `history` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_hist_1.history.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the training loss and the validation loss over the different epochs and see how it looks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(run_hist_1.history[\"loss\"],'r', marker='.', label=\"Train Loss\")\n",
    "ax.plot(run_hist_1.history[\"val_loss\"],'b', marker='.', label=\"Validation Loss\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the losses are still going down on both the training set and the validation set.  This suggests that the model might benefit from further training.  Let's train the model a little more and see what happens. Note that it will pick up from where it left off. Train for 1000 more epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Note that when we call \"fit\" again, it picks up where it left off\n",
    "run_hist_1b = model_1.fit(X_train_norm, y_train, validation_data=(X_test_norm, y_test), epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(run_hist_1.history[\"loss\"])\n",
    "m = len(run_hist_1b.history['loss'])\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "ax.plot(range(n), run_hist_1.history[\"loss\"],'r', marker='.', label=\"Train Loss - Run 1\")\n",
    "ax.plot(range(n, n+m), run_hist_1b.history[\"loss\"], 'hotpink', marker='.', label=\"Train Loss - Run 2\")\n",
    "\n",
    "ax.plot(range(n), run_hist_1.history[\"val_loss\"],'b', marker='.', label=\"Validation Loss - Run 1\")\n",
    "ax.plot(range(n, n+m), run_hist_1b.history[\"val_loss\"], 'LightSkyBlue', marker='.',  label=\"Validation Loss - Run 2\")\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this graph begins where the other left off.  While the training loss is still going down, it looks like the validation loss has stabilized (or even gotten worse!).  This suggests that our network will not benefit from further training.  What is the appropriate number of epochs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "Now it's your turn.  Do the following in the cells below:\n",
    "- Build a model with two hidden layers, each with 6 nodes\n",
    "- Use the \"relu\" activation function for the hidden layers, and \"sigmoid\" for the final layer\n",
    "- Use a learning rate of .003 and train for 1500 epochs\n",
    "- Graph the trajectory of the loss functions, accuracy on both train and test set\n",
    "- Plot the roc curve for the predictions\n",
    "\n",
    "Experiment with different learning rates, numbers of epochs, and network structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Type your code here to with layer 1,2 having activation relu and layer 3 with activation sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Type your code here to plot the loss accuracy and ROC curve"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py3 OpenCV3 (Forge)",
   "language": "python",
   "name": "opencv-forge"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
