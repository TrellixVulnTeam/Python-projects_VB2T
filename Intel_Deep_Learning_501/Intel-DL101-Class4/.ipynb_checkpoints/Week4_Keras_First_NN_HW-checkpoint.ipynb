{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Keras to Build and Train Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we will use a neural network to predict diabetes using the Pima Diabetes Dataset.  We will start by training a Random Forest to get a performance baseline.  Then we will use the Keras package to quickly build and train a neural network and compare the performance.  We will see how different network structures affect the performance, training time, and level of overfitting (or underfitting).\n",
    "\n",
    "## UCI Pima Diabetes Dataset\n",
    "\n",
    "* UCI ML Repositiory (http://archive.ics.uci.edu/ml/datasets/Pima+Indians+Diabetes)\n",
    "\n",
    "\n",
    "### Attributes: (all numeric-valued)\n",
    "   1. Number of times pregnant\n",
    "   2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
    "   3. Diastolic blood pressure (mm Hg)\n",
    "   4. Triceps skin fold thickness (mm)\n",
    "   5. 2-Hour serum insulin (mu U/ml)\n",
    "   6. Body mass index (weight in kg/(height in m)^2)\n",
    "   7. Diabetes pedigree function\n",
    "   8. Age (years)\n",
    "   9. Class variable (0 or 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The UCI Pima Diabetes Dataset which has 8 numerical predictors and a binary outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preliminaries\n",
    "\n",
    "from __future__ import absolute_import, division, print_function  # Python 2/3 compatibility\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve, roc_auc_score, roc_curve, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "## Import Keras objects for Deep Learning\n",
    "\n",
    "from keras.models  import Sequential, K\n",
    "from keras.layers import Input, Dense, Flatten, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam, SGD, RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load in the data set (Internet Access needed)\n",
    "\n",
    "#url not working\n",
    "#url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data\"\n",
    "names = [\"times_pregnant\", \"glucose_tolerance_test\", \"blood_pressure\", \"skin_thickness\", \"insulin\", \n",
    "         \"bmi\", \"pedigree_function\", \"age\", \"has_diabetes\"]\n",
    "#diabetes_df = pd.read_csv(url, names=names)\n",
    "\n",
    "diabetes_df = pd.read_csv(\"pima-indians-diabetes.csv\",names=names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>times_pregnant</th>\n",
       "      <th>glucose_tolerance_test</th>\n",
       "      <th>blood_pressure</th>\n",
       "      <th>skin_thickness</th>\n",
       "      <th>insulin</th>\n",
       "      <th>bmi</th>\n",
       "      <th>pedigree_function</th>\n",
       "      <th>age</th>\n",
       "      <th>has_diabetes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>8</td>\n",
       "      <td>109</td>\n",
       "      <td>76</td>\n",
       "      <td>39</td>\n",
       "      <td>114</td>\n",
       "      <td>27.9</td>\n",
       "      <td>0.640</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586</th>\n",
       "      <td>8</td>\n",
       "      <td>143</td>\n",
       "      <td>66</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>34.9</td>\n",
       "      <td>0.129</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>683</th>\n",
       "      <td>4</td>\n",
       "      <td>125</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32.3</td>\n",
       "      <td>0.536</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>1</td>\n",
       "      <td>111</td>\n",
       "      <td>86</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.143</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>693</th>\n",
       "      <td>7</td>\n",
       "      <td>129</td>\n",
       "      <td>68</td>\n",
       "      <td>49</td>\n",
       "      <td>125</td>\n",
       "      <td>38.5</td>\n",
       "      <td>0.439</td>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     times_pregnant  glucose_tolerance_test  blood_pressure  skin_thickness  \\\n",
       "188               8                     109              76              39   \n",
       "586               8                     143              66               0   \n",
       "683               4                     125              80               0   \n",
       "249               1                     111              86              19   \n",
       "693               7                     129              68              49   \n",
       "\n",
       "     insulin   bmi  pedigree_function  age  has_diabetes  \n",
       "188      114  27.9              0.640   31             1  \n",
       "586        0  34.9              0.129   41             1  \n",
       "683        0  32.3              0.536   27             1  \n",
       "249        0  30.1              0.143   23             0  \n",
       "693      125  38.5              0.439   43             1  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a peek at the data -- if there are lots of \"NaN\" you may have internet connectivity issues\n",
    "print(diabetes_df.shape)\n",
    "diabetes_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = diabetes_df.iloc[:, :-1].values\n",
    "y = diabetes_df[\"has_diabetes\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data to Train, and Test (75%, 25%)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=11111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3489583333333333, 0.6510416666666666)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y), np.mean(1-y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we see that about 35% of the patients in this dataset have diabetes, while 65% do not.  This means we can get an accuracy of 65% without any model - just declare that no one has diabetes. We will calculate the ROC-AUC score to evaluate performance of our model, and also look at the accuracy as well to see if we improved upon the 65% accuracy.\n",
    "## Exercise: Get a baseline performance using Random Forest\n",
    "To begin, and get a baseline for classifier performance:\n",
    "1. Train a Random Forest model with 200 trees on the training data.\n",
    "2. Calculate the accuracy and roc_auc_score of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Train the RF Model\n",
    "rf_model = RandomForestClassifier(n_estimators=200)\n",
    "rf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 0.771\n",
      "roc-auc is 0.833\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test set - both \"hard\" predictions, and the scores (percent of trees voting yes)\n",
    "y_pred_class_rf = rf_model.predict(X_test)\n",
    "y_pred_prob_rf = rf_model.predict_proba(X_test)\n",
    "\n",
    "\n",
    "print('accuracy is {:.3f}'.format(accuracy_score(y_test,y_pred_class_rf)))\n",
    "print('roc-auc is {:.3f}'.format(roc_auc_score(y_test,y_pred_prob_rf[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAecAAAHiCAYAAADSwATnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzs3XmcjXX/x/HX115C2bNXuKWVSHVXJpWKO1ru/CKp+66oX92FjEGIxIRKult+tJDctmhTI6KGKNlSdtmHyM4Ms5/v749zco9phhlzzvme5f18PM7Duc51neu8zzXH+ZzPtRprLSIiIhI6irkOICIiIidTcRYREQkxKs4iIiIhRsVZREQkxKg4i4iIhBgVZxERkRCj4ixRyRhzljFmpjHmiDHmI9d5ookx5mFjzMIcwynGmAsL8Lx6xhhrjCkR2IRuGWO2GWNuyWdcjDFmZ7AzSfCpOEcB33/2VN+X4B5jzHhjzDm5prnOGPONMSbZV7BmGmMa55qmvDHmNWPMDt+8NvmGK+fzusYY87QxZrUx5pgxZqcx5iNjzGWBfL8F9HegGlDJWntfUWfm+9L0+JZLsjFmgzHmH7mmsb7lkOK7HS7q6xYg13hjTIbv9Q4aY742xjTyjRtkjJmYK9/vOYufMaaEMWavMeZPJ0TwzTvLGFOjKBmttedYa7cUZR6nEy2FXSKHinP0uNNaew5wJdAE6PvHCGPMtcAc4DOgBnAB8DOw6I+OxhhTCpgHXALcDpQHrgMOAFfn85qjgWeAp4GKQEPgU6BtYcMH4Eu1LrDRWpvlxyy/+ZZxeaAH8I4x5i+5prnCV4zOsdaeW9jXPkMjfLlqAXuB8aeY9jBwR47hNsCh3BMZY8oC9wJHgAf8ljTC6ceBFJSKc5Sx1u4BZuMt0n8YAUyw1o621iZbaw9aa/sDi4FBvmm6AHWAu621a621HmvtXmvtEGttQu7XMcY0AJ4EOlprv7HWpltrj1tr/2Otfck3TaIx5tEcz8m9utMaY540xvwK/GqM+T9jzMu5XuczY0xP3/0axpgZxph9xpitxpin81oGxpjBwEDgf3wd5SPGmGLGmP7GmO2+TnGCMaaCb/o/uq5HjDE7gG9Os4ytb5kcBC4/1bT55CtIlod8azD2G2OeK8h8rbXHgUnApaeY7EO8f+s/dAEm5DHdvXgL+QvAQ6d5P5WMMZ8bY44aY5YAF+Uab40x9X332xpjfvJNm2SMGZTHLP9pjPnNGLPbGPNsjvkUM8b0McZsNsYcMMZMM8ZU9I1e4Pv3sO9vfq3vOf80xqwzxhwyxsw2xtT1PW6MMaN8y/+IMeYXY0yey833OY43xizxTfvZH6+b32fHGNPOGLPGGHPY9/yLc822uTFmrS/XOGNMmXxeO9/PvG/NyEfGmInGuzZnlTGmoTGmr+99JRljWuc1X3FPxTnKGGNq4e2MNvmGz8bbAee13XUacKvv/i3AV9balAK+1M3ATmvtkqIl5i6gBdAYb2H5H2OMATDGnAe0BqYYY4oBM/F2/DV9r9/dGHNb7hlaa58HhgFTfR3se8DDvttNwIXAOcAbuZ7aErgY+NM8c/IViXZAZXzLuZAKkuV64C943+fAPL7c88p1Dt4u96dTTPYpcKMx5lxjzLnADXjXqOT2EDAZmAI0MsY0PcU83wTSgPOBf/pu+TmG9wfBuXjXsDxhjLkr1zQ3AQ3w/u37mP9un30a7+elJd41QId8rw1wo+/fc31/8x988+0H3ANUAb7zvSd8874R79qec4H/wbuWKD9dfO+rBpAFvJ5r/InPjjGmoe91uvteNwGYabxrp/7wAN7P2UW+DP1zv2ABP/N34v3BdR7ev/tsvN/7NfH+sBpzivckLllrdYvwG7ANSAGSAYt39fS5vnG1fI81yuN5twOZvvtfAy8V4jWfAxafZppE4NEcww8DC3MMW6BVjmED7ABu9A0/Bnzju98C2JFr/n2Bcfm89iBgYo7hecD/5hj+C5AJlADq+bJceIr3EgN48HaT6UA20D3XNBY46pvmMPB6PvMqSJZaOcYvAe7PZ17j8RbGw8Ae4HPgonyWgQXqA+8C3YDHgXd8j9kc09XxvdcrfcOzgdH5vH5xX/ZGOR4blsffuX4+z38NGOW7/8d7zzmvEcB7vvvrgJtzjDs/j+VWIsf4WcAjOYaLAcfxbvJoBWwErgGKFeBz/FKO4cZAhu+9/+mzAwwApuV63V1ATI7/r4/nGN8G2Jzjc7azIJ9539/36xzj7sT7PVDcN1zOl+3cgv6/1i14N3XO0eMua205vP+5G+Ht6sDbXXjwfpHldj6w33f/QD7T5Kew0+cn6Y871vuNMgXo6HuoE/Af3/26QA3fasLDxruzVT+8O30VRA1ge47h7Xi/1HM+P4lT+816tyOXx9s5tcpjmqbW2nN9tzxXuxcwy54c94/j7a7z87Lv9apba9tZazef5n1MwNsJ5rdK+0FgnbV2pW/4P0AnY0zJPKat4suec9ltz2M6AIwxLYwx3/pW0x7B+wMh9w6Huef1xw5pdYFPcvz91+H9kZTfZ6AuMDrH9Afx/gCsaa39Bu/aijeB340xY40x5fPLnUemkrly5xx/0t/XWuvxja9ZgPeYO//pPvO/57ifCuy31mbnGIZTf3bEERXnKGOtnY+3m3rZN3wM+AHIa4/lDni7OIC5eFfJlS3gS80Dahljmp1immPA2TmGq+cVOdfwZODvvm2DLYAZvseTgK05Ct+51tpy1to2Bcz7G94vuz/Uwbt6MueXW4Eu4WatTQfigMvyWCXrryyB9B3eH1bVgIV5jO8CXGi8e/7vAV7FW4juyGPafXiz187xWJ1TvPYkvN19bWttBeD/8BbMnHLP6zff/STgjlyfgTLW2l3k/bdLArrlmv4sa+33ANba1621V+HdCbIhEHuK3LkzZfLfH7bkev2T/r6+zTS18XbPp3uPufMX5TMvIUzFOTq9BtxqjPljp7A+wEPGe9hTOWPMecaYF4FrgcG+aT7E+2UwwxjTyLddtZIxpp8x5k9fBtbaX4G3gMnGe5hRKWNMGWPM/caYPr7JVgL3GGPO9u0Q9Mjpgltrf8L7hf8uMNta+8fhSEuAo8aYOOM9hrm4MeZSY0zzAi6TyUAPY8wFvm2zf2yTLvTe3L6cGcAreHc8Kyy/Ziks3xqKO4F2vvsn+HakugjvHvpX+m6X4i2qf9oxzNelfQwM8v2dG+c1XQ7lgIPW2jRjzNV4147kNsA3r0uAfwBTfY//HzA0x05dVYwx7X3j9uFdQ5TzeOr/A/r65oMxpoIx5j7f/ea+Lr4k3h+RaXi78Px0NsY09u3D8QIwPUeHmts0oK0x5mbf/J/Fuynk+xzTPGmMqeXbsaxfjveYU1E/8xLCVJyjkLV2H97VlQN8wwvx7nxyD7Ab72q0JsD1viL7Rzd4C7Ae7/bno3i/HCoDP+bzUk/z31WDh4HNwN14d2IBGIV329zvwAf8dxX16Uz2ZZmU4z1l4y0oVwJb8XYt7wIVCjjP9/H+AFnge34a8K8CPvdU86xjjLnzDJ7n7yyFYq1dY61dk8eoh4DPrLWrrLV7/rjhPWzub+a/e0fn9BTeVad78K61GXeKl/5f4AVjTDLeHzbT8phmPt4d7ebhXWU/x/f4aLxd9xzf8xfjXbuC9e6pPhTv4YGHjTHXWGs/AYbj3aHwKLCa/3b/5fFubz+E9//DAXxrm/Lxoe+97QHK4P3s58lauwHoDPwb7+f0TryHOmbkmGwS3sMbt/huL+Yxn6J+5iWEmVw/jEVEpBCMMYl4d6x713UWiRzqnEVEREKMirOIiEiI0WptERGREKPOWUREJMSoOIuIiISY014hxRjzPvA3YK+19k8nfvcdQD8a7ynmjgMPW2tXnG6+lStXtvXq1TsxfOzYMcqWLej5LaSwtHwDS8s3cLRsA0vLN3ByL9vly5fvt9ZWKchzC3L5svF4j1XN6zR+4D0usIHv1gJ42/fvKdWrV49ly5adGE5MTCQmJqYAceRMaPkGlpZv4GjZBpaWb+DkXrbGmHxPXZvbaVdrW2sX4D3nbH7a473coLXWLgbONcb445zKIiIiUckfF/6uycknad/pe2y3H+YtIiJRZPXq1bz//vtkZ5/qbKnh4dixY2e8VsIfxTn3SekhnwsEGGO6Al0BqlWrRmJi4olxKSkpJw2Lf2n5BpaWb+Bo2QZWKC3fJUuWMGjQILKysihdurTrOGfMWktGRga1atU642Xrj+K8k5OvoFKLvK+ggrV2LDAWoFmzZjbnLwpt9wgsLd/A0vINHC3bwAqV5Tt27Fj69evHZZddxhdffEHNmjVP/6QQ5PF4WLduHaVKlWLXrl1nvGz9cSjV50AX43UNcMRaq1XaIiJyWh6Ph759+9KtWzdat27NggULwrYwW2vp27cv1loaNGhQpHkV5FCqyUAMUNkYsxN4Hu+FxLHW/h+QgPcwqk14D6X6R5ESiYhIVEhLS+Phhx9m6tSpdOvWjTfeeIMSJfyxQjf4MjMzWbRoEX369OG8884r8vxOuxSstR1PM94CTxY5iYiIRI0DBw7Qvn17Fi1axPDhw4mNjcV72ozwNGTIELp06eKXwgz+2eYsIiJhbP78+WzatClor+fxeBg5ciQ7duxg6tSpdOjQIWiv7W/p6enMmDGD559/nuLFi/ttvirOIiJRbOXKlfTo0SPor1upUiXmzp3L9ddfH/TX9qe33nqLe++916+FGVScRUSiVnZ2Nm+88QZ16tTh22+/pWTJkkF77UqVKnH22WcH7fX87dixY4wZM4aePXsGZP4qziIiUer9999n8+bNTJ06lQsvvNB1nLDy6aef0qlTp4DNX1elEhGJQkeOHOG5557j8ssv57777nMdJ2wcOXKEuLg4OnXqRPXq1QP2OirOIiJRaMiQIezfv58nn3wyrPeSDqaMjAyWLFlCXFxcwJeZVmuLiJzCjh07+P33313H8KsDBw4wevRo/vnPf9KwYUPXccLC/v37ef755xk1ahSlSpUK+OupOIuI5GHfvn0MGDCAd955B4/H4zqO35UvX56hQ4eybt0611FC3oEDB9i+fTvx8fFBKcyg4iwicpLMzEzeeustBg0aRHJyMk8++SS33Xab61h+d9lll1GtWjUV59PYvXs3L774IiNGjKBs2bJBe10VZxERn9mzZ9O9e3fWr1/PrbfeymuvvUbjxo1dxxJHdu7cyaFDhxg5cmTQD/vSDmEiEvV+/fVX7rzzTm6//XYyMzP57LPPmD17tgpzFNu9ezcjRoygQYMGTo7HVnEWkah19OhRevfuzSWXXEJiYiLDhw9nzZo1tGvXTnswR7HNmzfz+++/M3LkSMqUKeMkg1ZriwhZWVlkZWW5jpGnjIwM0tLS/DpPay2TJ0+mb9++7N27l3/84x8MGzYsoMetSng4evQob7/9NvHx8UE9Y1puKs4iUW737t00atSIo0ePuo4SdNdccw1ffPEFzZs3dx1FQsDatWtPdMyu15yoOItEuS+++IKjR4/St29fypcv7zrOn2zZsiUgp5Zs2LAhd999t/MvYQkNWVlZzJgxg379+oXEZ0LFWSTKJSQkUKdOHYYOHRoSX0q5JSYmEhMT4zqGRLAVK1awZcsWBgwY4DrKCdohTCSKpaenM3fuXNq0aROShVkk0Ky1LF26lHvvvdd1lJOocxaJYgsXLiQlJYU2bdq4jiISdIsWLWL16tV069bNdZQ/UecsEsVmzZpFqVKlaNWqlesoIkF17NgxDh06RNeuXV1HyZM6Z5EolpCQQMuWLYN6WkIR1+bOncuaNWt45plnXEfJlzpnkSi1bds21q1bp1XaElW2bt1KpUqVQrowg4qzSNSaNWsWgIqzRI0vvviCWbNm0aRJE9dRTkurtUWiVEJCAhdeeCENGjRwHUUk4BYuXEjz5s3529/+5jpKgahzFolCaWlpzJs3T4dQSVRISEhg06ZNVKtWzXWUAlPnLBKF5s+fT2pqqlZpS8T7+OOPad26Neecc47rKIWi4iwSRr755hvGjBlDdnZ2keazfv16ypQpozNvSURbsGABGRkZYVeYQcVZJGy89957dOvWjcqVK1O5cuUiz6979+6cddZZfkgmEnree+897r77bm688UbXUc6IirNIiLPWMmDAAIYOHcptt93GtGnTQvICFSKhYvXq1VSuXJmKFSu6jnLGtEOYSAhLT0+nc+fODB06lMcee4yZM2eqMIucwujRozn77LNp37696yhFos5ZJEQdPHiQu+66i++++474+Hji4uK0Z7XIKSQlJdG4ceOAXGI02NQ5i4SgLVu2cN111/Hjjz8yefJk+vTpo8Iskg9rLS+99BL79+/n1ltvdR3HL9Q5izh2+PBhxo0bR3JyMgAej4e33nqL7Oxs5s6dyw033OA4oUjostayc+dObrrpprA481dBqTiLOJKdnc27775L//792b9//0njGjVqxKeffspf/vIXR+lEQp+1lsGDB9O2bVtatGjhOo5fabW2iAPz58/nqquu4vHHH6dx48b89NNPZGdnn7itXbtWhVnkFDweD6tXr6Zz5840b97cdRy/U3EWCaLt27fToUMHYmJiOHz4MB999BGJiYlceeWVFCtW7MRN25dF8metpX///ng8HurXr+86TkBotbZIEBw7dozhw4czcuRIjDG88MIL9OrVSycBESmkrKwsEhMTiYuLo0KFCq7jBIw6Z5EAstYyefJkGjVqxJAhQ7jnnnvYsGEDAwYMUGEWOQPDhg2jdu3aEV2YQZ2ziN+kp6czb948MjIyTgz/+9//ZtGiRTRt2pQpU6bw17/+1XFKkfCUkZHB1KlT6d+/P8WKRX5fqeIs4ifTp0+nc+fOJz1WtWpV3nvvPR5++OGo+EIRCZR33nmHtm3bRs3/IxVnET85fvw4ALNnz6Zq1aoANGjQgLJly7qMJRLWUlNTeeONN4iNjXUdJahUnEX87JJLLqFmzZquY4iEPWstM2fO5IEHHnAdJeiiY/2AiIiEleTkZGJjY/n73/9OjRo1XMcJOhVnEREJKWlpaSxfvpw+ffpEzTbm3KLzXYuISEg6ePAgPXv25JprrqFy5cqu4zijbc4iIhISDhw4wI4dO4iPj6dMmTKu4zilzllERJz7/fffGThwIPXr14/4E4wUhDpnERFx6rfffmP//v2MGDFChx76qHMWERFn9u3bx0svvaRzAuSizllERJzYtm0bBw4cYOTIkZQuXdp1nJCizllERILu+PHj/Pvf/+ayyy5TYc6DOmeRQpg2bRqPPvoo2dnZJz3u8XhOPBatx2WKFNSGDRvYtm0bL7/8sq5dng8VZ5FCWL16NcnJyfTq1eukx5OSkqhduzbnn38+1atXd5ROJPRlZ2czffp04uLiVJhPQcVZpJCMMYwcOfKkxxITE4mJiXETSCRM/Pzzz6xevZrnnnvOdZSQp/VvIiIScB6Ph6VLl9KxY0fXUcKCOmcREQmoxYsXs3TpUv71r3+5jhI21DmLiEjAJCcnc+jQIZ566inXUcKKOmeRXGbMmMHLL7+c57ikpKQgpxEJX4mJiSxbtuxPO1DK6ak4i+Qyc+ZMfvrpJ1q2bPmncZdccgkdOnRwkEokvGzatImKFSuqMJ8hFWeRPFSvXp3Zs2e7jiESlr766is2btzI008/7TpK2FJxFhERv1mwYAFNmzbl9ttvdx0lrGmHMBER8Ys5c+awYcMGqlat6jpK2FPnLCIiRfbxxx9zyy230Lp1a9dRIoKKs0SFAwcO8NJLL3H06NHTTvv9998HIZFI5Pjxxx9JTU2lfPnyrqNEDBVniQqxsbF88MEHBV7dpl//IgUzbtw42rRpQ4sWLVxHiSgqzhLxli1bxrhx44iNjWXEiBGu44hEjF9//ZXy5ctTrVo111EijnYIk4hmreWZZ56hatWq9O/f33UckYjx5ptvkp2dzb333us6SkRS5ywRbcqUKXz//fe8++672h4m4id79uyhfv36NGrUyHWUiKXOWSLW8ePH6d27N02aNOHhhx92HUck7Flrefnll9mxYwe33Xab6zgRTZ2zRIwDBw4wY8YMMjMzAfjhhx/YuXMnkyZNonjx4o7TiYQ3ay27du3i+uuv5+qrr3YdJ+KpOEtE2LhxI23atGHz5s0nPf7Pf/6TG264wVEqkchgreXFF1/klltu4dprr3UdJyqoOEvYW7hwIe3bt6d48eIkJibSuHHjE+MqV67sMJlI+LPWsmrVKjp16sRFF13kOk7U0DZnCWtTp07l5ptvpnLlyvzwww+0bNmSKlWqnLgZY1xHFAlrgwYNIisrS4U5yFScJSxZaxk+fDj3338/LVq04IcfftCXh4gfZWdnM3v2bHr16kXTpk1dx4k6Ks4SdrKysnj88cfp06cPHTt25Ouvv6ZixYquY4lElBEjRlC7dm3KlSvnOkpU0jZnCSvJycl06NCBr776in79+jFkyBCKFdNvTBF/yczMZOLEicTFxen/lkMqzhJW3njjDb766ivGjh3LY4895jqOSMQZP348rVq1UmF2TMVZwsqRI0coXbq0CrOIn6WlpfHKK6/Qr18/7UgZAgr008gYc7sxZoMxZpMxpk8e4+sYY741xvxkjPnFGNPG/1FFRCQQrLXMmjWLhx56SIU5RJy2OBtjigNvAncAjYGOxpjGuSbrD0yz1jYB7gfe8ndQERHxv9TUVHr27Mmdd95JrVq1XMcRn4J0zlcDm6y1W6y1GcAUoH2uaSzwx1UFKgC/+S+iiIgEQmpqKps2baJv376UKKGtnKGkIH+NmkBSjuGdQO6rag8C5hhj/gWUBW7Ja0bGmK5AV4Bq1aqRmJh4YlxKSspJw+JfkbJ8d+zYgcfjCbn3EinLNxRp2QZGSkoK77zzDp07d2bt2rWsXbvWdaSIU5TPbkGKc14bIGyu4Y7AeGvtK8aYa4EPjTGXWms9Jz3J2rHAWIBmzZrZmJiYE+MSExPJOSz+FSnL96uvvqJYsWIh914iZfmGIi1b/zt48CBJSUmMHz+en3/+Wcs3QIry2S3Iau2dQO0cw7X482rrR4BpANbaH4AygE5qLCISYvbv38+AAQOoV68e5513nus4ko+CFOelQANjzAXGmFJ4d/j6PNc0O4CbAYwxF+Mtzvv8GVRERIpmz5497Nq1i5deeokKFSq4jiOncNribK3NAp4CZgPr8O6VvcYY84Ixpp1vsmeBx4wxPwOTgYettblXfYuIiCOHDh1iyJAh1K9fX6fkDAMF2j3PWpsAJOR6bGCO+2uBv/o3moiI+MOOHTv47bffePXVVyldurTrOFIAOj+biEgES09PZ/To0TRp0kSFOYzowDYJOW+//Tbx8fF5jjt8+HCQ04iEr19//ZUNGzbw8ssv68xfYUbFWULOd999x+HDh/n73/+e5/jLL788yIlEwo+1lunTpxMbG6vCHIZUnCUkVa9enffff991DJGwtHr1apYtW0bfvn1dR5EzpG3OIiIRxOPxsGzZMrp06eI6ihSBOmcRkQixbNkyFixYQM+ePV1HkSJS5ywiEgGOHDnCwYMH6dGjh+so4gcqziIiYe67777j7bffpnXr1tr5K0KoOIuIhLENGzZQsWJF4uLiXEcRP1JxFhEJU3PnzuXLL7/kkksuUcccYbRDmIhIGFqwYAGXX345t9xyi+soEgDqnEVEwkxiYiJr166latWqrqNIgKhzFhEJI5988gkxMTHExMS4jiIBpOIszqWnp/PWW2+xb5/3EuArV650nEgkNK1cuZKjR49y3nnnuY4iAabiLM69/PLL9O/fn5IlS554rH379g4TiYSeDz/8kJiYGB566CHXUSQIVJzFqV27djFs2DDuueceZsyY4TqOSEjasWMHpUuXpnbt2q6jSJBohzBxqm/fvmRnZzNy5EjXUURC0pgxYzh06BAdOnRwHUWCSMVZnFm8eDEffvghPXv25MILL3QdRyTk7Nu3jzp16nDFFVe4jiJBpuIsTng8Hrp3787555+vy9qJ5GHUqFFs2LCBO+64w3UUcUDbnMWJSZMm8eOPPzJ+/HjKlSvnOo5IyLDWsmvXLq677jpatGjhOo44os5Zgi4lJYW4uDiaN2/Ogw8+6DqOSMiw1hIfH8/WrVtVmKOcOmcJuuHDh/Pbb78xffp0ihXT70MR8BbmlStX0rFjRy644ALXccQxfTNKUG3bto2RI0fSqVMnrr32WtdxRELGiy++SFZWlgqzAOqcJch69+5N8eLFGT58uOsoIiHB4/GQkJBAz549KVu2rOs4EiLUOUvQzJ8/n48++oi4uDhq1arlOo5ISHj11VepW7euCrOcRJ2zBEV2djbdu3enTp069OrVy3UcEeeysrIYN24czz77rK7FLH+i4ixBMW/ePFauXMnEiRM5++yzXccRcW7ixIm0bNlShVnypOIsQZGcnAzA5Zdf7jiJiFvp6ekMHz6cAQMGqDBLvrTNWUQkSKy1zJ07l4ceekiFWU5JxVlEJAiOHz9Ojx49uPXWW6lbt67rOBLiVJxFRAIsNTWVVatW0adPH0qVKuU6joQBFWcRkQA6evQovXr1olGjRlSvXt11HAkT2iFM/GbGjBk88MADZGVl/Wmcx+MB0Ok6JaocOnSIHTt28MILL1ChQgXXcSSMqDiL30ycOJFzzz2XRx99NM/xFStW5OKLLw5yKhE3Dh48yIABAxg6dCjnnnuu6zgSZlScxS/S09OZO3cunTt35sUXX3QdR8Spffv2sWvXLuLj4ylfvrzrOBKGtI5R/GLhwoWkpKTQpk0b11FEnEpOTmbw4MHUr19fhVnOmDpn8YtZs2ZRqlQpWrVq5TqKiDO7du1i69atvPrqq9orW4pEnbP4RUJCAi1bttTJ+yVqZWVlMXr0aJo1a6bCLEWmzlmKbNu2baxbt46uXbu6jiLixJYtW/j5558ZMWKE6ygSIdQ5S5HNmjULQNubJSpZa5kxYwZ/+9vfXEeRCKLOWYosISGBCy+8kAYNGriOIhJU69at47vvviM2NtZ1FIkw6pylSNLS0pg3bx5t2rTRifwlqmRnZ7N8+XIeeeQR11EkAqnjDC2VAAAgAElEQVRzliJZsGABqampWqUtUeWnn35izpw5xMXFuY4iEUqdsxRJQkICZcqUISYmxnUUkaA4dOgQhw4d0qpsCSgVZzlj1lq+/PJLbrrpJs466yzXcUQC7vvvv+fNN9+kVatWOk+8BJQ+XXLGpk+fzqZNm+jQoYPrKCIBt27dOs477zyee+4511EkCqg4yxlJTU0lNjaWK664ggcffNB1HJGAmj9/Pl988QWNGjXSjo8SFNohTM7IK6+8wvbt2xk/fjzFixd3HUckYObPn0+jRo1o2bKl6ygSRdQ5S6H9cbWde++9VzuCSUT7/vvvWbVqFdWqVXMdRaKMOmcptD59+pCdnc3IkSNdRxEJmM8++4zrrruO6667znUUiULqnKVQFi9ezMSJE3n22We54IILXMcRCYi1a9eyf/9+qlSp4jqKRCkVZymUGTNmUKpUKfr27es6ikhA/Oc//6F06dI685c4peIshZKdnU3p0qU555xzXEcR8bs9e/ZQrFgxLrroItdRJMqpOIuIAO+++y5JSUl07NjRdRQRFWcRkYMHD3L++efTvHlz11FEAO2tLSJR7vXXX+eyyy6jbdu2rqOInKDiHKUOHDjAihUrCv287du3ByCNiBs7d+6kRYsWtGjRwnUUkZOoOEeh48ePc9VVV51xoa1Zs6afE4kE30svvUSLFi246aabXEcR+RMV5yj08ssvs337dj744IMz2iu1bt26AUglEhzWWpYvX06nTp2oU6eO6zgieVJxjjJJSUm89NJL3HfffXTp0sV1HJGgGz58OC1btlRhlpCm4hxl+vTpg8fjYcSIEa6jiASVx+Nh5syZPPPMM7r+uIQ8HUoVRb7//nsmTZpEbGws9erVcx1HJKjefPNN6tatq8IsYUGdc5hLTU3l6NGjp53u4MGDxMfHU6NGDeLi4oKQTCQ0ZGdn88477/DUU0/pWswSNlScw5jH46FevXrs3bu3wM+ZMGGCTr0pUWXq1KnExMSoMEtYUXEOY9nZ2ezdu5c777yTO+6445TTbty4kZiYGNq1axekdCJuZWRkMGzYMAYOHEixYtqCJ+FFxTkCtGjRgieeeOKU0yQmJhITExOcQCKOeTwe5s+fz0MPPaTCLGFJn1oRiSipqan06NGD66+/Xtccl7ClzllEIsbx48dZt24dvXv31l7ZEtbUOYtIREhOTj5xmKBOMSvhTp1zCFi9ejX33XcfBw8eLNTzrLUA2gtVot6RI0fYtm0bgwYNolKlSq7jiBSZirNj1lqeeuop9u7dS4cOHQr9/OLFi5/R80QixeHDh+nXrx8vvvgiFStWdB1HxC9UnB2bMWMG8+fP5+233+bxxx93HUckrOzfv58dO3YQHx9PhQoVXMcR8Rttc3YoNTWVXr16cfnll/PYY4+5jiMSVlJTUxk0aBANGjRQYZaIo87ZoVdffZXt27fzzTffULx4cddxRMLG7t27WbduHaNGjaJkyZKu44j4nTpnR3bt2sWwYcO45557dLF3kULweDy89tprXHPNNSrMErHUOReBtZZu3bqxZcuWQj83KSmJrKwsRo4cGYBkIpFp27ZtLF68mOHDh7uOIhJQBeqcjTG3G2M2GGM2GWP65DNNB2PMWmPMGmPMJP/GDE3Hjx/nnXfeYdOmTaSlpRXqVqVKFcaMGcOFF17o+m2IhI2PP/6Ye+65x3UMkYA7bedsjCkOvAncCuwElhpjPrfWrs0xTQOgL/BXa+0hY0zVQAUORU8++SSxsbGuY4hErA0bNvD111/Ts2dP11FEgqIgnfPVwCZr7RZrbQYwBWifa5rHgDettYcArLUFv4ahiMgpZGdns2LFCh1qKFGlIMW5JpCUY3in77GcGgINjTGLjDGLjTG3+yugiESvX375hUmTJtGxY0dKlNAuMhI9CvJpz+vckDaP+TQAYoBawHfGmEuttYdPmpExXYGuANWqVSMxMfHEuJSUlJOGw0FqaioAmzdvDvns4bh8w4mWr/8dOXKErVu30r59ey3bANJnN3CKsmwLUpx3ArVzDNcCfstjmsXW2kxgqzFmA95ivTTnRNbascBYgGbNmtmc1xcOx+sNHzt2DICLLroo5LOH4/INJ1q+/rVkyRK+/fZbBg8erGUbYFq+gVOUZVuQ1dpLgQbGmAuMMaWA+4HPc03zKXATgDGmMt7V3IU/vkhEot6aNWuoUKECgwYNch1FxJnTFmdrbRbwFDAbWAdMs9auMca8YIxp55tsNnDAGLMW+BaItdYeCFRoEYlMixYt4vPPP6dhw4a62ppEtQLtYWGtTQAScj02MMd9C/T03URECm3BggU0bNiQ6667ToVZop5O3ykizi1btowVK1ZQvXp1FWYRVJxFxLGZM2dSo0YNunfv7jqKSMjQgYOnYa1lzpw57N69+0/j0tPTHSQSiRybN29m9+7d1KhRw3UUkZCi4nwaU6ZMoVOnTqecpmrVqDpbqYhfTJ06lcsuu4yuXbu6jiISclScT+HYsWP07t2bpk2bMn369Dy3hZUoUYJatWo5SCcSvg4cOEBWVhaNGzd2HUUkJKk4n8LIkSPZuXMnkydP5oILLnAdRyQijB8/nvr16/PAAw+4jiISsrRDWD527NjB8OHD+Z//+R+uv/5613FEIsKRI0eoUqWK/k+JnIY653zExcUBMGLECMdJRCLDW2+9Rf369Wnbtq3rKCIhT8U5DwsXLmTKlCkMHDiQOnXquI4jEvaSkpJo3rw5zZs3dx1FJCxotXYeRo0aRbVq1ejdu7frKCJh75VXXmH9+vUqzCKFoM45D8eOHaNevXqULVvWdRSRsGWtZcmSJdx///3UrJn7EvAicirqnEUkIF599VWysrJUmEXOgDpnEfEray2ffPIJTz75JGXKlHEdRyQsqXMWEb8aO3YsdevWVWEWKQJ1ziLiF9nZ2bz11ls89dRTurKUSBGpcxYRv/j4449p1aqVCrOIH6g4i0iRZGZmMmDAAO6++24uueQS13FEIoKKs4icMY/Hw6JFi3jooYcoUUJbyUT8RcVZRM5IWloaPXr04KqrrqJ+/fqu44hEFP3UFZFCS01NZcOGDfTq1Yty5cq5jiMScdQ5i0ihHDt2jNjYWGrUqEHt2rVdxxGJSOqcRaTAkpOT2bp1KwMGDKBq1aqu44hELHXOIlIgycnJ9OnThxo1alCtWjXXcUQimjpnETmtgwcPsmXLFoYNG0aFChVcxxGJeOqcReSUMjIyGDhwIA0aNFBhFgkSdc4ikq/ff/+dlStX8tprr+k4ZpEgUucsInmy1vL6669z/fXXqzCLBJn+x+XBWus6gohTSUlJJCYmMnToUNdRRKKSOudcrLVs3LhRe6NKVPv000+57777XMcQiVoqzrls2LCBbdu2cccdd7iOIhJ0mzdvZtSoUfzrX//S9ZhFHFJxziUhIQFAxVmiTmZmJitWrOCpp55yHUUk6mmbcy4JCQk0btyYunXruo4iEjRr1qxh2rRpDB482HUUEUGd80lSUlJYsGABbdq0cR1FJGj27t3L4cOHGThwoOsoIuKj4pzDvHnzyMzMVHGWqLF8+XJef/11rrvuOooXL+46joj4qDjnkJCQQLly5fjrX//qOopIwK1evZpy5coxZMgQjDGu44hIDirOPtZaZs2axS233EKpUqVcxxEJqCVLlvDpp5/SoEEDFWaREKTi7LNmzRqSkpK0Slsi3nfffUetWrV47rnnVJhFQpSKs48OoZJo8Msvv7BkyRJq1KihwiwSwlScfWbNmsUVV1xBzZo1XUcRCYiEhAQqVKjAs88+6zqKiJyGijNw5MgRFi5cqFXaErGSkpLYtm2bjt8XCRMqzsCyZcvIysripptuch1FxO+mT5/OgQMH+N///V/XUUSkgFScgaysLADOOeccx0lE/OvIkSOkpqZy5ZVXuo4iIoWg03eKRKgPP/yQmjVr8uCDD7qOIiKFpM5ZJAIdPXqUSpUq0apVK9dRROQMqHMWiTBjxoyhVq1atG3b1nUUETlDKs4iEWT79u00a9aMq666ynUUESkCrdYWiRCjR49m7dq1KswiEUCds0iYs9by/fff06FDB84//3zXcUTED9Q5i4S5119/naysLBVmkQiizlkkTFlr+eijj3j88ccpXbq06zgi4kfqnEXC1Lhx46hbt64Ks0gEUucsEmY8Hg+vv/46zzzzjK4sJRKh1DmLhJkvvviCVq1aqTCLRDAVZ5EwkZWVxYABA7jtttu4/PLLXccRkQBScRYJA9nZ2SxZsoQHH3xQ25hFooCKs0iIy8jIoFevXlx88cU0bNjQdRwRCQLtECYSwtLS0ti4cSPdu3fnvPPOcx1HRIJEnbNIiDp+/DixsbFUqVKFunXruo4jIkEUtZ1zfHw8CQkJABw6dMhxGpGTHTt2jM2bN9OvXz+d+UskCkVt5zxx4kQ2bNhAqVKlqFatGnfffTeNGzd2HUuEY8eO0bt3b6pXr67CLBKlorZzBmjZsiUfffSR6xgiJxw+fJgNGzYwbNgwKlSo4DqOiDgStZ2zSKjJyspi4MCBNGzYUIVZJMpFdecsEir27dvHjz/+yKhRoyhevLjrOCLimDpnEcestbzxxhvExMSoMIsIoM5ZxKldu3Yxe/ZsBg8e7DqKiIQQdc4ijlhr+fzzz+nYsaPrKCISYtQ5iziwdetWpk6dSp8+fVxHEZEQpM5ZJMjS09NZuXIlPXv2dB1FREKUirNIEK1bt47Bgwdz9913U6pUKddxRCREqTiLBMmePXs4cuQIQ4YMcR1FREKcirNIEKxcuZLRo0dz9dVX63ApETmtqCzOe/bsYfv27VSsWNF1FIkCq1evpmzZsgwdOpRixaLyv5yIFFJUflP069fvxAXsRQJpxYoVTJ8+nfr166swi0iBRd23xbJlyxg/fjzdu3enQYMGruNIBFu0aBGVK1fm+eefxxjjOo6IhJGoKs7WWrp3706VKlXo37+/6zgSwdavX8/ChQupXbu2CrOIFFpUFeepU6eyaNEihg0bRvny5V3HkQg1Z84cihUrRlxcnAqziJyRAhVnY8ztxpgNxphNxph8T2lkjPm7McYaY5r5L6J/HD9+nNjYWJo0acLDDz/sOo5EqN9//53169fTsGFD11FEJIyd9vSdxpjiwJvArcBOYKkx5nNr7dpc05UDngZ+DETQoho5ciQ7d+5k0qRJOpRFAuLTTz/l/PPP5+mnn3YdRUTCXEE656uBTdbaLdbaDGAK0D6P6YYAI4A0P+bzi6SkJIYPH06HDh244YYbXMeRCJSamsrRo0dp0aKF6ygiEgEKUpxrAkk5hnf6HjvBGNMEqG2t/cKP2fwmLi4Oay0jRoxwHUUi0OTJk1m1ahVdunRxHUVEIkRBrkqV1x4t9sRIY4oBo4CHTzsjY7oCXQGqVatGYmLiiXEpKSknDfvLqlWrmDx5Mg8++CBbt25l69atfn+NcBCo5Rvtjh07xvbt27n00ku1fANEn93A0vINnKIsW2OtPfUExlwLDLLW3uYb7gtgrY33DVcANgMpvqdUBw4C7ay1y/Kbb7NmzeyyZf8dnZiYSExMzBm9ifx4PB6uvvpq9uzZw4YNGyhbtqxf5x9OArF8o937779PxYoVueuuu7R8A0jLNrC0fAMn97I1xiy31hZoh+mCdM5LgQbGmAuAXcD9QKc/RlprjwCVc7x4ItDrVIU5WD744AOWL1/Ohx9+GNWFWfxvy5YtNG3alCuvvNJ1FBGJQKfd5mytzQKeAmYD64Bp1to1xpgXjDHtAh2wKF544QVatGhBp06dTj+xSAG9+eabrFmzRoVZRAKmIJ0z1toEICHXYwPzmTam6LH8Y//+/dxzzz06p7H4zXfffcd9991H1apVXUcRkQimqiVSQG+//TaZmZkqzCIScAXqnEWimbWWKVOm8Oijj1KyZEnXcUQkCqhzFjmNSZMmUa9ePRVmEQkadc4i+fB4PLz22ms888wzOuWriASVOmeRfMyZM4ebbrpJhVlEgk7FWSSX7Oxs+vfvz4033kiTJk1cxxGRKKTiLJJDdnY2K1as4IEHHuDss892HUdEopSKs4hPZmYmsbGx1K1bl4svvth1HBGJYtohTARIT0/n119/5amnntJxzCLinDpniXppaWnExsZy7rnncuGFF7qOIyKizlmi2/Hjx9m0aRN9+vShRo0aruOIiADqnCWKpaWl0bt3b6pWrarCLCIhRZ2zRKWjR4+yatUqhg0bRvny5V3HERE5iTpniToej4cBAwbQqFEjFWYRCUnqnCWqHDhwgAULFjBq1ChdSlREQpa+nSSqvPXWW9x8880qzCIS0tQ5S1TYs2cPn332GQMGDHAdRUTktCK2ffjhhx9ISUnRXriCtZaZM2fy4IMPuo4iIlIgEdk5ezwennnmGWrUqEG3bt1cxxGHtm/fzoQJE9Qxi0hYicji/OGHH7J06VImTJjAOeec4zqOOJKWlsYvv/xC7969XUcRESmUiFutnZKSQt++fWnRogUPPPCA6zjiyMaNGxk4cCB/+9vfKF26tOs4IiKFEnGdc3x8PLt37+aTTz7RHrlR6rfffuPIkSMMGzYMY4zrOCIihRZR1Wvr1q288sorPPjgg7Ro0cJ1HHFg1apVjB49mqZNm1KiRMT99hSRKBExxfnXX3+ldevWlChRgvj4eNdxxIHVq1dTpkwZ4uPjKV68uOs4IiJnLCKK86JFi7j22ms5fPgwX3/9NTVr1nQdSYJs9erVTJs2jYsuukibM0Qk7IX9t9jUqVO5+eabqVixIosXL+baa691HUmC7IcffqBs2bIMHjxYhVlEIkLYfpNZaxk+fDj3338/zZs354cffuCiiy5yHUuCbMuWLXz77bfUq1dPO3+JSMQIy+KclZXFE088QZ8+fbj//vv5+uuvqVSpkutYEmTz5s3j+PHj9O3bV4VZRCJK2BXn5ORk2rVrx5gxY+jTpw//+c9/KFOmjOtYEmQHDx5k9erVXHrppSrMIhJxwu5Yk4ceeog5c+YwZswYunbt6jqOOPDFF19QoUIFnnnmGddRREQCIqw657lz5/LJJ58wZMgQFeYolZaWxsGDB7nhhhtcRxERCZiw6ZyzsrLo3r07F1xwAT169HAdRxyYNm0aZcqUoUuXLq6jiIgEVNgU57Fjx7JmzRo+/vhjbWOOQkePHqV8+fLcfvvtrqOIiARcWBTngwcPMnDgQG666Sbuuusu13EkyD744APOPvts7rvvPtdRRESCIiyK8+DBgzl06BCvvfaa9syNMr/++itNmzblsssucx1FRCRoQn6HsN27d/Pmm2/StWtXLr/8ctdxJIjGjBnD2rVrVZhFJOqEfOe8d+9esrOzad26tesoEkTffvst9957L5UrV3YdRUQk6EK+c5bo8+6775KZmanCLCJRK+Q7Z4ke1lomTpzIww8/rGsxi0hUU+csIWP69OnUq1dPhVlEop6+BcU5ay2vvvoqTz/9NCVLlnQdR0TEuZAszv/+97+ZMGECAMePH3ecRgLt22+/pWXLlirMIiI+IVmcP/nkEzZv3sy1114LQOPGjbnmmmscpxJ/83g8DBw4kN69e1O+fHnXcUREQkZIFmeASy+9lC+//NJ1DAmQ7OxsVq1axf3336/CLCKSi3YIk6DLzMwkLi6OKlWqcOmll7qOIyISckK2c5bIlJGRwaZNm+jWrRs1a9Z0HUdEJCSpc5agSU9Pp3fv3px99tk0aNDAdRwRkZClzlmCIjU1lY0bNxIbG6uOWUTkNNQ5S8BlZmYSGxtL5cqVVZhFRApAnbMEVHJyMitWrCA+Pp5y5cq5jiMiEhbUOUvAWGsZNGgQjRs3VmEWESkEdc4SEIcOHeLrr79m5MiRFCum34AiIoWhb00JiLFjx9K6dWsVZhGRM6DOWfxq7969TJs2jbi4ONdRRETCltoa8RtrLV9++SX/+Mc/XEcREQlr6pzFL3bu3MnYsWN54YUXXEcREQl76pylyFJTU1m9ejX9+vVzHUVEJCKoOEuRbN68meeee47bbruNMmXKuI4jIhIRVJzljO3cuZMjR44wfPhwjDGu44iIRIyQ2Obs8XhYu3YtHo8HgIMHD+oavyFu3bp1jBs3jmHDhlGiREh8jEREIkZIfKsuWLCAJ5988qTH2rRp4yiNnM6aNWsoVaoU8fHxFC9e3HUcEZGIExLFOSUlBYAxY8bQqFEjAC655BKXkSQf69evZ9KkSQwZMkQnGBERCZCQKM5/aNq0Kc2aNXMdQ/KxZMkSzjvvPF588UVtYxYRCSC1PlIgO3fu5KuvvqJ+/foqzCIiARZSnbOEpvnz51OuXDkGDBigwiwiEgTqnOWUkpOT+emnn2jSpIkKs4hIkKhzlnzNmjWLkiVL0r17d9dRRESiijpnyVNGRgb79u3jlltucR1FRCTqqHOWP/n444/xeDx06dLFdRQRkaik4iwnOXLkCOeccw6tW7d2HUVEJGqpOMsJEydOpFixYnTq1Ml1FBGRqKbiLID3zF9NmzalcePGrqOIiEQ97RAmvPfee6xZs0aFWUQkRKhzjnLz5s3j7rvvpmLFiq6jiIiIjzrnKDZhwgTS09NVmEVEQow65yg1YcIEOnXqpGsxi4iEIHXOUejzzz+nTp06KswiIiGqQMXZGHO7MWaDMWaTMaZPHuN7GmPWGmN+McbMM8bU9X9UKSprLa+88gq33XYbMTExruOIiEg+TlucjTHFgTeBO4DGQEdjTO7den8CmllrLwemAyP8HVSKbtGiRVx//fWULl3adRQRETmFgnTOVwObrLVbrLUZwBSgfc4JrLXfWmuP+wYXA7X8G1OKwuPx8P7773PxxRfTokUL13FEROQ0CrLRsSaQlGN4J3Cqb/hHgFl5jTDGdAW6AlSrVo3ExEQAVq1aBcDy5ctJSUkpQCQpqOzsbHbs2EHz5s1PLGfxv5SUlBOfZ/EvLdvA0vINnKIs24IU57wu4mvznNCYzkAzoGVe4621Y4GxAM2aNbN/bPf8oyBfddVVNGvWrACRpCCysrLo168fTz75JFu3btV25gBKTEzU8g0QLdvA0vINnKIs24Ks1t4J1M4xXAv4LfdExphbgOeAdtba9DNKI36TmZnJpk2beOSRR6hbV/vniYiEk4IU56VAA2PMBcaYUsD9wOc5JzDGNAHG4C3Me/0fUwojIyOD3r17U7JkSf7yl7+4jiMiIoV02tXa1tosY8xTwGygOPC+tXaNMeYFYJm19nNgJHAO8JExBmCHtbZdAHNLPtLS0li/fj29evWiZs2aruOIiMgZKNBZKKy1CUBCrscG5rh/i59zyRnIzs6md+/exMbGqjCLiIQxnSIqQhw7dozFixcTHx9P2bJlXccREZEi0Ok7I8QLL7zApZdeqsIsIhIB1DmHucOHD/Pll1/y0ksv4dveLyIiYU6dc5h77733uOOOO1SYRUQiiDrnMLV//34mTJjAs88+6zqKiIj4mTrnMGSt5auvvuKxxx5zHUVERAJAxTnM/Pbbb/Tr14/OnTtTrlw513FERCQAVJzDyLFjx1i7di0DBw48/cQiIhK2VJzDxLZt2+jXrx+tWrXirLPOch1HREQCSMU5DOzcuZPDhw8zcuRIihXTn0xEJNLpmz7Ebdy4kVGjRnHJJZdQqlQp13FERCQIVJxD2Nq1awEYPnw4JUuWdJxGRESCRcU5RG3evJkJEyZw0UUXUaKEDkcXEYkmKs4haPny5aSnpzNs2DCKFy/uOo6IiASZinOI2bt3LzNnzuTiiy/Wzl8iIlFK60tDyMKFCylRogSDBg1yHUVERBxSaxYiUlNTWbp0KS1atHAdRUREHFPnHAK+/vprMjIy6NGjh+soIiISAtQ5O5aZmcnvv/9O27ZtXUcREZEQoc7Zoc8//5yUlBQ6d+7sOoqIiIQQFWdHDh06RNmyZWnXrp3rKCIiEmJUnB2YMmUKGRkZdOnSxXUUEREJQSrOQbZmzRqaNGnCX/7yF9dRREQkRGmHsCCaMGECa9asUWEWEZFTUuccJHPmzKF9+/ZUqFDBdRQREQlx6pyDYMqUKaSnp6swi4hIgahzDrDx48fzwAMP6JKPIiJSYOqcA+irr76iVq1aKswiIlIo6pwDwFrLK6+8whNPPEHZsmVdxxERkTCjztnPrLUsXbqUa6+9VoVZRETOiIqzH3k8Hp5//nnq1KnDX//6V9dxREQkTKk4+4nH42Hjxo3cddddVK9e3XUcEREJYyrOfpCdnU3fvn0pUaIETZs2dR1HRETCnHYIK6KsrCw2b97MP/7xD+rXr+86joiIRAB1zkWQmZlJ7969McbQqFEj13FERCRCqHM+Q+np6axZs4Znn32WmjVruo4jIiIRRJ3zGfB4PMTFxVGpUiUVZhER8Tt1zoV0/PhxFixYQHx8PGeddZbrOCIiEoHUORfS0KFDueKKK1SYRUQkYNQ5F9DRo0f55JNPePHFFzHGuI4jIiIRTJ1zAY0bN462bduqMIuISMCpcz6NgwcP8u6779K7d2/XUUREJEqocz4Fj8fD119/Tbdu3VxHERGRKKLinI89e/YQFxdHhw4dqFChgus4IiISRVSc85CcnMz69esZNGiQtjGLiEjQqTjnsmPHDvr168f111+v6zGLiIgTKs45JCUlcfjwYV5++WVKlNC+ciIi4oaKs8/mzZsZNWoUjRo1onTp0q7jiIhIFFN7CKxfvx6A4cOHU7JkScdpREQk2kV957xjxw7GjRtHgwYNVJhFRCQkRHXnvHLlSooVK0Z8fDzFikX97xQREQkRUVuRDh8+zCeffMKll16qwiwiIiElKjvnxYsXk5GRweDBg11HERER+ZOoaxkzMjL44YcfuOGGG1xHERERyVNUdc7ffPMNhw8fpkePHq6jiIiI5CtqOufMzEx2797NPffc4zqKiIjIKSmVAuYAAAbySURBVEVF5/zll1+yb98+Hn74YddRRERETivii/P+/fspW7Ysbdu2dR1FRESkQCK6OH/00UckJyfzz3/+03UUERGRAovY4vzLL7/QpEkT6tev7zqKiIhIoUTkDmGTJ09m1apVKswiIhKWIq5znjVrFm3btqV8+fKuo4iIiJyRiCrOM2bMoFixYirMIiIS1iKmOI8fP56OHTvqWswiIhL2ImKb8zfffEP16tVVmEVEJCKEdedsreXVV1/l0UcfpUKFCq7jiIiI+EXYds7WWn755ReaN2+uwiwiIhElLIuztZYhQ4Zw3nnnceONN7qOIyIi4ldht1rb4/GwZcsW7rjjDurUqeM6joiIiN+FVefs8Xjo378/mZmZNG/e3HUcERGRgAibzjk7O5vNmzfTuXNnLr74YtdxREREAiYsOuesrCzi4uLIzs6mcePGruOIiIgEVMh3zpmZmfz88888++yznH/++a7jiIiIBFxId87WWvr06UPFihVVmEVEJGqEbOeclpbG3LlzGTp0KGXKlHEdR0REJGhCtnMeMWIETZo0UWEWEZGoU6DibIy53RizwRizyRjTJ4/xpY0xU33jfzTG1DvTQCkpKbz33nsMGDCAmjVrnulsREREwtZpi7MxpjjwJnAH0BjoaIzJvcv0I8Aha219YBQw/EwDffjhh7Rr1w5jzJnOQkREJKwVpHO+Gthkrd1irc0ApgDtc03THvjAd386cLM5g+r6/vvv88QTT1ClSpXCPlVERCRiFKQ41wSScgzv9D2W5zTW2izgCFCpsGHuu+++wj5FREQk4hRkb+28OmB7BtNgjOkKdAWoVq0aiYmJgPdY5ueff55jx46deEz8KyUlRcs2gLR8A0fLNrC0fAOnKMu2IMV5J1A7x3At4Ld8ptlpjCnx/+3dS2gdZRjG8f/jpYhYayAILrRVaMGSjeUs6kYjikgWcVMkQtFKcVHRhYorF4ruFBEEoUYsoqCoGw2idKENFTFioFjaglC1FkFovXVTFC+vixkknCaZL5dvLuc8PxiYkzMZXh6GefPNTOYDNgG/9u8oIqaBaYBerxfj4+P/fzcyMsLCz7a+ZmdnnW9GzjcfZ5uX881nLdmmXNb+Ctgq6XpJG4ApYKZvmxng/nJ9F/BpRFwwcjYzM7NqlSPniPhb0sPAQeBi4EBEHJf0DDAfETPAa8Cbkk5SjJinchZtZmY2yNTUAFfSWeCHBT8aBX5upJjh4Hzzcr75ONu8nG8+/dlujoikf0dqrDn3kzQfEb2m6xhUzjcv55uPs83L+eazlmxb+/pOMzOzYeXmbGZm1jJtas7TTRcw4JxvXs43H2ebl/PNZ9XZtuaes5mZmRXaNHI2MzMzGmjOdU4/OYwS8n1M0glJRyV9ImlzE3V2UVW2C7bbJSkk+QnYFUjJV9I95fF7XNJbddfYVQnnheskHZJ0pDw3TDRRZxdJOiDpjKRjS3wvSS+V2R+VtCNpxxFR20LxEpNvgRuADcDXwPa+bR4C9pfrU8A7ddbY5SUx39uAy8v1fc53/bItt9sIHAbmgF7TdXdlSTx2twJHgJHy89VN192FJTHbaWBfub4dONV03V1ZgFuAHcCxJb6fAD6mmINiJ/Blyn7rHjnXNv3kkKrMNyIORcT58uMcxbvSrVrKsQvwLPAc8EedxQ2AlHwfBF6OiN8AIuJMzTV2VUq2AVxZrm/iwvkTbAkRcZhF5pJY4G7gjSjMAVdJuqZqv3U359qmnxxSKfkutJfiLzqrVpmtpJuAayPiwzoLGxApx+42YJukzyXNSbqrtuq6LSXbp4Hdkn4EPgIeqae0obDS8zKQNivVelq36SdtUcnZSdoN9IBbs1Y0OJbNVtJFwIvAnroKGjApx+4lFJe2xymu+HwmaSwifs9cW9elZHsv8HpEvCDpZoq5EsYi4t/85Q28VfW0ukfOK5l+kuWmn7RFpeSLpDuAJ4HJiPizptq6rirbjcAYMCvpFMW9pRk/FJYs9dzwQUT8FRHfA99QNGtbXkq2e4F3ASLiC+AyivdC29olnZf71d2cPf1kXpX5lpdeX6FozL5nl27ZbCPiXESMRsSWiNhCcT9/MiLmmym3c1LODe9TPNCIpFGKy9zf1VplN6Vkexq4HUDSjRTN+WytVQ6uGeC+8qntncC5iPip6pdqvawdnn4yq8R8nweuAN4rn7M7HRGTjRXdEYnZ2iol5nsQuFPSCeAf4ImI+KW5qrshMdvHgVclPUpxyXWPB0VpJL1NcatltLxn/xRwKUBE7Ke4hz8BnATOAw8k7df5m5mZtYvfEGZmZtYybs5mZmYt4+ZsZmbWMm7OZmZmLePmbGZm1jJuzmZmZi3j5mxmZtYybs5mZmYt8x9JYsD12C7OHQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_roc(y_test, y_pred, model_name):\n",
    "    fpr, tpr, thr = roc_curve(y_test, y_pred)\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    ax.plot(fpr, tpr, 'k-')\n",
    "    ax.plot([0, 1], [0, 1], 'k--', linewidth=.5)  # roc curve for random model\n",
    "    ax.grid(True)\n",
    "    ax.set(title='ROC Curve for {} on PIMA diabetes problem'.format(model_name),\n",
    "           xlim=[-0.01, 1.01], ylim=[-0.01, 1.01])\n",
    "\n",
    "\n",
    "plot_roc(y_test, y_pred_prob_rf[:, 1], 'RF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Single Hidden Layer Neural Network\n",
    "\n",
    "We will use the Sequential model to quickly build a neural network.  Our first network will be a single layer network.  We have 8 variables, so we set the input shape to 8.  Let's start by having a single hidden layer with 12 nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First let's normalize the data\n",
    "## This aids the training of neural nets by providing numerical stability\n",
    "## Random Forest does not need this as it finds a split only, as opposed to performing matrix multiplications\n",
    "\n",
    "\n",
    "normalizer = StandardScaler()\n",
    "X_train_norm = normalizer.fit_transform(X_train)\n",
    "X_test_norm = normalizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Model \n",
    "# Input size is 8-dimensional\n",
    "# 1 hidden layer, 12 hidden nodes, sigmoid activation\n",
    "# Final layer has just one node with a sigmoid activation (standard for binary classification)\n",
    "\n",
    "model_1 = Sequential([\n",
    "    Dense(12, input_shape=(8,), activation=\"relu\"),\n",
    "    Dense(1, activation=\"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 12)                108       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 121\n",
      "Trainable params: 121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#  This is a nice tool to view the model you have created and count the parameters\n",
    "\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comprehension question:\n",
    "Why do we have 121 parameters?  Does that make sense?\n",
    "\n",
    "\n",
    "Let's fit our model for 200 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 576 samples, validate on 192 samples\n",
      "Epoch 1/200\n",
      "576/576 [==============================] - 0s 702us/step - loss: 1.0712 - acc: 0.3576 - val_loss: 1.1019 - val_acc: 0.3750\n",
      "Epoch 2/200\n",
      "576/576 [==============================] - 0s 91us/step - loss: 1.0335 - acc: 0.3594 - val_loss: 1.0630 - val_acc: 0.3750\n",
      "Epoch 3/200\n",
      "576/576 [==============================] - 0s 84us/step - loss: 0.9988 - acc: 0.3663 - val_loss: 1.0272 - val_acc: 0.3750\n",
      "Epoch 4/200\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.9668 - acc: 0.3646 - val_loss: 0.9941 - val_acc: 0.3750\n",
      "Epoch 5/200\n",
      "576/576 [==============================] - 0s 79us/step - loss: 0.9373 - acc: 0.3698 - val_loss: 0.9637 - val_acc: 0.3802\n",
      "Epoch 6/200\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.9102 - acc: 0.3802 - val_loss: 0.9357 - val_acc: 0.3854\n",
      "Epoch 7/200\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.8853 - acc: 0.3889 - val_loss: 0.9099 - val_acc: 0.3906\n",
      "Epoch 8/200\n",
      "576/576 [==============================] - 0s 88us/step - loss: 0.8624 - acc: 0.3906 - val_loss: 0.8861 - val_acc: 0.3906\n",
      "Epoch 9/200\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.8414 - acc: 0.3941 - val_loss: 0.8642 - val_acc: 0.4062\n",
      "Epoch 10/200\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.8220 - acc: 0.4028 - val_loss: 0.8440 - val_acc: 0.4115\n",
      "Epoch 11/200\n",
      "576/576 [==============================] - 0s 85us/step - loss: 0.8041 - acc: 0.4097 - val_loss: 0.8253 - val_acc: 0.4115\n",
      "Epoch 12/200\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.7877 - acc: 0.4219 - val_loss: 0.8082 - val_acc: 0.4219\n",
      "Epoch 13/200\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.7725 - acc: 0.4375 - val_loss: 0.7923 - val_acc: 0.4531\n",
      "Epoch 14/200\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.7585 - acc: 0.4566 - val_loss: 0.7777 - val_acc: 0.4688\n",
      "Epoch 15/200\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.7457 - acc: 0.4705 - val_loss: 0.7641 - val_acc: 0.4792\n",
      "Epoch 16/200\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.7338 - acc: 0.4826 - val_loss: 0.7516 - val_acc: 0.4896\n",
      "Epoch 17/200\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.7228 - acc: 0.5174 - val_loss: 0.7400 - val_acc: 0.5000\n",
      "Epoch 18/200\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.7127 - acc: 0.5434 - val_loss: 0.7293 - val_acc: 0.4948\n",
      "Epoch 19/200\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.7033 - acc: 0.5625 - val_loss: 0.7193 - val_acc: 0.5052\n",
      "Epoch 20/200\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.6946 - acc: 0.5799 - val_loss: 0.7101 - val_acc: 0.5417\n",
      "Epoch 21/200\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.6866 - acc: 0.5885 - val_loss: 0.7016 - val_acc: 0.5417\n",
      "Epoch 22/200\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.6792 - acc: 0.6076 - val_loss: 0.6936 - val_acc: 0.5312\n",
      "Epoch 23/200\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.6724 - acc: 0.6181 - val_loss: 0.6862 - val_acc: 0.5417\n",
      "Epoch 24/200\n",
      "576/576 [==============================] - 0s 83us/step - loss: 0.6659 - acc: 0.6250 - val_loss: 0.6794 - val_acc: 0.5469\n",
      "Epoch 25/200\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.6599 - acc: 0.6372 - val_loss: 0.6729 - val_acc: 0.5521\n",
      "Epoch 26/200\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.6544 - acc: 0.6545 - val_loss: 0.6669 - val_acc: 0.5625\n",
      "Epoch 27/200\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.6491 - acc: 0.6597 - val_loss: 0.6613 - val_acc: 0.5625\n",
      "Epoch 28/200\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.6442 - acc: 0.6719 - val_loss: 0.6560 - val_acc: 0.5521\n",
      "Epoch 29/200\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.6397 - acc: 0.6771 - val_loss: 0.6511 - val_acc: 0.5677\n",
      "Epoch 30/200\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.6353 - acc: 0.6788 - val_loss: 0.6465 - val_acc: 0.5729\n",
      "Epoch 31/200\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.6313 - acc: 0.6858 - val_loss: 0.6421 - val_acc: 0.5885\n",
      "Epoch 32/200\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.6275 - acc: 0.6910 - val_loss: 0.6380 - val_acc: 0.6042\n",
      "Epoch 33/200\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.6239 - acc: 0.6962 - val_loss: 0.6341 - val_acc: 0.6198\n",
      "Epoch 34/200\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.6205 - acc: 0.6997 - val_loss: 0.6304 - val_acc: 0.6354\n",
      "Epoch 35/200\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.6172 - acc: 0.7031 - val_loss: 0.6269 - val_acc: 0.6458\n",
      "Epoch 36/200\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.6142 - acc: 0.7118 - val_loss: 0.6236 - val_acc: 0.6510\n",
      "Epoch 37/200\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.6113 - acc: 0.7135 - val_loss: 0.6205 - val_acc: 0.6510\n",
      "Epoch 38/200\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.6085 - acc: 0.7118 - val_loss: 0.6175 - val_acc: 0.6615\n",
      "Epoch 39/200\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.6059 - acc: 0.7222 - val_loss: 0.6147 - val_acc: 0.6719\n",
      "Epoch 40/200\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.6033 - acc: 0.7274 - val_loss: 0.6120 - val_acc: 0.6719\n",
      "Epoch 41/200\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.6010 - acc: 0.7274 - val_loss: 0.6094 - val_acc: 0.6823\n",
      "Epoch 42/200\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.5987 - acc: 0.7396 - val_loss: 0.6070 - val_acc: 0.6823\n",
      "Epoch 43/200\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.5966 - acc: 0.7431 - val_loss: 0.6047 - val_acc: 0.6875\n",
      "Epoch 44/200\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.5945 - acc: 0.7396 - val_loss: 0.6025 - val_acc: 0.6875\n",
      "Epoch 45/200\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.5925 - acc: 0.7344 - val_loss: 0.6003 - val_acc: 0.6875\n",
      "Epoch 46/200\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.5905 - acc: 0.7361 - val_loss: 0.5983 - val_acc: 0.7031\n",
      "Epoch 47/200\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.5887 - acc: 0.7344 - val_loss: 0.5963 - val_acc: 0.7031\n",
      "Epoch 48/200\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.5869 - acc: 0.7361 - val_loss: 0.5944 - val_acc: 0.7083\n",
      "Epoch 49/200\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.5852 - acc: 0.7361 - val_loss: 0.5926 - val_acc: 0.7083\n",
      "Epoch 50/200\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.5835 - acc: 0.7396 - val_loss: 0.5908 - val_acc: 0.7031\n",
      "Epoch 51/200\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.5819 - acc: 0.7413 - val_loss: 0.5891 - val_acc: 0.7031\n",
      "Epoch 52/200\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.5803 - acc: 0.7431 - val_loss: 0.5874 - val_acc: 0.7031\n",
      "Epoch 53/200\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.5788 - acc: 0.7431 - val_loss: 0.5858 - val_acc: 0.7031\n",
      "Epoch 54/200\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.5773 - acc: 0.7413 - val_loss: 0.5843 - val_acc: 0.7031\n",
      "Epoch 55/200\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.5760 - acc: 0.7413 - val_loss: 0.5827 - val_acc: 0.7083\n",
      "Epoch 56/200\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.5746 - acc: 0.7413 - val_loss: 0.5813 - val_acc: 0.7083\n",
      "Epoch 57/200\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.5733 - acc: 0.7431 - val_loss: 0.5799 - val_acc: 0.7031\n",
      "Epoch 58/200\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.5719 - acc: 0.7431 - val_loss: 0.5785 - val_acc: 0.6979\n",
      "Epoch 59/200\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.5707 - acc: 0.7413 - val_loss: 0.5771 - val_acc: 0.6979\n",
      "Epoch 60/200\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.5694 - acc: 0.7396 - val_loss: 0.5758 - val_acc: 0.6979\n",
      "Epoch 61/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 63us/step - loss: 0.5682 - acc: 0.7396 - val_loss: 0.5746 - val_acc: 0.7083\n",
      "Epoch 62/200\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.5670 - acc: 0.7413 - val_loss: 0.5733 - val_acc: 0.7083\n",
      "Epoch 63/200\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.5659 - acc: 0.7431 - val_loss: 0.5721 - val_acc: 0.7135\n",
      "Epoch 64/200\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.5648 - acc: 0.7431 - val_loss: 0.5709 - val_acc: 0.7135\n",
      "Epoch 65/200\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.5636 - acc: 0.7431 - val_loss: 0.5698 - val_acc: 0.7135\n",
      "Epoch 66/200\n",
      "576/576 [==============================] - 0s 92us/step - loss: 0.5626 - acc: 0.7431 - val_loss: 0.5686 - val_acc: 0.7188\n",
      "Epoch 67/200\n",
      "576/576 [==============================] - 0s 107us/step - loss: 0.5615 - acc: 0.7431 - val_loss: 0.5675 - val_acc: 0.7188\n",
      "Epoch 68/200\n",
      "576/576 [==============================] - 0s 89us/step - loss: 0.5604 - acc: 0.7448 - val_loss: 0.5664 - val_acc: 0.7240\n",
      "Epoch 69/200\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.5594 - acc: 0.7448 - val_loss: 0.5654 - val_acc: 0.7240\n",
      "Epoch 70/200\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.5584 - acc: 0.7448 - val_loss: 0.5643 - val_acc: 0.7240\n",
      "Epoch 71/200\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.5574 - acc: 0.7448 - val_loss: 0.5633 - val_acc: 0.7240\n",
      "Epoch 72/200\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.5565 - acc: 0.7483 - val_loss: 0.5623 - val_acc: 0.7240\n",
      "Epoch 73/200\n",
      "576/576 [==============================] - 0s 96us/step - loss: 0.5555 - acc: 0.7483 - val_loss: 0.5613 - val_acc: 0.7240\n",
      "Epoch 74/200\n",
      "576/576 [==============================] - 0s 94us/step - loss: 0.5546 - acc: 0.7465 - val_loss: 0.5604 - val_acc: 0.7240\n",
      "Epoch 75/200\n",
      "576/576 [==============================] - 0s 87us/step - loss: 0.5536 - acc: 0.7483 - val_loss: 0.5594 - val_acc: 0.7240\n",
      "Epoch 76/200\n",
      "576/576 [==============================] - 0s 84us/step - loss: 0.5527 - acc: 0.7448 - val_loss: 0.5585 - val_acc: 0.7240\n",
      "Epoch 77/200\n",
      "576/576 [==============================] - 0s 93us/step - loss: 0.5518 - acc: 0.7483 - val_loss: 0.5575 - val_acc: 0.7240\n",
      "Epoch 78/200\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.5509 - acc: 0.7500 - val_loss: 0.5566 - val_acc: 0.7240\n",
      "Epoch 79/200\n",
      "576/576 [==============================] - 0s 85us/step - loss: 0.5500 - acc: 0.7535 - val_loss: 0.5557 - val_acc: 0.7240\n",
      "Epoch 80/200\n",
      "576/576 [==============================] - 0s 90us/step - loss: 0.5492 - acc: 0.7535 - val_loss: 0.5548 - val_acc: 0.7292\n",
      "Epoch 81/200\n",
      "576/576 [==============================] - 0s 79us/step - loss: 0.5483 - acc: 0.7517 - val_loss: 0.5539 - val_acc: 0.7292\n",
      "Epoch 82/200\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.6214 - acc: 0.625 - 0s 79us/step - loss: 0.5474 - acc: 0.7517 - val_loss: 0.5531 - val_acc: 0.7292\n",
      "Epoch 83/200\n",
      "576/576 [==============================] - 0s 94us/step - loss: 0.5466 - acc: 0.7517 - val_loss: 0.5522 - val_acc: 0.7292\n",
      "Epoch 84/200\n",
      "576/576 [==============================] - 0s 92us/step - loss: 0.5458 - acc: 0.7500 - val_loss: 0.5514 - val_acc: 0.7240\n",
      "Epoch 85/200\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.5450 - acc: 0.7483 - val_loss: 0.5506 - val_acc: 0.7240\n",
      "Epoch 86/200\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.5442 - acc: 0.7483 - val_loss: 0.5497 - val_acc: 0.7240\n",
      "Epoch 87/200\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.5434 - acc: 0.7465 - val_loss: 0.5489 - val_acc: 0.7240\n",
      "Epoch 88/200\n",
      "576/576 [==============================] - 0s 90us/step - loss: 0.5425 - acc: 0.7448 - val_loss: 0.5482 - val_acc: 0.7292\n",
      "Epoch 89/200\n",
      "576/576 [==============================] - 0s 90us/step - loss: 0.5417 - acc: 0.7448 - val_loss: 0.5474 - val_acc: 0.7292\n",
      "Epoch 90/200\n",
      "576/576 [==============================] - 0s 86us/step - loss: 0.5410 - acc: 0.7465 - val_loss: 0.5466 - val_acc: 0.7292\n",
      "Epoch 91/200\n",
      "576/576 [==============================] - 0s 93us/step - loss: 0.5402 - acc: 0.7465 - val_loss: 0.5459 - val_acc: 0.7292\n",
      "Epoch 92/200\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.5394 - acc: 0.7500 - val_loss: 0.5451 - val_acc: 0.7292\n",
      "Epoch 93/200\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.5387 - acc: 0.7517 - val_loss: 0.5444 - val_acc: 0.7344\n",
      "Epoch 94/200\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.5379 - acc: 0.7535 - val_loss: 0.5436 - val_acc: 0.7344\n",
      "Epoch 95/200\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.5371 - acc: 0.7535 - val_loss: 0.5429 - val_acc: 0.7344\n",
      "Epoch 96/200\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.5364 - acc: 0.7535 - val_loss: 0.5422 - val_acc: 0.7344\n",
      "Epoch 97/200\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.5357 - acc: 0.7535 - val_loss: 0.5415 - val_acc: 0.7396\n",
      "Epoch 98/200\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.5350 - acc: 0.7535 - val_loss: 0.5408 - val_acc: 0.7396\n",
      "Epoch 99/200\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.5343 - acc: 0.7517 - val_loss: 0.5401 - val_acc: 0.7396\n",
      "Epoch 100/200\n",
      "576/576 [==============================] - 0s 85us/step - loss: 0.5336 - acc: 0.7535 - val_loss: 0.5394 - val_acc: 0.7396\n",
      "Epoch 101/200\n",
      "576/576 [==============================] - 0s 98us/step - loss: 0.5329 - acc: 0.7552 - val_loss: 0.5387 - val_acc: 0.7396\n",
      "Epoch 102/200\n",
      "576/576 [==============================] - 0s 85us/step - loss: 0.5322 - acc: 0.7535 - val_loss: 0.5381 - val_acc: 0.7396\n",
      "Epoch 103/200\n",
      "576/576 [==============================] - 0s 79us/step - loss: 0.5315 - acc: 0.7569 - val_loss: 0.5374 - val_acc: 0.7396\n",
      "Epoch 104/200\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.5309 - acc: 0.7552 - val_loss: 0.5368 - val_acc: 0.7396\n",
      "Epoch 105/200\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.5302 - acc: 0.7569 - val_loss: 0.5362 - val_acc: 0.7396\n",
      "Epoch 106/200\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.5295 - acc: 0.7587 - val_loss: 0.5356 - val_acc: 0.7448\n",
      "Epoch 107/200\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.5289 - acc: 0.7569 - val_loss: 0.5350 - val_acc: 0.7448\n",
      "Epoch 108/200\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.5283 - acc: 0.7569 - val_loss: 0.5344 - val_acc: 0.7448\n",
      "Epoch 109/200\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.5277 - acc: 0.7569 - val_loss: 0.5338 - val_acc: 0.7448\n",
      "Epoch 110/200\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.5271 - acc: 0.7569 - val_loss: 0.5333 - val_acc: 0.7448\n",
      "Epoch 111/200\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.5265 - acc: 0.7569 - val_loss: 0.5327 - val_acc: 0.7448\n",
      "Epoch 112/200\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.5259 - acc: 0.7552 - val_loss: 0.5321 - val_acc: 0.7448\n",
      "Epoch 113/200\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.5253 - acc: 0.7569 - val_loss: 0.5316 - val_acc: 0.7448\n",
      "Epoch 114/200\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.5247 - acc: 0.7552 - val_loss: 0.5311 - val_acc: 0.7448\n",
      "Epoch 115/200\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.5241 - acc: 0.7535 - val_loss: 0.5305 - val_acc: 0.7448\n",
      "Epoch 116/200\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.5236 - acc: 0.7517 - val_loss: 0.5300 - val_acc: 0.7448\n",
      "Epoch 117/200\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.5230 - acc: 0.7517 - val_loss: 0.5295 - val_acc: 0.7396\n",
      "Epoch 118/200\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.5224 - acc: 0.7535 - val_loss: 0.5290 - val_acc: 0.7396\n",
      "Epoch 119/200\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.5219 - acc: 0.7535 - val_loss: 0.5284 - val_acc: 0.7344\n",
      "Epoch 120/200\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.5214 - acc: 0.7535 - val_loss: 0.5279 - val_acc: 0.7344\n",
      "Epoch 121/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 67us/step - loss: 0.5208 - acc: 0.7535 - val_loss: 0.5274 - val_acc: 0.7344\n",
      "Epoch 122/200\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.5203 - acc: 0.7535 - val_loss: 0.5269 - val_acc: 0.7344\n",
      "Epoch 123/200\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.5197 - acc: 0.7517 - val_loss: 0.5265 - val_acc: 0.7344\n",
      "Epoch 124/200\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.5192 - acc: 0.7552 - val_loss: 0.5260 - val_acc: 0.7396\n",
      "Epoch 125/200\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.5187 - acc: 0.7517 - val_loss: 0.5255 - val_acc: 0.7396\n",
      "Epoch 126/200\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.5182 - acc: 0.7500 - val_loss: 0.5250 - val_acc: 0.7396\n",
      "Epoch 127/200\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.5177 - acc: 0.7517 - val_loss: 0.5246 - val_acc: 0.7396\n",
      "Epoch 128/200\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.5173 - acc: 0.7500 - val_loss: 0.5241 - val_acc: 0.7396\n",
      "Epoch 129/200\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.5167 - acc: 0.7500 - val_loss: 0.5237 - val_acc: 0.7396\n",
      "Epoch 130/200\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.5162 - acc: 0.7483 - val_loss: 0.5232 - val_acc: 0.7396\n",
      "Epoch 131/200\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.5158 - acc: 0.7483 - val_loss: 0.5228 - val_acc: 0.7396\n",
      "Epoch 132/200\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.5153 - acc: 0.7500 - val_loss: 0.5224 - val_acc: 0.7396\n",
      "Epoch 133/200\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.5148 - acc: 0.7517 - val_loss: 0.5219 - val_acc: 0.7396\n",
      "Epoch 134/200\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.5144 - acc: 0.7500 - val_loss: 0.5215 - val_acc: 0.7396\n",
      "Epoch 135/200\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.5139 - acc: 0.7517 - val_loss: 0.5211 - val_acc: 0.7396\n",
      "Epoch 136/200\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.5135 - acc: 0.7535 - val_loss: 0.5207 - val_acc: 0.7396\n",
      "Epoch 137/200\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.5130 - acc: 0.7517 - val_loss: 0.5203 - val_acc: 0.7396\n",
      "Epoch 138/200\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.5126 - acc: 0.7517 - val_loss: 0.5199 - val_acc: 0.7396\n",
      "Epoch 139/200\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.5122 - acc: 0.7500 - val_loss: 0.5195 - val_acc: 0.7396\n",
      "Epoch 140/200\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.5118 - acc: 0.7500 - val_loss: 0.5191 - val_acc: 0.7396\n",
      "Epoch 141/200\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.5113 - acc: 0.7517 - val_loss: 0.5187 - val_acc: 0.7396\n",
      "Epoch 142/200\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.5109 - acc: 0.7535 - val_loss: 0.5184 - val_acc: 0.7396\n",
      "Epoch 143/200\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.5104 - acc: 0.7535 - val_loss: 0.5180 - val_acc: 0.7396\n",
      "Epoch 144/200\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.5100 - acc: 0.7535 - val_loss: 0.5176 - val_acc: 0.7396\n",
      "Epoch 145/200\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.5096 - acc: 0.7535 - val_loss: 0.5172 - val_acc: 0.7396\n",
      "Epoch 146/200\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.5092 - acc: 0.7535 - val_loss: 0.5169 - val_acc: 0.7396\n",
      "Epoch 147/200\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.5088 - acc: 0.7535 - val_loss: 0.5165 - val_acc: 0.7396\n",
      "Epoch 148/200\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.5085 - acc: 0.7535 - val_loss: 0.5162 - val_acc: 0.7396\n",
      "Epoch 149/200\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.5081 - acc: 0.7535 - val_loss: 0.5158 - val_acc: 0.7396\n",
      "Epoch 150/200\n",
      "576/576 [==============================] - 0s 84us/step - loss: 0.5077 - acc: 0.7552 - val_loss: 0.5155 - val_acc: 0.7396\n",
      "Epoch 151/200\n",
      "576/576 [==============================] - 0s 93us/step - loss: 0.5073 - acc: 0.7569 - val_loss: 0.5152 - val_acc: 0.7396\n",
      "Epoch 152/200\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.5069 - acc: 0.7569 - val_loss: 0.5148 - val_acc: 0.7396\n",
      "Epoch 153/200\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.5066 - acc: 0.7587 - val_loss: 0.5145 - val_acc: 0.7396\n",
      "Epoch 154/200\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.5062 - acc: 0.7587 - val_loss: 0.5142 - val_acc: 0.7396\n",
      "Epoch 155/200\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.5058 - acc: 0.7587 - val_loss: 0.5138 - val_acc: 0.7396\n",
      "Epoch 156/200\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.5054 - acc: 0.7587 - val_loss: 0.5135 - val_acc: 0.7448\n",
      "Epoch 157/200\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.5051 - acc: 0.7587 - val_loss: 0.5132 - val_acc: 0.7448\n",
      "Epoch 158/200\n",
      "576/576 [==============================] - 0s 85us/step - loss: 0.5047 - acc: 0.7587 - val_loss: 0.5129 - val_acc: 0.7448\n",
      "Epoch 159/200\n",
      "576/576 [==============================] - 0s 91us/step - loss: 0.5043 - acc: 0.7587 - val_loss: 0.5125 - val_acc: 0.7448\n",
      "Epoch 160/200\n",
      "576/576 [==============================] - 0s 87us/step - loss: 0.5040 - acc: 0.7587 - val_loss: 0.5122 - val_acc: 0.7448\n",
      "Epoch 161/200\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.5037 - acc: 0.7587 - val_loss: 0.5119 - val_acc: 0.7448\n",
      "Epoch 162/200\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.5033 - acc: 0.7587 - val_loss: 0.5116 - val_acc: 0.7448\n",
      "Epoch 163/200\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.5029 - acc: 0.7587 - val_loss: 0.5113 - val_acc: 0.7500\n",
      "Epoch 164/200\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.5026 - acc: 0.7604 - val_loss: 0.5110 - val_acc: 0.7500\n",
      "Epoch 165/200\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.5023 - acc: 0.7604 - val_loss: 0.5107 - val_acc: 0.7500\n",
      "Epoch 166/200\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.5019 - acc: 0.7604 - val_loss: 0.5105 - val_acc: 0.7500\n",
      "Epoch 167/200\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.5015 - acc: 0.7604 - val_loss: 0.5102 - val_acc: 0.7500\n",
      "Epoch 168/200\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.5012 - acc: 0.7604 - val_loss: 0.5099 - val_acc: 0.7500\n",
      "Epoch 169/200\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.5008 - acc: 0.7604 - val_loss: 0.5096 - val_acc: 0.7500\n",
      "Epoch 170/200\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.5005 - acc: 0.7604 - val_loss: 0.5093 - val_acc: 0.7500\n",
      "Epoch 171/200\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.5002 - acc: 0.7587 - val_loss: 0.5091 - val_acc: 0.7500\n",
      "Epoch 172/200\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4999 - acc: 0.7604 - val_loss: 0.5088 - val_acc: 0.7552\n",
      "Epoch 173/200\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4995 - acc: 0.7587 - val_loss: 0.5085 - val_acc: 0.7552\n",
      "Epoch 174/200\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4992 - acc: 0.7587 - val_loss: 0.5082 - val_acc: 0.7552\n",
      "Epoch 175/200\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4989 - acc: 0.7587 - val_loss: 0.5080 - val_acc: 0.7552\n",
      "Epoch 176/200\n",
      "576/576 [==============================] - 0s 87us/step - loss: 0.4986 - acc: 0.7604 - val_loss: 0.5077 - val_acc: 0.7552\n",
      "Epoch 177/200\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.4983 - acc: 0.7622 - val_loss: 0.5074 - val_acc: 0.7552\n",
      "Epoch 178/200\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.4979 - acc: 0.7587 - val_loss: 0.5072 - val_acc: 0.7604\n",
      "Epoch 179/200\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.4976 - acc: 0.7604 - val_loss: 0.5069 - val_acc: 0.7604\n",
      "Epoch 180/200\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.4973 - acc: 0.7604 - val_loss: 0.5066 - val_acc: 0.7604\n",
      "Epoch 181/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 70us/step - loss: 0.4970 - acc: 0.7622 - val_loss: 0.5064 - val_acc: 0.7604\n",
      "Epoch 182/200\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4967 - acc: 0.7622 - val_loss: 0.5061 - val_acc: 0.7604\n",
      "Epoch 183/200\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4964 - acc: 0.7622 - val_loss: 0.5058 - val_acc: 0.7604\n",
      "Epoch 184/200\n",
      "576/576 [==============================] - 0s 79us/step - loss: 0.4961 - acc: 0.7622 - val_loss: 0.5056 - val_acc: 0.7604\n",
      "Epoch 185/200\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4958 - acc: 0.7622 - val_loss: 0.5053 - val_acc: 0.7604\n",
      "Epoch 186/200\n",
      "576/576 [==============================] - 0s 88us/step - loss: 0.4955 - acc: 0.7622 - val_loss: 0.5051 - val_acc: 0.7656\n",
      "Epoch 187/200\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.4479 - acc: 0.812 - 0s 84us/step - loss: 0.4952 - acc: 0.7622 - val_loss: 0.5048 - val_acc: 0.7656\n",
      "Epoch 188/200\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.4949 - acc: 0.7622 - val_loss: 0.5046 - val_acc: 0.7656\n",
      "Epoch 189/200\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4946 - acc: 0.7622 - val_loss: 0.5043 - val_acc: 0.7656\n",
      "Epoch 190/200\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4943 - acc: 0.7639 - val_loss: 0.5041 - val_acc: 0.7656\n",
      "Epoch 191/200\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4940 - acc: 0.7622 - val_loss: 0.5038 - val_acc: 0.7656\n",
      "Epoch 192/200\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4937 - acc: 0.7622 - val_loss: 0.5036 - val_acc: 0.7656\n",
      "Epoch 193/200\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4935 - acc: 0.7656 - val_loss: 0.5033 - val_acc: 0.7656\n",
      "Epoch 194/200\n",
      "576/576 [==============================] - 0s 96us/step - loss: 0.4932 - acc: 0.7656 - val_loss: 0.5031 - val_acc: 0.7656\n",
      "Epoch 195/200\n",
      "576/576 [==============================] - 0s 95us/step - loss: 0.4929 - acc: 0.7639 - val_loss: 0.5029 - val_acc: 0.7656\n",
      "Epoch 196/200\n",
      "576/576 [==============================] - 0s 83us/step - loss: 0.4926 - acc: 0.7639 - val_loss: 0.5026 - val_acc: 0.7656\n",
      "Epoch 197/200\n",
      "576/576 [==============================] - 0s 106us/step - loss: 0.4923 - acc: 0.7656 - val_loss: 0.5024 - val_acc: 0.7656\n",
      "Epoch 198/200\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4921 - acc: 0.7674 - val_loss: 0.5022 - val_acc: 0.7656\n",
      "Epoch 199/200\n",
      "576/576 [==============================] - 0s 79us/step - loss: 0.4918 - acc: 0.7674 - val_loss: 0.5020 - val_acc: 0.7656\n",
      "Epoch 200/200\n",
      "576/576 [==============================] - 0s 88us/step - loss: 0.4915 - acc: 0.7674 - val_loss: 0.5018 - val_acc: 0.7656\n"
     ]
    }
   ],
   "source": [
    "# Fit(Train) the Model\n",
    "\n",
    "# Compile the model with Optimizer, Loss Function and Metrics\n",
    "# Roc-Auc is not available in Keras as an off the shelf metric yet, so we will skip it here.\n",
    "\n",
    "model_1.compile(SGD(lr = .003), \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "run_hist_1 = model_1.fit(X_train_norm, y_train, validation_data=(X_test_norm, y_test), epochs=200)\n",
    "# the fit function returns the run history. \n",
    "# It is very convenient, as it contains information about the model fit, iterations etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Like we did for the Random Forest, we generate two kinds of predictions\n",
    "#  One is a hard decision, the other is a probabilitistic score.\n",
    "\n",
    "y_pred_class_nn_1 = model_1.predict_classes(X_test_norm)\n",
    "y_pred_prob_nn_1 = model_1.predict(X_test_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0]], dtype=int32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check out the outputs to get a feel for how keras apis work.\n",
    "y_pred_class_nn_1[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.4922324 ],\n",
       "       [0.5831003 ],\n",
       "       [0.20948234],\n",
       "       [0.29910883],\n",
       "       [0.18098752],\n",
       "       [0.52890146],\n",
       "       [0.2363662 ],\n",
       "       [0.302567  ],\n",
       "       [0.8715758 ],\n",
       "       [0.18558808]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_prob_nn_1[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 0.766\n",
      "roc-auc is 0.805\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print model performance and plot the roc curve\n",
    "print('accuracy is {:.3f}'.format(accuracy_score(y_test,y_pred_class_nn_1)))\n",
    "print('roc-auc is {:.3f}'.format(roc_auc_score(y_test,y_pred_prob_nn_1)))\n",
    "\n",
    "plot_roc(y_test, y_pred_prob_nn_1, 'NN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There may be some variation in exact numbers due to randomness, but you should get results in the same ballpark as the Random Forest - between 75% and 85% accuracy, between .8 and .9 for AUC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the `run_hist_1` object that was created, specifically its `history` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['acc', 'loss', 'val_acc', 'val_loss']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_hist_1.history.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the training loss and the validation loss over the different epochs and see how it looks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1a2737c450>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(run_hist_1.history[\"loss\"],'r', marker='.', label=\"Train Loss\")\n",
    "ax.plot(run_hist_1.history[\"val_loss\"],'b', marker='.', label=\"Validation Loss\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the losses are still going down on both the training set and the validation set.  This suggests that the model might benefit from further training.  Let's train the model a little more and see what happens. Note that it will pick up from where it left off. Train for 1000 more epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 576 samples, validate on 192 samples\n",
      "Epoch 1/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4913 - acc: 0.7674 - val_loss: 0.5016 - val_acc: 0.7656\n",
      "Epoch 2/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4910 - acc: 0.7674 - val_loss: 0.5013 - val_acc: 0.7656\n",
      "Epoch 3/1000\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.4908 - acc: 0.7674 - val_loss: 0.5011 - val_acc: 0.7708\n",
      "Epoch 4/1000\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.4905 - acc: 0.7691 - val_loss: 0.5009 - val_acc: 0.7708\n",
      "Epoch 5/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4902 - acc: 0.7674 - val_loss: 0.5007 - val_acc: 0.7708\n",
      "Epoch 6/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4900 - acc: 0.7691 - val_loss: 0.5005 - val_acc: 0.7708\n",
      "Epoch 7/1000\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.4897 - acc: 0.7691 - val_loss: 0.5003 - val_acc: 0.7708\n",
      "Epoch 8/1000\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.4895 - acc: 0.7691 - val_loss: 0.5001 - val_acc: 0.7708\n",
      "Epoch 9/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4893 - acc: 0.7691 - val_loss: 0.4999 - val_acc: 0.7656\n",
      "Epoch 10/1000\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.4890 - acc: 0.7691 - val_loss: 0.4997 - val_acc: 0.7656\n",
      "Epoch 11/1000\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4888 - acc: 0.7691 - val_loss: 0.4996 - val_acc: 0.7656\n",
      "Epoch 12/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4885 - acc: 0.7691 - val_loss: 0.4994 - val_acc: 0.7656\n",
      "Epoch 13/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4883 - acc: 0.7691 - val_loss: 0.4992 - val_acc: 0.7656\n",
      "Epoch 14/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4881 - acc: 0.7691 - val_loss: 0.4990 - val_acc: 0.7656\n",
      "Epoch 15/1000\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.4879 - acc: 0.7691 - val_loss: 0.4988 - val_acc: 0.7656\n",
      "Epoch 16/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4876 - acc: 0.7691 - val_loss: 0.4986 - val_acc: 0.7604\n",
      "Epoch 17/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4874 - acc: 0.7691 - val_loss: 0.4984 - val_acc: 0.7604\n",
      "Epoch 18/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4872 - acc: 0.7691 - val_loss: 0.4983 - val_acc: 0.7604\n",
      "Epoch 19/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4869 - acc: 0.7691 - val_loss: 0.4981 - val_acc: 0.7604\n",
      "Epoch 20/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4867 - acc: 0.7708 - val_loss: 0.4979 - val_acc: 0.7604\n",
      "Epoch 21/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4865 - acc: 0.7708 - val_loss: 0.4977 - val_acc: 0.7604\n",
      "Epoch 22/1000\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4863 - acc: 0.7708 - val_loss: 0.4976 - val_acc: 0.7604\n",
      "Epoch 23/1000\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.4861 - acc: 0.7726 - val_loss: 0.4974 - val_acc: 0.7604\n",
      "Epoch 24/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4858 - acc: 0.7726 - val_loss: 0.4972 - val_acc: 0.7604\n",
      "Epoch 25/1000\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4856 - acc: 0.7726 - val_loss: 0.4971 - val_acc: 0.7604\n",
      "Epoch 26/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4854 - acc: 0.7726 - val_loss: 0.4969 - val_acc: 0.7604\n",
      "Epoch 27/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4852 - acc: 0.7726 - val_loss: 0.4968 - val_acc: 0.7604\n",
      "Epoch 28/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4850 - acc: 0.7726 - val_loss: 0.4966 - val_acc: 0.7604\n",
      "Epoch 29/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4848 - acc: 0.7726 - val_loss: 0.4964 - val_acc: 0.7604\n",
      "Epoch 30/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4846 - acc: 0.7726 - val_loss: 0.4963 - val_acc: 0.7604\n",
      "Epoch 31/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4844 - acc: 0.7726 - val_loss: 0.4961 - val_acc: 0.7604\n",
      "Epoch 32/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4842 - acc: 0.7726 - val_loss: 0.4960 - val_acc: 0.7604\n",
      "Epoch 33/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4840 - acc: 0.7726 - val_loss: 0.4958 - val_acc: 0.7604\n",
      "Epoch 34/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4838 - acc: 0.7726 - val_loss: 0.4957 - val_acc: 0.7604\n",
      "Epoch 35/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4836 - acc: 0.7726 - val_loss: 0.4955 - val_acc: 0.7604\n",
      "Epoch 36/1000\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.4834 - acc: 0.7726 - val_loss: 0.4953 - val_acc: 0.7604\n",
      "Epoch 37/1000\n",
      "576/576 [==============================] - 0s 97us/step - loss: 0.4832 - acc: 0.7726 - val_loss: 0.4952 - val_acc: 0.7604\n",
      "Epoch 38/1000\n",
      "576/576 [==============================] - 0s 93us/step - loss: 0.4830 - acc: 0.7726 - val_loss: 0.4950 - val_acc: 0.7604\n",
      "Epoch 39/1000\n",
      "576/576 [==============================] - 0s 97us/step - loss: 0.4828 - acc: 0.7726 - val_loss: 0.4949 - val_acc: 0.7604\n",
      "Epoch 40/1000\n",
      "576/576 [==============================] - 0s 87us/step - loss: 0.4826 - acc: 0.7708 - val_loss: 0.4947 - val_acc: 0.7604\n",
      "Epoch 41/1000\n",
      "576/576 [==============================] - 0s 85us/step - loss: 0.4824 - acc: 0.7708 - val_loss: 0.4946 - val_acc: 0.7656\n",
      "Epoch 42/1000\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4823 - acc: 0.7691 - val_loss: 0.4945 - val_acc: 0.7656\n",
      "Epoch 43/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4821 - acc: 0.7708 - val_loss: 0.4943 - val_acc: 0.7656\n",
      "Epoch 44/1000\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.4819 - acc: 0.7691 - val_loss: 0.4942 - val_acc: 0.7656\n",
      "Epoch 45/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4817 - acc: 0.7656 - val_loss: 0.4940 - val_acc: 0.7656\n",
      "Epoch 46/1000\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4815 - acc: 0.7674 - val_loss: 0.4939 - val_acc: 0.7656\n",
      "Epoch 47/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4813 - acc: 0.7674 - val_loss: 0.4937 - val_acc: 0.7656\n",
      "Epoch 48/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4812 - acc: 0.7674 - val_loss: 0.4936 - val_acc: 0.7708\n",
      "Epoch 49/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4809 - acc: 0.7674 - val_loss: 0.4935 - val_acc: 0.7708\n",
      "Epoch 50/1000\n",
      "576/576 [==============================] - 0s 79us/step - loss: 0.4808 - acc: 0.7674 - val_loss: 0.4933 - val_acc: 0.7708\n",
      "Epoch 51/1000\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4806 - acc: 0.7674 - val_loss: 0.4932 - val_acc: 0.7708\n",
      "Epoch 52/1000\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.4804 - acc: 0.7674 - val_loss: 0.4931 - val_acc: 0.7708\n",
      "Epoch 53/1000\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.4802 - acc: 0.7674 - val_loss: 0.4929 - val_acc: 0.7708\n",
      "Epoch 54/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4800 - acc: 0.7674 - val_loss: 0.4928 - val_acc: 0.7708\n",
      "Epoch 55/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4799 - acc: 0.7691 - val_loss: 0.4927 - val_acc: 0.7708\n",
      "Epoch 56/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4797 - acc: 0.7674 - val_loss: 0.4926 - val_acc: 0.7708\n",
      "Epoch 57/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4795 - acc: 0.7674 - val_loss: 0.4924 - val_acc: 0.7708\n",
      "Epoch 58/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4793 - acc: 0.7691 - val_loss: 0.4923 - val_acc: 0.7708\n",
      "Epoch 59/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4792 - acc: 0.7691 - val_loss: 0.4922 - val_acc: 0.7708\n",
      "Epoch 60/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4790 - acc: 0.7674 - val_loss: 0.4921 - val_acc: 0.7708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/1000\n",
      "576/576 [==============================] - 0s 79us/step - loss: 0.4788 - acc: 0.7674 - val_loss: 0.4920 - val_acc: 0.7708\n",
      "Epoch 62/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4787 - acc: 0.7674 - val_loss: 0.4918 - val_acc: 0.7708\n",
      "Epoch 63/1000\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4785 - acc: 0.7674 - val_loss: 0.4917 - val_acc: 0.7708\n",
      "Epoch 64/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4783 - acc: 0.7674 - val_loss: 0.4916 - val_acc: 0.7708\n",
      "Epoch 65/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4782 - acc: 0.7674 - val_loss: 0.4915 - val_acc: 0.7708\n",
      "Epoch 66/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4780 - acc: 0.7674 - val_loss: 0.4914 - val_acc: 0.7708\n",
      "Epoch 67/1000\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4779 - acc: 0.7674 - val_loss: 0.4913 - val_acc: 0.7708\n",
      "Epoch 68/1000\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4777 - acc: 0.7674 - val_loss: 0.4912 - val_acc: 0.7656\n",
      "Epoch 69/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4775 - acc: 0.7674 - val_loss: 0.4911 - val_acc: 0.7656\n",
      "Epoch 70/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4774 - acc: 0.7674 - val_loss: 0.4910 - val_acc: 0.7656\n",
      "Epoch 71/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4773 - acc: 0.7691 - val_loss: 0.4909 - val_acc: 0.7656\n",
      "Epoch 72/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4771 - acc: 0.7674 - val_loss: 0.4907 - val_acc: 0.7656\n",
      "Epoch 73/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4769 - acc: 0.7674 - val_loss: 0.4906 - val_acc: 0.7656\n",
      "Epoch 74/1000\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4768 - acc: 0.7674 - val_loss: 0.4905 - val_acc: 0.7656\n",
      "Epoch 75/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4766 - acc: 0.7674 - val_loss: 0.4904 - val_acc: 0.7656\n",
      "Epoch 76/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4765 - acc: 0.7674 - val_loss: 0.4904 - val_acc: 0.7656\n",
      "Epoch 77/1000\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4764 - acc: 0.7674 - val_loss: 0.4903 - val_acc: 0.7656\n",
      "Epoch 78/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4762 - acc: 0.7674 - val_loss: 0.4902 - val_acc: 0.7656\n",
      "Epoch 79/1000\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4761 - acc: 0.7674 - val_loss: 0.4901 - val_acc: 0.7656\n",
      "Epoch 80/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4759 - acc: 0.7674 - val_loss: 0.4900 - val_acc: 0.7656\n",
      "Epoch 81/1000\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4758 - acc: 0.7674 - val_loss: 0.4899 - val_acc: 0.7656\n",
      "Epoch 82/1000\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.4756 - acc: 0.7674 - val_loss: 0.4898 - val_acc: 0.7656\n",
      "Epoch 83/1000\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4755 - acc: 0.7674 - val_loss: 0.4897 - val_acc: 0.7656\n",
      "Epoch 84/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4753 - acc: 0.7674 - val_loss: 0.4896 - val_acc: 0.7656\n",
      "Epoch 85/1000\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.4752 - acc: 0.7674 - val_loss: 0.4895 - val_acc: 0.7656\n",
      "Epoch 86/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4751 - acc: 0.7674 - val_loss: 0.4894 - val_acc: 0.7656\n",
      "Epoch 87/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4749 - acc: 0.7674 - val_loss: 0.4894 - val_acc: 0.7656\n",
      "Epoch 88/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4748 - acc: 0.7674 - val_loss: 0.4893 - val_acc: 0.7656\n",
      "Epoch 89/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4746 - acc: 0.7674 - val_loss: 0.4892 - val_acc: 0.7656\n",
      "Epoch 90/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4745 - acc: 0.7674 - val_loss: 0.4891 - val_acc: 0.7656\n",
      "Epoch 91/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4744 - acc: 0.7674 - val_loss: 0.4890 - val_acc: 0.7604\n",
      "Epoch 92/1000\n",
      "576/576 [==============================] - 0s 79us/step - loss: 0.4743 - acc: 0.7674 - val_loss: 0.4889 - val_acc: 0.7656\n",
      "Epoch 93/1000\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4741 - acc: 0.7674 - val_loss: 0.4888 - val_acc: 0.7656\n",
      "Epoch 94/1000\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.4740 - acc: 0.7674 - val_loss: 0.4887 - val_acc: 0.7656\n",
      "Epoch 95/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4739 - acc: 0.7674 - val_loss: 0.4886 - val_acc: 0.7656\n",
      "Epoch 96/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4737 - acc: 0.7674 - val_loss: 0.4886 - val_acc: 0.7656\n",
      "Epoch 97/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4736 - acc: 0.7674 - val_loss: 0.4885 - val_acc: 0.7656\n",
      "Epoch 98/1000\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4734 - acc: 0.7674 - val_loss: 0.4884 - val_acc: 0.7656\n",
      "Epoch 99/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4733 - acc: 0.7674 - val_loss: 0.4883 - val_acc: 0.7656\n",
      "Epoch 100/1000\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4732 - acc: 0.7674 - val_loss: 0.4882 - val_acc: 0.7656\n",
      "Epoch 101/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4730 - acc: 0.7691 - val_loss: 0.4881 - val_acc: 0.7656\n",
      "Epoch 102/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4729 - acc: 0.7674 - val_loss: 0.4881 - val_acc: 0.7656\n",
      "Epoch 103/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4728 - acc: 0.7691 - val_loss: 0.4880 - val_acc: 0.7656\n",
      "Epoch 104/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4727 - acc: 0.7691 - val_loss: 0.4879 - val_acc: 0.7656\n",
      "Epoch 105/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4725 - acc: 0.7674 - val_loss: 0.4878 - val_acc: 0.7656\n",
      "Epoch 106/1000\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.4724 - acc: 0.7674 - val_loss: 0.4878 - val_acc: 0.7656\n",
      "Epoch 107/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4723 - acc: 0.7674 - val_loss: 0.4877 - val_acc: 0.7656\n",
      "Epoch 108/1000\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4722 - acc: 0.7674 - val_loss: 0.4876 - val_acc: 0.7708\n",
      "Epoch 109/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4721 - acc: 0.7674 - val_loss: 0.4875 - val_acc: 0.7708\n",
      "Epoch 110/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4719 - acc: 0.7656 - val_loss: 0.4875 - val_acc: 0.7708\n",
      "Epoch 111/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4718 - acc: 0.7656 - val_loss: 0.4874 - val_acc: 0.7708\n",
      "Epoch 112/1000\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4717 - acc: 0.7674 - val_loss: 0.4873 - val_acc: 0.7708\n",
      "Epoch 113/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4716 - acc: 0.7674 - val_loss: 0.4873 - val_acc: 0.7708\n",
      "Epoch 114/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4714 - acc: 0.7656 - val_loss: 0.4872 - val_acc: 0.7708\n",
      "Epoch 115/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4713 - acc: 0.7656 - val_loss: 0.4871 - val_acc: 0.7708\n",
      "Epoch 116/1000\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.4712 - acc: 0.7656 - val_loss: 0.4871 - val_acc: 0.7708\n",
      "Epoch 117/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4711 - acc: 0.7656 - val_loss: 0.4870 - val_acc: 0.7708\n",
      "Epoch 118/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4709 - acc: 0.7656 - val_loss: 0.4869 - val_acc: 0.7708\n",
      "Epoch 119/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4709 - acc: 0.7639 - val_loss: 0.4869 - val_acc: 0.7708\n",
      "Epoch 120/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4707 - acc: 0.7656 - val_loss: 0.4868 - val_acc: 0.7708\n",
      "Epoch 121/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 61us/step - loss: 0.4706 - acc: 0.7656 - val_loss: 0.4867 - val_acc: 0.7708\n",
      "Epoch 122/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4705 - acc: 0.7656 - val_loss: 0.4867 - val_acc: 0.7708\n",
      "Epoch 123/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4704 - acc: 0.7639 - val_loss: 0.4866 - val_acc: 0.7708\n",
      "Epoch 124/1000\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4703 - acc: 0.7639 - val_loss: 0.4866 - val_acc: 0.7708\n",
      "Epoch 125/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4701 - acc: 0.7639 - val_loss: 0.4865 - val_acc: 0.7708\n",
      "Epoch 126/1000\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4700 - acc: 0.7639 - val_loss: 0.4864 - val_acc: 0.7708\n",
      "Epoch 127/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4699 - acc: 0.7674 - val_loss: 0.4864 - val_acc: 0.7708\n",
      "Epoch 128/1000\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.4698 - acc: 0.7656 - val_loss: 0.4863 - val_acc: 0.7708\n",
      "Epoch 129/1000\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.4697 - acc: 0.7656 - val_loss: 0.4862 - val_acc: 0.7708\n",
      "Epoch 130/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4696 - acc: 0.7639 - val_loss: 0.4862 - val_acc: 0.7708\n",
      "Epoch 131/1000\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.4694 - acc: 0.7639 - val_loss: 0.4861 - val_acc: 0.7708\n",
      "Epoch 132/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4693 - acc: 0.7656 - val_loss: 0.4860 - val_acc: 0.7708\n",
      "Epoch 133/1000\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4692 - acc: 0.7656 - val_loss: 0.4860 - val_acc: 0.7708\n",
      "Epoch 134/1000\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4691 - acc: 0.7639 - val_loss: 0.4859 - val_acc: 0.7708\n",
      "Epoch 135/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4690 - acc: 0.7656 - val_loss: 0.4859 - val_acc: 0.7708\n",
      "Epoch 136/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4689 - acc: 0.7656 - val_loss: 0.4858 - val_acc: 0.7708\n",
      "Epoch 137/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4688 - acc: 0.7639 - val_loss: 0.4857 - val_acc: 0.7708\n",
      "Epoch 138/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4687 - acc: 0.7639 - val_loss: 0.4857 - val_acc: 0.7708\n",
      "Epoch 139/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4686 - acc: 0.7639 - val_loss: 0.4856 - val_acc: 0.7708\n",
      "Epoch 140/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4685 - acc: 0.7639 - val_loss: 0.4856 - val_acc: 0.7708\n",
      "Epoch 141/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4684 - acc: 0.7639 - val_loss: 0.4855 - val_acc: 0.7708\n",
      "Epoch 142/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4683 - acc: 0.7639 - val_loss: 0.4854 - val_acc: 0.7708\n",
      "Epoch 143/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4681 - acc: 0.7639 - val_loss: 0.4854 - val_acc: 0.7708\n",
      "Epoch 144/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4680 - acc: 0.7639 - val_loss: 0.4853 - val_acc: 0.7708\n",
      "Epoch 145/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4679 - acc: 0.7639 - val_loss: 0.4853 - val_acc: 0.7708\n",
      "Epoch 146/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4678 - acc: 0.7639 - val_loss: 0.4852 - val_acc: 0.7708\n",
      "Epoch 147/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4677 - acc: 0.7639 - val_loss: 0.4852 - val_acc: 0.7708\n",
      "Epoch 148/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4677 - acc: 0.7639 - val_loss: 0.4851 - val_acc: 0.7708\n",
      "Epoch 149/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4675 - acc: 0.7656 - val_loss: 0.4851 - val_acc: 0.7708\n",
      "Epoch 150/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4674 - acc: 0.7656 - val_loss: 0.4850 - val_acc: 0.7708\n",
      "Epoch 151/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4673 - acc: 0.7656 - val_loss: 0.4850 - val_acc: 0.7708\n",
      "Epoch 152/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4672 - acc: 0.7656 - val_loss: 0.4849 - val_acc: 0.7708\n",
      "Epoch 153/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4671 - acc: 0.7656 - val_loss: 0.4849 - val_acc: 0.7708\n",
      "Epoch 154/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4670 - acc: 0.7656 - val_loss: 0.4848 - val_acc: 0.7708\n",
      "Epoch 155/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4670 - acc: 0.7656 - val_loss: 0.4848 - val_acc: 0.7708\n",
      "Epoch 156/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4669 - acc: 0.7656 - val_loss: 0.4847 - val_acc: 0.7708\n",
      "Epoch 157/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4668 - acc: 0.7656 - val_loss: 0.4847 - val_acc: 0.7708\n",
      "Epoch 158/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4666 - acc: 0.7656 - val_loss: 0.4846 - val_acc: 0.7708\n",
      "Epoch 159/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4666 - acc: 0.7656 - val_loss: 0.4846 - val_acc: 0.7708\n",
      "Epoch 160/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4665 - acc: 0.7656 - val_loss: 0.4845 - val_acc: 0.7708\n",
      "Epoch 161/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4664 - acc: 0.7656 - val_loss: 0.4845 - val_acc: 0.7708\n",
      "Epoch 162/1000\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4663 - acc: 0.7656 - val_loss: 0.4844 - val_acc: 0.7708\n",
      "Epoch 163/1000\n",
      "576/576 [==============================] - 0s 85us/step - loss: 0.4662 - acc: 0.7656 - val_loss: 0.4844 - val_acc: 0.7708\n",
      "Epoch 164/1000\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.4661 - acc: 0.7656 - val_loss: 0.4843 - val_acc: 0.7708\n",
      "Epoch 165/1000\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.4660 - acc: 0.7656 - val_loss: 0.4843 - val_acc: 0.7708\n",
      "Epoch 166/1000\n",
      "576/576 [==============================] - 0s 84us/step - loss: 0.4659 - acc: 0.7656 - val_loss: 0.4843 - val_acc: 0.7708\n",
      "Epoch 167/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4658 - acc: 0.7656 - val_loss: 0.4842 - val_acc: 0.7708\n",
      "Epoch 168/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4657 - acc: 0.7656 - val_loss: 0.4842 - val_acc: 0.7708\n",
      "Epoch 169/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4656 - acc: 0.7656 - val_loss: 0.4841 - val_acc: 0.7708\n",
      "Epoch 170/1000\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.4655 - acc: 0.7656 - val_loss: 0.4841 - val_acc: 0.7708\n",
      "Epoch 171/1000\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4654 - acc: 0.7656 - val_loss: 0.4841 - val_acc: 0.7708\n",
      "Epoch 172/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4654 - acc: 0.7656 - val_loss: 0.4840 - val_acc: 0.7708\n",
      "Epoch 173/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4653 - acc: 0.7656 - val_loss: 0.4840 - val_acc: 0.7760\n",
      "Epoch 174/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4652 - acc: 0.7656 - val_loss: 0.4839 - val_acc: 0.7760\n",
      "Epoch 175/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4651 - acc: 0.7656 - val_loss: 0.4839 - val_acc: 0.7760\n",
      "Epoch 176/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4650 - acc: 0.7656 - val_loss: 0.4838 - val_acc: 0.7760\n",
      "Epoch 177/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4649 - acc: 0.7656 - val_loss: 0.4838 - val_acc: 0.7760\n",
      "Epoch 178/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4648 - acc: 0.7656 - val_loss: 0.4837 - val_acc: 0.7760\n",
      "Epoch 179/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4647 - acc: 0.7656 - val_loss: 0.4837 - val_acc: 0.7760\n",
      "Epoch 180/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4646 - acc: 0.7656 - val_loss: 0.4837 - val_acc: 0.7760\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 181/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4645 - acc: 0.7656 - val_loss: 0.4836 - val_acc: 0.7708\n",
      "Epoch 182/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4644 - acc: 0.7656 - val_loss: 0.4836 - val_acc: 0.7708\n",
      "Epoch 183/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4643 - acc: 0.7656 - val_loss: 0.4835 - val_acc: 0.7708\n",
      "Epoch 184/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4643 - acc: 0.7656 - val_loss: 0.4835 - val_acc: 0.7708\n",
      "Epoch 185/1000\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4641 - acc: 0.7656 - val_loss: 0.4835 - val_acc: 0.7708\n",
      "Epoch 186/1000\n",
      "576/576 [==============================] - 0s 85us/step - loss: 0.4641 - acc: 0.7656 - val_loss: 0.4834 - val_acc: 0.7760\n",
      "Epoch 187/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4640 - acc: 0.7674 - val_loss: 0.4834 - val_acc: 0.7760\n",
      "Epoch 188/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4639 - acc: 0.7656 - val_loss: 0.4834 - val_acc: 0.7760\n",
      "Epoch 189/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4638 - acc: 0.7674 - val_loss: 0.4833 - val_acc: 0.7760\n",
      "Epoch 190/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4637 - acc: 0.7674 - val_loss: 0.4833 - val_acc: 0.7760\n",
      "Epoch 191/1000\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4636 - acc: 0.7674 - val_loss: 0.4833 - val_acc: 0.7760\n",
      "Epoch 192/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4635 - acc: 0.7674 - val_loss: 0.4832 - val_acc: 0.7760\n",
      "Epoch 193/1000\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.4635 - acc: 0.7674 - val_loss: 0.4832 - val_acc: 0.7760\n",
      "Epoch 194/1000\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.4633 - acc: 0.7674 - val_loss: 0.4831 - val_acc: 0.7708\n",
      "Epoch 195/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4633 - acc: 0.7674 - val_loss: 0.4831 - val_acc: 0.7708\n",
      "Epoch 196/1000\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4632 - acc: 0.7674 - val_loss: 0.4831 - val_acc: 0.7708\n",
      "Epoch 197/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4631 - acc: 0.7674 - val_loss: 0.4830 - val_acc: 0.7708\n",
      "Epoch 198/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4630 - acc: 0.7656 - val_loss: 0.4830 - val_acc: 0.7708\n",
      "Epoch 199/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4629 - acc: 0.7656 - val_loss: 0.4830 - val_acc: 0.7708\n",
      "Epoch 200/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4629 - acc: 0.7656 - val_loss: 0.4829 - val_acc: 0.7708\n",
      "Epoch 201/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4628 - acc: 0.7656 - val_loss: 0.4829 - val_acc: 0.7708\n",
      "Epoch 202/1000\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4627 - acc: 0.7656 - val_loss: 0.4829 - val_acc: 0.7708\n",
      "Epoch 203/1000\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4626 - acc: 0.7656 - val_loss: 0.4828 - val_acc: 0.7708\n",
      "Epoch 204/1000\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.4625 - acc: 0.7656 - val_loss: 0.4828 - val_acc: 0.7708\n",
      "Epoch 205/1000\n",
      "576/576 [==============================] - 0s 84us/step - loss: 0.4624 - acc: 0.7656 - val_loss: 0.4828 - val_acc: 0.7708\n",
      "Epoch 206/1000\n",
      "576/576 [==============================] - 0s 89us/step - loss: 0.4623 - acc: 0.7674 - val_loss: 0.4827 - val_acc: 0.7708\n",
      "Epoch 207/1000\n",
      "576/576 [==============================] - 0s 84us/step - loss: 0.4622 - acc: 0.7674 - val_loss: 0.4827 - val_acc: 0.7708\n",
      "Epoch 208/1000\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4622 - acc: 0.7656 - val_loss: 0.4826 - val_acc: 0.7708\n",
      "Epoch 209/1000\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4621 - acc: 0.7656 - val_loss: 0.4826 - val_acc: 0.7708\n",
      "Epoch 210/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4620 - acc: 0.7656 - val_loss: 0.4826 - val_acc: 0.7708\n",
      "Epoch 211/1000\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4619 - acc: 0.7656 - val_loss: 0.4825 - val_acc: 0.7708\n",
      "Epoch 212/1000\n",
      "576/576 [==============================] - 0s 84us/step - loss: 0.4618 - acc: 0.7656 - val_loss: 0.4825 - val_acc: 0.7708\n",
      "Epoch 213/1000\n",
      "576/576 [==============================] - 0s 83us/step - loss: 0.4617 - acc: 0.7656 - val_loss: 0.4825 - val_acc: 0.7708\n",
      "Epoch 214/1000\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.4616 - acc: 0.7656 - val_loss: 0.4824 - val_acc: 0.7656\n",
      "Epoch 215/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4615 - acc: 0.7674 - val_loss: 0.4824 - val_acc: 0.7656\n",
      "Epoch 216/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4614 - acc: 0.7656 - val_loss: 0.4823 - val_acc: 0.7656\n",
      "Epoch 217/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4614 - acc: 0.7656 - val_loss: 0.4823 - val_acc: 0.7604\n",
      "Epoch 218/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4613 - acc: 0.7656 - val_loss: 0.4823 - val_acc: 0.7604\n",
      "Epoch 219/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4612 - acc: 0.7674 - val_loss: 0.4823 - val_acc: 0.7604\n",
      "Epoch 220/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4611 - acc: 0.7674 - val_loss: 0.4822 - val_acc: 0.7604\n",
      "Epoch 221/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4610 - acc: 0.7656 - val_loss: 0.4822 - val_acc: 0.7604\n",
      "Epoch 222/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4609 - acc: 0.7656 - val_loss: 0.4822 - val_acc: 0.7604\n",
      "Epoch 223/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4608 - acc: 0.7674 - val_loss: 0.4821 - val_acc: 0.7604\n",
      "Epoch 224/1000\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.4607 - acc: 0.7691 - val_loss: 0.4821 - val_acc: 0.7604\n",
      "Epoch 225/1000\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4607 - acc: 0.7674 - val_loss: 0.4821 - val_acc: 0.7604\n",
      "Epoch 226/1000\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4606 - acc: 0.7691 - val_loss: 0.4821 - val_acc: 0.7604\n",
      "Epoch 227/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4605 - acc: 0.7674 - val_loss: 0.4820 - val_acc: 0.7604\n",
      "Epoch 228/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4604 - acc: 0.7674 - val_loss: 0.4820 - val_acc: 0.7604\n",
      "Epoch 229/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4603 - acc: 0.7674 - val_loss: 0.4820 - val_acc: 0.7604\n",
      "Epoch 230/1000\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4603 - acc: 0.7691 - val_loss: 0.4819 - val_acc: 0.7604\n",
      "Epoch 231/1000\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4601 - acc: 0.7674 - val_loss: 0.4819 - val_acc: 0.7604\n",
      "Epoch 232/1000\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4601 - acc: 0.7691 - val_loss: 0.4819 - val_acc: 0.7604\n",
      "Epoch 233/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4600 - acc: 0.7708 - val_loss: 0.4819 - val_acc: 0.7552\n",
      "Epoch 234/1000\n",
      "576/576 [==============================] - 0s 92us/step - loss: 0.4599 - acc: 0.7691 - val_loss: 0.4818 - val_acc: 0.7552\n",
      "Epoch 235/1000\n",
      "576/576 [==============================] - 0s 96us/step - loss: 0.4598 - acc: 0.7691 - val_loss: 0.4818 - val_acc: 0.7552\n",
      "Epoch 236/1000\n",
      "576/576 [==============================] - 0s 84us/step - loss: 0.4597 - acc: 0.7691 - val_loss: 0.4818 - val_acc: 0.7552\n",
      "Epoch 237/1000\n",
      "576/576 [==============================] - 0s 87us/step - loss: 0.4597 - acc: 0.7708 - val_loss: 0.4818 - val_acc: 0.7552\n",
      "Epoch 238/1000\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.4596 - acc: 0.7708 - val_loss: 0.4817 - val_acc: 0.7552\n",
      "Epoch 239/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4595 - acc: 0.7691 - val_loss: 0.4817 - val_acc: 0.7552\n",
      "Epoch 240/1000\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4595 - acc: 0.7691 - val_loss: 0.4817 - val_acc: 0.7552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 241/1000\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4594 - acc: 0.7691 - val_loss: 0.4817 - val_acc: 0.7552\n",
      "Epoch 242/1000\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4593 - acc: 0.7708 - val_loss: 0.4816 - val_acc: 0.7552\n",
      "Epoch 243/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4592 - acc: 0.7691 - val_loss: 0.4816 - val_acc: 0.7552\n",
      "Epoch 244/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4591 - acc: 0.7691 - val_loss: 0.4816 - val_acc: 0.7552\n",
      "Epoch 245/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4590 - acc: 0.7708 - val_loss: 0.4816 - val_acc: 0.7552\n",
      "Epoch 246/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4590 - acc: 0.7708 - val_loss: 0.4816 - val_acc: 0.7552\n",
      "Epoch 247/1000\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4589 - acc: 0.7708 - val_loss: 0.4815 - val_acc: 0.7552\n",
      "Epoch 248/1000\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.4588 - acc: 0.7708 - val_loss: 0.4815 - val_acc: 0.7552\n",
      "Epoch 249/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4588 - acc: 0.7726 - val_loss: 0.4815 - val_acc: 0.7552\n",
      "Epoch 250/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4587 - acc: 0.7708 - val_loss: 0.4815 - val_acc: 0.7552\n",
      "Epoch 251/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4586 - acc: 0.7708 - val_loss: 0.4815 - val_acc: 0.7552\n",
      "Epoch 252/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4585 - acc: 0.7708 - val_loss: 0.4814 - val_acc: 0.7552\n",
      "Epoch 253/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4584 - acc: 0.7708 - val_loss: 0.4814 - val_acc: 0.7552\n",
      "Epoch 254/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4584 - acc: 0.7726 - val_loss: 0.4814 - val_acc: 0.7552\n",
      "Epoch 255/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4583 - acc: 0.7743 - val_loss: 0.4814 - val_acc: 0.7552\n",
      "Epoch 256/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4582 - acc: 0.7726 - val_loss: 0.4814 - val_acc: 0.7552\n",
      "Epoch 257/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4581 - acc: 0.7726 - val_loss: 0.4813 - val_acc: 0.7552\n",
      "Epoch 258/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4581 - acc: 0.7726 - val_loss: 0.4813 - val_acc: 0.7552\n",
      "Epoch 259/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4580 - acc: 0.7726 - val_loss: 0.4813 - val_acc: 0.7552\n",
      "Epoch 260/1000\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.4580 - acc: 0.7726 - val_loss: 0.4813 - val_acc: 0.7552\n",
      "Epoch 261/1000\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.4579 - acc: 0.7726 - val_loss: 0.4813 - val_acc: 0.7552\n",
      "Epoch 262/1000\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4578 - acc: 0.7726 - val_loss: 0.4812 - val_acc: 0.7552\n",
      "Epoch 263/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4578 - acc: 0.7726 - val_loss: 0.4812 - val_acc: 0.7552\n",
      "Epoch 264/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4577 - acc: 0.7743 - val_loss: 0.4812 - val_acc: 0.7552\n",
      "Epoch 265/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4576 - acc: 0.7726 - val_loss: 0.4812 - val_acc: 0.7552\n",
      "Epoch 266/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4575 - acc: 0.7726 - val_loss: 0.4812 - val_acc: 0.7552\n",
      "Epoch 267/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4574 - acc: 0.7743 - val_loss: 0.4811 - val_acc: 0.7552\n",
      "Epoch 268/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4574 - acc: 0.7743 - val_loss: 0.4811 - val_acc: 0.7552\n",
      "Epoch 269/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4573 - acc: 0.7743 - val_loss: 0.4811 - val_acc: 0.7552\n",
      "Epoch 270/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4572 - acc: 0.7743 - val_loss: 0.4811 - val_acc: 0.7500\n",
      "Epoch 271/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4571 - acc: 0.7726 - val_loss: 0.4811 - val_acc: 0.7500\n",
      "Epoch 272/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4571 - acc: 0.7743 - val_loss: 0.4810 - val_acc: 0.7500\n",
      "Epoch 273/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4571 - acc: 0.7760 - val_loss: 0.4810 - val_acc: 0.7500\n",
      "Epoch 274/1000\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4570 - acc: 0.7743 - val_loss: 0.4810 - val_acc: 0.7500\n",
      "Epoch 275/1000\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4569 - acc: 0.7743 - val_loss: 0.4810 - val_acc: 0.7500\n",
      "Epoch 276/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4569 - acc: 0.7726 - val_loss: 0.4810 - val_acc: 0.7500\n",
      "Epoch 277/1000\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4568 - acc: 0.7708 - val_loss: 0.4809 - val_acc: 0.7500\n",
      "Epoch 278/1000\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4567 - acc: 0.7760 - val_loss: 0.4809 - val_acc: 0.7500\n",
      "Epoch 279/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4566 - acc: 0.7760 - val_loss: 0.4809 - val_acc: 0.7500\n",
      "Epoch 280/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4566 - acc: 0.7743 - val_loss: 0.4809 - val_acc: 0.7500\n",
      "Epoch 281/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4565 - acc: 0.7760 - val_loss: 0.4809 - val_acc: 0.7500\n",
      "Epoch 282/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4564 - acc: 0.7743 - val_loss: 0.4809 - val_acc: 0.7500\n",
      "Epoch 283/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4564 - acc: 0.7726 - val_loss: 0.4809 - val_acc: 0.7500\n",
      "Epoch 284/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4563 - acc: 0.7743 - val_loss: 0.4808 - val_acc: 0.7500\n",
      "Epoch 285/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4563 - acc: 0.7743 - val_loss: 0.4808 - val_acc: 0.7500\n",
      "Epoch 286/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4562 - acc: 0.7743 - val_loss: 0.4808 - val_acc: 0.7500\n",
      "Epoch 287/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4561 - acc: 0.7743 - val_loss: 0.4808 - val_acc: 0.7500\n",
      "Epoch 288/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4561 - acc: 0.7743 - val_loss: 0.4808 - val_acc: 0.7500\n",
      "Epoch 289/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4560 - acc: 0.7743 - val_loss: 0.4808 - val_acc: 0.7500\n",
      "Epoch 290/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4559 - acc: 0.7743 - val_loss: 0.4808 - val_acc: 0.7500\n",
      "Epoch 291/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4559 - acc: 0.7743 - val_loss: 0.4807 - val_acc: 0.7500\n",
      "Epoch 292/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4558 - acc: 0.7743 - val_loss: 0.4807 - val_acc: 0.7500\n",
      "Epoch 293/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4557 - acc: 0.7760 - val_loss: 0.4807 - val_acc: 0.7500\n",
      "Epoch 294/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4557 - acc: 0.7760 - val_loss: 0.4807 - val_acc: 0.7500\n",
      "Epoch 295/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4556 - acc: 0.7743 - val_loss: 0.4807 - val_acc: 0.7500\n",
      "Epoch 296/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4555 - acc: 0.7760 - val_loss: 0.4807 - val_acc: 0.7500\n",
      "Epoch 297/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4555 - acc: 0.7760 - val_loss: 0.4807 - val_acc: 0.7500\n",
      "Epoch 298/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4554 - acc: 0.7760 - val_loss: 0.4806 - val_acc: 0.7500\n",
      "Epoch 299/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4553 - acc: 0.7760 - val_loss: 0.4806 - val_acc: 0.7500\n",
      "Epoch 300/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4553 - acc: 0.7760 - val_loss: 0.4806 - val_acc: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 301/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4552 - acc: 0.7760 - val_loss: 0.4806 - val_acc: 0.7500\n",
      "Epoch 302/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4552 - acc: 0.7760 - val_loss: 0.4806 - val_acc: 0.7500\n",
      "Epoch 303/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4551 - acc: 0.7760 - val_loss: 0.4806 - val_acc: 0.7500\n",
      "Epoch 304/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4550 - acc: 0.7760 - val_loss: 0.4806 - val_acc: 0.7500\n",
      "Epoch 305/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4550 - acc: 0.7760 - val_loss: 0.4805 - val_acc: 0.7500\n",
      "Epoch 306/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4549 - acc: 0.7760 - val_loss: 0.4805 - val_acc: 0.7500\n",
      "Epoch 307/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4549 - acc: 0.7760 - val_loss: 0.4805 - val_acc: 0.7500\n",
      "Epoch 308/1000\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4548 - acc: 0.7760 - val_loss: 0.4805 - val_acc: 0.7500\n",
      "Epoch 309/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4547 - acc: 0.7778 - val_loss: 0.4805 - val_acc: 0.7500\n",
      "Epoch 310/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4547 - acc: 0.7778 - val_loss: 0.4805 - val_acc: 0.7500\n",
      "Epoch 311/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4546 - acc: 0.7778 - val_loss: 0.4804 - val_acc: 0.7500\n",
      "Epoch 312/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4546 - acc: 0.7778 - val_loss: 0.4804 - val_acc: 0.7500\n",
      "Epoch 313/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4545 - acc: 0.7778 - val_loss: 0.4804 - val_acc: 0.7500\n",
      "Epoch 314/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4544 - acc: 0.7778 - val_loss: 0.4804 - val_acc: 0.7500\n",
      "Epoch 315/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4544 - acc: 0.7778 - val_loss: 0.4804 - val_acc: 0.7500\n",
      "Epoch 316/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4543 - acc: 0.7778 - val_loss: 0.4804 - val_acc: 0.7500\n",
      "Epoch 317/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4543 - acc: 0.7778 - val_loss: 0.4803 - val_acc: 0.7500\n",
      "Epoch 318/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4542 - acc: 0.7778 - val_loss: 0.4803 - val_acc: 0.7500\n",
      "Epoch 319/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4541 - acc: 0.7778 - val_loss: 0.4803 - val_acc: 0.7500\n",
      "Epoch 320/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4541 - acc: 0.7778 - val_loss: 0.4803 - val_acc: 0.7500\n",
      "Epoch 321/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4540 - acc: 0.7778 - val_loss: 0.4803 - val_acc: 0.7500\n",
      "Epoch 322/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4539 - acc: 0.7778 - val_loss: 0.4803 - val_acc: 0.7500\n",
      "Epoch 323/1000\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4539 - acc: 0.7778 - val_loss: 0.4803 - val_acc: 0.7500\n",
      "Epoch 324/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4538 - acc: 0.7778 - val_loss: 0.4802 - val_acc: 0.7500\n",
      "Epoch 325/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4537 - acc: 0.7778 - val_loss: 0.4802 - val_acc: 0.7500\n",
      "Epoch 326/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4536 - acc: 0.7778 - val_loss: 0.4802 - val_acc: 0.7500\n",
      "Epoch 327/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4536 - acc: 0.7778 - val_loss: 0.4802 - val_acc: 0.7500\n",
      "Epoch 328/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4535 - acc: 0.7778 - val_loss: 0.4802 - val_acc: 0.7500\n",
      "Epoch 329/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4534 - acc: 0.7778 - val_loss: 0.4802 - val_acc: 0.7500\n",
      "Epoch 330/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4534 - acc: 0.7778 - val_loss: 0.4801 - val_acc: 0.7500\n",
      "Epoch 331/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4533 - acc: 0.7778 - val_loss: 0.4801 - val_acc: 0.7500\n",
      "Epoch 332/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4532 - acc: 0.7778 - val_loss: 0.4801 - val_acc: 0.7500\n",
      "Epoch 333/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4532 - acc: 0.7778 - val_loss: 0.4801 - val_acc: 0.7500\n",
      "Epoch 334/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4531 - acc: 0.7778 - val_loss: 0.4801 - val_acc: 0.7500\n",
      "Epoch 335/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4530 - acc: 0.7778 - val_loss: 0.4801 - val_acc: 0.7500\n",
      "Epoch 336/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4530 - acc: 0.7778 - val_loss: 0.4801 - val_acc: 0.7500\n",
      "Epoch 337/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4529 - acc: 0.7778 - val_loss: 0.4800 - val_acc: 0.7500\n",
      "Epoch 338/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4529 - acc: 0.7778 - val_loss: 0.4800 - val_acc: 0.7500\n",
      "Epoch 339/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4528 - acc: 0.7778 - val_loss: 0.4800 - val_acc: 0.7500\n",
      "Epoch 340/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4527 - acc: 0.7778 - val_loss: 0.4800 - val_acc: 0.7500\n",
      "Epoch 341/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4527 - acc: 0.7778 - val_loss: 0.4800 - val_acc: 0.7500\n",
      "Epoch 342/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4527 - acc: 0.7778 - val_loss: 0.4800 - val_acc: 0.7500\n",
      "Epoch 343/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4526 - acc: 0.7778 - val_loss: 0.4799 - val_acc: 0.7500\n",
      "Epoch 344/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4525 - acc: 0.7778 - val_loss: 0.4799 - val_acc: 0.7500\n",
      "Epoch 345/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4524 - acc: 0.7778 - val_loss: 0.4799 - val_acc: 0.7500\n",
      "Epoch 346/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4524 - acc: 0.7778 - val_loss: 0.4799 - val_acc: 0.7500\n",
      "Epoch 347/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4523 - acc: 0.7778 - val_loss: 0.4799 - val_acc: 0.7500\n",
      "Epoch 348/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4523 - acc: 0.7778 - val_loss: 0.4799 - val_acc: 0.7500\n",
      "Epoch 349/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4522 - acc: 0.7778 - val_loss: 0.4798 - val_acc: 0.7500\n",
      "Epoch 350/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4521 - acc: 0.7778 - val_loss: 0.4798 - val_acc: 0.7500\n",
      "Epoch 351/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4521 - acc: 0.7778 - val_loss: 0.4798 - val_acc: 0.7500\n",
      "Epoch 352/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4520 - acc: 0.7778 - val_loss: 0.4798 - val_acc: 0.7500\n",
      "Epoch 353/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4519 - acc: 0.7778 - val_loss: 0.4798 - val_acc: 0.7500\n",
      "Epoch 354/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4519 - acc: 0.7778 - val_loss: 0.4798 - val_acc: 0.7500\n",
      "Epoch 355/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4518 - acc: 0.7778 - val_loss: 0.4798 - val_acc: 0.7500\n",
      "Epoch 356/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4518 - acc: 0.7778 - val_loss: 0.4797 - val_acc: 0.7500\n",
      "Epoch 357/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4517 - acc: 0.7778 - val_loss: 0.4797 - val_acc: 0.7500\n",
      "Epoch 358/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4517 - acc: 0.7778 - val_loss: 0.4797 - val_acc: 0.7500\n",
      "Epoch 359/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4516 - acc: 0.7778 - val_loss: 0.4797 - val_acc: 0.7500\n",
      "Epoch 360/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4516 - acc: 0.7778 - val_loss: 0.4797 - val_acc: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 361/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4515 - acc: 0.7778 - val_loss: 0.4797 - val_acc: 0.7500\n",
      "Epoch 362/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4515 - acc: 0.7778 - val_loss: 0.4797 - val_acc: 0.7500\n",
      "Epoch 363/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4514 - acc: 0.7778 - val_loss: 0.4797 - val_acc: 0.7500\n",
      "Epoch 364/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4513 - acc: 0.7778 - val_loss: 0.4797 - val_acc: 0.7500\n",
      "Epoch 365/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4513 - acc: 0.7778 - val_loss: 0.4796 - val_acc: 0.7500\n",
      "Epoch 366/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4512 - acc: 0.7778 - val_loss: 0.4796 - val_acc: 0.7500\n",
      "Epoch 367/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4512 - acc: 0.7778 - val_loss: 0.4796 - val_acc: 0.7500\n",
      "Epoch 368/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4511 - acc: 0.7778 - val_loss: 0.4796 - val_acc: 0.7500\n",
      "Epoch 369/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4511 - acc: 0.7778 - val_loss: 0.4796 - val_acc: 0.7500\n",
      "Epoch 370/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4510 - acc: 0.7778 - val_loss: 0.4796 - val_acc: 0.7500\n",
      "Epoch 371/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4509 - acc: 0.7778 - val_loss: 0.4796 - val_acc: 0.7500\n",
      "Epoch 372/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4509 - acc: 0.7778 - val_loss: 0.4796 - val_acc: 0.7500\n",
      "Epoch 373/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4508 - acc: 0.7778 - val_loss: 0.4796 - val_acc: 0.7500\n",
      "Epoch 374/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4507 - acc: 0.7778 - val_loss: 0.4796 - val_acc: 0.7500\n",
      "Epoch 375/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4507 - acc: 0.7778 - val_loss: 0.4796 - val_acc: 0.7500\n",
      "Epoch 376/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4507 - acc: 0.7778 - val_loss: 0.4795 - val_acc: 0.7500\n",
      "Epoch 377/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4506 - acc: 0.7778 - val_loss: 0.4795 - val_acc: 0.7500\n",
      "Epoch 378/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4505 - acc: 0.7778 - val_loss: 0.4795 - val_acc: 0.7500\n",
      "Epoch 379/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4505 - acc: 0.7778 - val_loss: 0.4795 - val_acc: 0.7500\n",
      "Epoch 380/1000\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4504 - acc: 0.7778 - val_loss: 0.4795 - val_acc: 0.7500\n",
      "Epoch 381/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4504 - acc: 0.7778 - val_loss: 0.4795 - val_acc: 0.7500\n",
      "Epoch 382/1000\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.4503 - acc: 0.7760 - val_loss: 0.4795 - val_acc: 0.7500\n",
      "Epoch 383/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4503 - acc: 0.7778 - val_loss: 0.4795 - val_acc: 0.7500\n",
      "Epoch 384/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4502 - acc: 0.7760 - val_loss: 0.4795 - val_acc: 0.7500\n",
      "Epoch 385/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4502 - acc: 0.7778 - val_loss: 0.4795 - val_acc: 0.7500\n",
      "Epoch 386/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4501 - acc: 0.7778 - val_loss: 0.4795 - val_acc: 0.7500\n",
      "Epoch 387/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4500 - acc: 0.7760 - val_loss: 0.4795 - val_acc: 0.7500\n",
      "Epoch 388/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4500 - acc: 0.7778 - val_loss: 0.4795 - val_acc: 0.7500\n",
      "Epoch 389/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4500 - acc: 0.7778 - val_loss: 0.4795 - val_acc: 0.7500\n",
      "Epoch 390/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4499 - acc: 0.7778 - val_loss: 0.4795 - val_acc: 0.7500\n",
      "Epoch 391/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4498 - acc: 0.7760 - val_loss: 0.4795 - val_acc: 0.7500\n",
      "Epoch 392/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4498 - acc: 0.7778 - val_loss: 0.4795 - val_acc: 0.7500\n",
      "Epoch 393/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4497 - acc: 0.7760 - val_loss: 0.4795 - val_acc: 0.7500\n",
      "Epoch 394/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4497 - acc: 0.7760 - val_loss: 0.4795 - val_acc: 0.7500\n",
      "Epoch 395/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4496 - acc: 0.7760 - val_loss: 0.4795 - val_acc: 0.7500\n",
      "Epoch 396/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4496 - acc: 0.7760 - val_loss: 0.4794 - val_acc: 0.7500\n",
      "Epoch 397/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4495 - acc: 0.7760 - val_loss: 0.4794 - val_acc: 0.7500\n",
      "Epoch 398/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4495 - acc: 0.7760 - val_loss: 0.4794 - val_acc: 0.7500\n",
      "Epoch 399/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4495 - acc: 0.7760 - val_loss: 0.4794 - val_acc: 0.7500\n",
      "Epoch 400/1000\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.4494 - acc: 0.7760 - val_loss: 0.4794 - val_acc: 0.7500\n",
      "Epoch 401/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4494 - acc: 0.7760 - val_loss: 0.4794 - val_acc: 0.7500\n",
      "Epoch 402/1000\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4493 - acc: 0.7760 - val_loss: 0.4794 - val_acc: 0.7500\n",
      "Epoch 403/1000\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4492 - acc: 0.7760 - val_loss: 0.4794 - val_acc: 0.7500\n",
      "Epoch 404/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4492 - acc: 0.7760 - val_loss: 0.4794 - val_acc: 0.7500\n",
      "Epoch 405/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4491 - acc: 0.7760 - val_loss: 0.4794 - val_acc: 0.7500\n",
      "Epoch 406/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4491 - acc: 0.7743 - val_loss: 0.4794 - val_acc: 0.7500\n",
      "Epoch 407/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4490 - acc: 0.7743 - val_loss: 0.4794 - val_acc: 0.7500\n",
      "Epoch 408/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4489 - acc: 0.7760 - val_loss: 0.4794 - val_acc: 0.7500\n",
      "Epoch 409/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4489 - acc: 0.7743 - val_loss: 0.4794 - val_acc: 0.7500\n",
      "Epoch 410/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4488 - acc: 0.7743 - val_loss: 0.4794 - val_acc: 0.7500\n",
      "Epoch 411/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4488 - acc: 0.7743 - val_loss: 0.4794 - val_acc: 0.7500\n",
      "Epoch 412/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4487 - acc: 0.7743 - val_loss: 0.4793 - val_acc: 0.7500\n",
      "Epoch 413/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4487 - acc: 0.7743 - val_loss: 0.4793 - val_acc: 0.7500\n",
      "Epoch 414/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4486 - acc: 0.7743 - val_loss: 0.4793 - val_acc: 0.7500\n",
      "Epoch 415/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4486 - acc: 0.7743 - val_loss: 0.4793 - val_acc: 0.7500\n",
      "Epoch 416/1000\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4485 - acc: 0.7743 - val_loss: 0.4793 - val_acc: 0.7500\n",
      "Epoch 417/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4485 - acc: 0.7743 - val_loss: 0.4793 - val_acc: 0.7500\n",
      "Epoch 418/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4485 - acc: 0.7743 - val_loss: 0.4793 - val_acc: 0.7500\n",
      "Epoch 419/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4484 - acc: 0.7743 - val_loss: 0.4793 - val_acc: 0.7500\n",
      "Epoch 420/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4483 - acc: 0.7743 - val_loss: 0.4793 - val_acc: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 421/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4483 - acc: 0.7743 - val_loss: 0.4793 - val_acc: 0.7500\n",
      "Epoch 422/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4482 - acc: 0.7743 - val_loss: 0.4793 - val_acc: 0.7500\n",
      "Epoch 423/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4482 - acc: 0.7743 - val_loss: 0.4793 - val_acc: 0.7500\n",
      "Epoch 424/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4482 - acc: 0.7743 - val_loss: 0.4793 - val_acc: 0.7500\n",
      "Epoch 425/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4481 - acc: 0.7760 - val_loss: 0.4793 - val_acc: 0.7500\n",
      "Epoch 426/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4480 - acc: 0.7743 - val_loss: 0.4793 - val_acc: 0.7500\n",
      "Epoch 427/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4480 - acc: 0.7743 - val_loss: 0.4793 - val_acc: 0.7500\n",
      "Epoch 428/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4479 - acc: 0.7743 - val_loss: 0.4793 - val_acc: 0.7500\n",
      "Epoch 429/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4479 - acc: 0.7743 - val_loss: 0.4793 - val_acc: 0.7500\n",
      "Epoch 430/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4478 - acc: 0.7743 - val_loss: 0.4793 - val_acc: 0.7500\n",
      "Epoch 431/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4478 - acc: 0.7743 - val_loss: 0.4793 - val_acc: 0.7500\n",
      "Epoch 432/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4478 - acc: 0.7760 - val_loss: 0.4793 - val_acc: 0.7500\n",
      "Epoch 433/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4477 - acc: 0.7760 - val_loss: 0.4792 - val_acc: 0.7500\n",
      "Epoch 434/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4476 - acc: 0.7760 - val_loss: 0.4792 - val_acc: 0.7500\n",
      "Epoch 435/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4476 - acc: 0.7760 - val_loss: 0.4792 - val_acc: 0.7500\n",
      "Epoch 436/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4475 - acc: 0.7760 - val_loss: 0.4792 - val_acc: 0.7500\n",
      "Epoch 437/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4475 - acc: 0.7760 - val_loss: 0.4792 - val_acc: 0.7500\n",
      "Epoch 438/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4475 - acc: 0.7760 - val_loss: 0.4792 - val_acc: 0.7500\n",
      "Epoch 439/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4474 - acc: 0.7760 - val_loss: 0.4792 - val_acc: 0.7500\n",
      "Epoch 440/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4474 - acc: 0.7760 - val_loss: 0.4792 - val_acc: 0.7500\n",
      "Epoch 441/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4473 - acc: 0.7760 - val_loss: 0.4792 - val_acc: 0.7500\n",
      "Epoch 442/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4473 - acc: 0.7760 - val_loss: 0.4792 - val_acc: 0.7500\n",
      "Epoch 443/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4472 - acc: 0.7760 - val_loss: 0.4792 - val_acc: 0.7500\n",
      "Epoch 444/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4471 - acc: 0.7760 - val_loss: 0.4792 - val_acc: 0.7500\n",
      "Epoch 445/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4471 - acc: 0.7760 - val_loss: 0.4792 - val_acc: 0.7500\n",
      "Epoch 446/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4471 - acc: 0.7760 - val_loss: 0.4792 - val_acc: 0.7500\n",
      "Epoch 447/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4470 - acc: 0.7760 - val_loss: 0.4792 - val_acc: 0.7500\n",
      "Epoch 448/1000\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4469 - acc: 0.7760 - val_loss: 0.4792 - val_acc: 0.7500\n",
      "Epoch 449/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4469 - acc: 0.7760 - val_loss: 0.4792 - val_acc: 0.7500\n",
      "Epoch 450/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4468 - acc: 0.7760 - val_loss: 0.4792 - val_acc: 0.7500\n",
      "Epoch 451/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4468 - acc: 0.7760 - val_loss: 0.4792 - val_acc: 0.7500\n",
      "Epoch 452/1000\n",
      "576/576 [==============================] - 0s 193us/step - loss: 0.4468 - acc: 0.7760 - val_loss: 0.4792 - val_acc: 0.7500\n",
      "Epoch 453/1000\n",
      "576/576 [==============================] - 0s 113us/step - loss: 0.4467 - acc: 0.7760 - val_loss: 0.4792 - val_acc: 0.7500\n",
      "Epoch 454/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4466 - acc: 0.7760 - val_loss: 0.4792 - val_acc: 0.7500\n",
      "Epoch 455/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4466 - acc: 0.7778 - val_loss: 0.4792 - val_acc: 0.7500\n",
      "Epoch 456/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4465 - acc: 0.7778 - val_loss: 0.4792 - val_acc: 0.7500\n",
      "Epoch 457/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4465 - acc: 0.7778 - val_loss: 0.4792 - val_acc: 0.7500\n",
      "Epoch 458/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4465 - acc: 0.7778 - val_loss: 0.4792 - val_acc: 0.7500\n",
      "Epoch 459/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4464 - acc: 0.7778 - val_loss: 0.4792 - val_acc: 0.7500\n",
      "Epoch 460/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4463 - acc: 0.7778 - val_loss: 0.4792 - val_acc: 0.7500\n",
      "Epoch 461/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4463 - acc: 0.7778 - val_loss: 0.4792 - val_acc: 0.7500\n",
      "Epoch 462/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4463 - acc: 0.7778 - val_loss: 0.4791 - val_acc: 0.7500\n",
      "Epoch 463/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4462 - acc: 0.7778 - val_loss: 0.4791 - val_acc: 0.7500\n",
      "Epoch 464/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4462 - acc: 0.7778 - val_loss: 0.4791 - val_acc: 0.7500\n",
      "Epoch 465/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4461 - acc: 0.7778 - val_loss: 0.4791 - val_acc: 0.7500\n",
      "Epoch 466/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4460 - acc: 0.7778 - val_loss: 0.4791 - val_acc: 0.7500\n",
      "Epoch 467/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4460 - acc: 0.7778 - val_loss: 0.4791 - val_acc: 0.7500\n",
      "Epoch 468/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4459 - acc: 0.7778 - val_loss: 0.4791 - val_acc: 0.7500\n",
      "Epoch 469/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4459 - acc: 0.7778 - val_loss: 0.4791 - val_acc: 0.7500\n",
      "Epoch 470/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4458 - acc: 0.7778 - val_loss: 0.4791 - val_acc: 0.7500\n",
      "Epoch 471/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4458 - acc: 0.7778 - val_loss: 0.4791 - val_acc: 0.7500\n",
      "Epoch 472/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4457 - acc: 0.7778 - val_loss: 0.4791 - val_acc: 0.7500\n",
      "Epoch 473/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4457 - acc: 0.7778 - val_loss: 0.4791 - val_acc: 0.7500\n",
      "Epoch 474/1000\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.4456 - acc: 0.7778 - val_loss: 0.4791 - val_acc: 0.7500\n",
      "Epoch 475/1000\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.3200 - acc: 0.875 - 0s 85us/step - loss: 0.4455 - acc: 0.7778 - val_loss: 0.4791 - val_acc: 0.7500\n",
      "Epoch 476/1000\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.4455 - acc: 0.7778 - val_loss: 0.4791 - val_acc: 0.7500\n",
      "Epoch 477/1000\n",
      "576/576 [==============================] - 0s 87us/step - loss: 0.4455 - acc: 0.7778 - val_loss: 0.4791 - val_acc: 0.7500\n",
      "Epoch 478/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4454 - acc: 0.7778 - val_loss: 0.4791 - val_acc: 0.7500\n",
      "Epoch 479/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4453 - acc: 0.7795 - val_loss: 0.4791 - val_acc: 0.7500\n",
      "Epoch 480/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 60us/step - loss: 0.4453 - acc: 0.7795 - val_loss: 0.4791 - val_acc: 0.7500\n",
      "Epoch 481/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4452 - acc: 0.7795 - val_loss: 0.4791 - val_acc: 0.7500\n",
      "Epoch 482/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4452 - acc: 0.7795 - val_loss: 0.4791 - val_acc: 0.7500\n",
      "Epoch 483/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4451 - acc: 0.7778 - val_loss: 0.4791 - val_acc: 0.7500\n",
      "Epoch 484/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4451 - acc: 0.7795 - val_loss: 0.4791 - val_acc: 0.7500\n",
      "Epoch 485/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4450 - acc: 0.7795 - val_loss: 0.4791 - val_acc: 0.7500\n",
      "Epoch 486/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4450 - acc: 0.7795 - val_loss: 0.4791 - val_acc: 0.7500\n",
      "Epoch 487/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4449 - acc: 0.7795 - val_loss: 0.4791 - val_acc: 0.7500\n",
      "Epoch 488/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4449 - acc: 0.7778 - val_loss: 0.4791 - val_acc: 0.7500\n",
      "Epoch 489/1000\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.4448 - acc: 0.7778 - val_loss: 0.4791 - val_acc: 0.7500\n",
      "Epoch 490/1000\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4448 - acc: 0.7778 - val_loss: 0.4791 - val_acc: 0.7500\n",
      "Epoch 491/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4447 - acc: 0.7778 - val_loss: 0.4791 - val_acc: 0.7500\n",
      "Epoch 492/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4447 - acc: 0.7778 - val_loss: 0.4791 - val_acc: 0.7500\n",
      "Epoch 493/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4447 - acc: 0.7778 - val_loss: 0.4791 - val_acc: 0.7500\n",
      "Epoch 494/1000\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.4337 - acc: 0.781 - 0s 47us/step - loss: 0.4446 - acc: 0.7778 - val_loss: 0.4792 - val_acc: 0.7500\n",
      "Epoch 495/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4446 - acc: 0.7778 - val_loss: 0.4792 - val_acc: 0.7500\n",
      "Epoch 496/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4445 - acc: 0.7778 - val_loss: 0.4792 - val_acc: 0.7500\n",
      "Epoch 497/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4444 - acc: 0.7778 - val_loss: 0.4792 - val_acc: 0.7500\n",
      "Epoch 498/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4444 - acc: 0.7778 - val_loss: 0.4792 - val_acc: 0.7500\n",
      "Epoch 499/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4443 - acc: 0.7778 - val_loss: 0.4792 - val_acc: 0.7500\n",
      "Epoch 500/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4443 - acc: 0.7778 - val_loss: 0.4792 - val_acc: 0.7500\n",
      "Epoch 501/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4442 - acc: 0.7778 - val_loss: 0.4792 - val_acc: 0.7500\n",
      "Epoch 502/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4442 - acc: 0.7778 - val_loss: 0.4792 - val_acc: 0.7500\n",
      "Epoch 503/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4441 - acc: 0.7778 - val_loss: 0.4792 - val_acc: 0.7500\n",
      "Epoch 504/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4441 - acc: 0.7760 - val_loss: 0.4792 - val_acc: 0.7500\n",
      "Epoch 505/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4440 - acc: 0.7778 - val_loss: 0.4792 - val_acc: 0.7500\n",
      "Epoch 506/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4440 - acc: 0.7778 - val_loss: 0.4792 - val_acc: 0.7500\n",
      "Epoch 507/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4439 - acc: 0.7778 - val_loss: 0.4792 - val_acc: 0.7500\n",
      "Epoch 508/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4439 - acc: 0.7795 - val_loss: 0.4793 - val_acc: 0.7500\n",
      "Epoch 509/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4438 - acc: 0.7795 - val_loss: 0.4793 - val_acc: 0.7500\n",
      "Epoch 510/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4438 - acc: 0.7795 - val_loss: 0.4793 - val_acc: 0.7500\n",
      "Epoch 511/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4437 - acc: 0.7778 - val_loss: 0.4793 - val_acc: 0.7500\n",
      "Epoch 512/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4437 - acc: 0.7778 - val_loss: 0.4793 - val_acc: 0.7500\n",
      "Epoch 513/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4436 - acc: 0.7795 - val_loss: 0.4793 - val_acc: 0.7500\n",
      "Epoch 514/1000\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4436 - acc: 0.7795 - val_loss: 0.4793 - val_acc: 0.7500\n",
      "Epoch 515/1000\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4435 - acc: 0.7795 - val_loss: 0.4793 - val_acc: 0.7500\n",
      "Epoch 516/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4435 - acc: 0.7812 - val_loss: 0.4793 - val_acc: 0.7500\n",
      "Epoch 517/1000\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4434 - acc: 0.7812 - val_loss: 0.4794 - val_acc: 0.7500\n",
      "Epoch 518/1000\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4434 - acc: 0.7795 - val_loss: 0.4794 - val_acc: 0.7500\n",
      "Epoch 519/1000\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4433 - acc: 0.7812 - val_loss: 0.4794 - val_acc: 0.7500\n",
      "Epoch 520/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4433 - acc: 0.7795 - val_loss: 0.4794 - val_acc: 0.7500\n",
      "Epoch 521/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4432 - acc: 0.7812 - val_loss: 0.4794 - val_acc: 0.7500\n",
      "Epoch 522/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4432 - acc: 0.7812 - val_loss: 0.4794 - val_acc: 0.7500\n",
      "Epoch 523/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4431 - acc: 0.7812 - val_loss: 0.4794 - val_acc: 0.7500\n",
      "Epoch 524/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4431 - acc: 0.7812 - val_loss: 0.4794 - val_acc: 0.7500\n",
      "Epoch 525/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4431 - acc: 0.7812 - val_loss: 0.4794 - val_acc: 0.7500\n",
      "Epoch 526/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4430 - acc: 0.7812 - val_loss: 0.4795 - val_acc: 0.7500\n",
      "Epoch 527/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4430 - acc: 0.7795 - val_loss: 0.4795 - val_acc: 0.7500\n",
      "Epoch 528/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4429 - acc: 0.7812 - val_loss: 0.4795 - val_acc: 0.7500\n",
      "Epoch 529/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4429 - acc: 0.7812 - val_loss: 0.4795 - val_acc: 0.7500\n",
      "Epoch 530/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4428 - acc: 0.7812 - val_loss: 0.4795 - val_acc: 0.7500\n",
      "Epoch 531/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4428 - acc: 0.7812 - val_loss: 0.4795 - val_acc: 0.7500\n",
      "Epoch 532/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4427 - acc: 0.7812 - val_loss: 0.4795 - val_acc: 0.7500\n",
      "Epoch 533/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4427 - acc: 0.7812 - val_loss: 0.4795 - val_acc: 0.7500\n",
      "Epoch 534/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4426 - acc: 0.7812 - val_loss: 0.4795 - val_acc: 0.7500\n",
      "Epoch 535/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4426 - acc: 0.7795 - val_loss: 0.4795 - val_acc: 0.7500\n",
      "Epoch 536/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4425 - acc: 0.7812 - val_loss: 0.4796 - val_acc: 0.7500\n",
      "Epoch 537/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4425 - acc: 0.7812 - val_loss: 0.4796 - val_acc: 0.7500\n",
      "Epoch 538/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4425 - acc: 0.7812 - val_loss: 0.4796 - val_acc: 0.7500\n",
      "Epoch 539/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 53us/step - loss: 0.4424 - acc: 0.7812 - val_loss: 0.4796 - val_acc: 0.7500\n",
      "Epoch 540/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4424 - acc: 0.7812 - val_loss: 0.4796 - val_acc: 0.7500\n",
      "Epoch 541/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4423 - acc: 0.7812 - val_loss: 0.4796 - val_acc: 0.7500\n",
      "Epoch 542/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4423 - acc: 0.7812 - val_loss: 0.4796 - val_acc: 0.7500\n",
      "Epoch 543/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4422 - acc: 0.7812 - val_loss: 0.4796 - val_acc: 0.7500\n",
      "Epoch 544/1000\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.4422 - acc: 0.7812 - val_loss: 0.4796 - val_acc: 0.7500\n",
      "Epoch 545/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4421 - acc: 0.7795 - val_loss: 0.4797 - val_acc: 0.7500\n",
      "Epoch 546/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4421 - acc: 0.7812 - val_loss: 0.4797 - val_acc: 0.7500\n",
      "Epoch 547/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4420 - acc: 0.7812 - val_loss: 0.4797 - val_acc: 0.7500\n",
      "Epoch 548/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4420 - acc: 0.7812 - val_loss: 0.4797 - val_acc: 0.7500\n",
      "Epoch 549/1000\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4420 - acc: 0.7812 - val_loss: 0.4797 - val_acc: 0.7500\n",
      "Epoch 550/1000\n",
      "576/576 [==============================] - 0s 86us/step - loss: 0.4419 - acc: 0.7812 - val_loss: 0.4797 - val_acc: 0.7500\n",
      "Epoch 551/1000\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.4419 - acc: 0.7812 - val_loss: 0.4797 - val_acc: 0.7500\n",
      "Epoch 552/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4418 - acc: 0.7812 - val_loss: 0.4797 - val_acc: 0.7500\n",
      "Epoch 553/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4418 - acc: 0.7812 - val_loss: 0.4797 - val_acc: 0.7500\n",
      "Epoch 554/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4417 - acc: 0.7812 - val_loss: 0.4797 - val_acc: 0.7500\n",
      "Epoch 555/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4417 - acc: 0.7812 - val_loss: 0.4797 - val_acc: 0.7500\n",
      "Epoch 556/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4416 - acc: 0.7812 - val_loss: 0.4797 - val_acc: 0.7500\n",
      "Epoch 557/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4416 - acc: 0.7812 - val_loss: 0.4798 - val_acc: 0.7500\n",
      "Epoch 558/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4415 - acc: 0.7812 - val_loss: 0.4798 - val_acc: 0.7500\n",
      "Epoch 559/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4415 - acc: 0.7812 - val_loss: 0.4798 - val_acc: 0.7500\n",
      "Epoch 560/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4414 - acc: 0.7812 - val_loss: 0.4798 - val_acc: 0.7500\n",
      "Epoch 561/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4414 - acc: 0.7812 - val_loss: 0.4798 - val_acc: 0.7500\n",
      "Epoch 562/1000\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4414 - acc: 0.7812 - val_loss: 0.4798 - val_acc: 0.7500\n",
      "Epoch 563/1000\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.4413 - acc: 0.7812 - val_loss: 0.4798 - val_acc: 0.7500\n",
      "Epoch 564/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4413 - acc: 0.7795 - val_loss: 0.4798 - val_acc: 0.7500\n",
      "Epoch 565/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4412 - acc: 0.7795 - val_loss: 0.4798 - val_acc: 0.7500\n",
      "Epoch 566/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4412 - acc: 0.7795 - val_loss: 0.4798 - val_acc: 0.7500\n",
      "Epoch 567/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4412 - acc: 0.7812 - val_loss: 0.4798 - val_acc: 0.7500\n",
      "Epoch 568/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4411 - acc: 0.7812 - val_loss: 0.4798 - val_acc: 0.7500\n",
      "Epoch 569/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4411 - acc: 0.7812 - val_loss: 0.4799 - val_acc: 0.7500\n",
      "Epoch 570/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4410 - acc: 0.7812 - val_loss: 0.4799 - val_acc: 0.7500\n",
      "Epoch 571/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4410 - acc: 0.7812 - val_loss: 0.4799 - val_acc: 0.7500\n",
      "Epoch 572/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4409 - acc: 0.7812 - val_loss: 0.4799 - val_acc: 0.7500\n",
      "Epoch 573/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4409 - acc: 0.7812 - val_loss: 0.4799 - val_acc: 0.7500\n",
      "Epoch 574/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4408 - acc: 0.7812 - val_loss: 0.4799 - val_acc: 0.7500\n",
      "Epoch 575/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4408 - acc: 0.7795 - val_loss: 0.4799 - val_acc: 0.7500\n",
      "Epoch 576/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4407 - acc: 0.7812 - val_loss: 0.4799 - val_acc: 0.7500\n",
      "Epoch 577/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4407 - acc: 0.7812 - val_loss: 0.4799 - val_acc: 0.7500\n",
      "Epoch 578/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4406 - acc: 0.7812 - val_loss: 0.4800 - val_acc: 0.7500\n",
      "Epoch 579/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4406 - acc: 0.7812 - val_loss: 0.4800 - val_acc: 0.7500\n",
      "Epoch 580/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4405 - acc: 0.7830 - val_loss: 0.4800 - val_acc: 0.7500\n",
      "Epoch 581/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4405 - acc: 0.7812 - val_loss: 0.4800 - val_acc: 0.7500\n",
      "Epoch 582/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4404 - acc: 0.7830 - val_loss: 0.4800 - val_acc: 0.7500\n",
      "Epoch 583/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4404 - acc: 0.7812 - val_loss: 0.4800 - val_acc: 0.7500\n",
      "Epoch 584/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4404 - acc: 0.7830 - val_loss: 0.4800 - val_acc: 0.7500\n",
      "Epoch 585/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4403 - acc: 0.7847 - val_loss: 0.4801 - val_acc: 0.7500\n",
      "Epoch 586/1000\n",
      "576/576 [==============================] - 0s 84us/step - loss: 0.4403 - acc: 0.7847 - val_loss: 0.4801 - val_acc: 0.7500\n",
      "Epoch 587/1000\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.3943 - acc: 0.843 - 0s 70us/step - loss: 0.4402 - acc: 0.7812 - val_loss: 0.4801 - val_acc: 0.7500\n",
      "Epoch 588/1000\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.4402 - acc: 0.7847 - val_loss: 0.4801 - val_acc: 0.7500\n",
      "Epoch 589/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4401 - acc: 0.7847 - val_loss: 0.4801 - val_acc: 0.7500\n",
      "Epoch 590/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4401 - acc: 0.7847 - val_loss: 0.4801 - val_acc: 0.7500\n",
      "Epoch 591/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4400 - acc: 0.7830 - val_loss: 0.4802 - val_acc: 0.7500\n",
      "Epoch 592/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4400 - acc: 0.7847 - val_loss: 0.4802 - val_acc: 0.7500\n",
      "Epoch 593/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4399 - acc: 0.7847 - val_loss: 0.4802 - val_acc: 0.7500\n",
      "Epoch 594/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4399 - acc: 0.7830 - val_loss: 0.4802 - val_acc: 0.7500\n",
      "Epoch 595/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4399 - acc: 0.7847 - val_loss: 0.4802 - val_acc: 0.7500\n",
      "Epoch 596/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4398 - acc: 0.7847 - val_loss: 0.4802 - val_acc: 0.7500\n",
      "Epoch 597/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4397 - acc: 0.7847 - val_loss: 0.4802 - val_acc: 0.7500\n",
      "Epoch 598/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 61us/step - loss: 0.4397 - acc: 0.7847 - val_loss: 0.4803 - val_acc: 0.7500\n",
      "Epoch 599/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4397 - acc: 0.7830 - val_loss: 0.4803 - val_acc: 0.7500\n",
      "Epoch 600/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4396 - acc: 0.7830 - val_loss: 0.4803 - val_acc: 0.7500\n",
      "Epoch 601/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4396 - acc: 0.7847 - val_loss: 0.4803 - val_acc: 0.7500\n",
      "Epoch 602/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4395 - acc: 0.7847 - val_loss: 0.4803 - val_acc: 0.7500\n",
      "Epoch 603/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4395 - acc: 0.7847 - val_loss: 0.4803 - val_acc: 0.7500\n",
      "Epoch 604/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4394 - acc: 0.7847 - val_loss: 0.4803 - val_acc: 0.7500\n",
      "Epoch 605/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4394 - acc: 0.7847 - val_loss: 0.4804 - val_acc: 0.7500\n",
      "Epoch 606/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4394 - acc: 0.7847 - val_loss: 0.4804 - val_acc: 0.7500\n",
      "Epoch 607/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4393 - acc: 0.7847 - val_loss: 0.4804 - val_acc: 0.7500\n",
      "Epoch 608/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4392 - acc: 0.7865 - val_loss: 0.4804 - val_acc: 0.7500\n",
      "Epoch 609/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4392 - acc: 0.7847 - val_loss: 0.4804 - val_acc: 0.7500\n",
      "Epoch 610/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4392 - acc: 0.7847 - val_loss: 0.4804 - val_acc: 0.7500\n",
      "Epoch 611/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4391 - acc: 0.7865 - val_loss: 0.4805 - val_acc: 0.7500\n",
      "Epoch 612/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4391 - acc: 0.7865 - val_loss: 0.4805 - val_acc: 0.7500\n",
      "Epoch 613/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4390 - acc: 0.7847 - val_loss: 0.4805 - val_acc: 0.7500\n",
      "Epoch 614/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4390 - acc: 0.7847 - val_loss: 0.4805 - val_acc: 0.7500\n",
      "Epoch 615/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4389 - acc: 0.7847 - val_loss: 0.4805 - val_acc: 0.7500\n",
      "Epoch 616/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4389 - acc: 0.7847 - val_loss: 0.4805 - val_acc: 0.7500\n",
      "Epoch 617/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4388 - acc: 0.7865 - val_loss: 0.4806 - val_acc: 0.7500\n",
      "Epoch 618/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4388 - acc: 0.7847 - val_loss: 0.4806 - val_acc: 0.7500\n",
      "Epoch 619/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4388 - acc: 0.7847 - val_loss: 0.4806 - val_acc: 0.7500\n",
      "Epoch 620/1000\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.3216 - acc: 0.843 - 0s 64us/step - loss: 0.4387 - acc: 0.7847 - val_loss: 0.4806 - val_acc: 0.7500\n",
      "Epoch 621/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4387 - acc: 0.7865 - val_loss: 0.4806 - val_acc: 0.7500\n",
      "Epoch 622/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4387 - acc: 0.7847 - val_loss: 0.4806 - val_acc: 0.7500\n",
      "Epoch 623/1000\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.4386 - acc: 0.7847 - val_loss: 0.4806 - val_acc: 0.7500\n",
      "Epoch 624/1000\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.4385 - acc: 0.7847 - val_loss: 0.4807 - val_acc: 0.7500\n",
      "Epoch 625/1000\n",
      "576/576 [==============================] - 0s 85us/step - loss: 0.4385 - acc: 0.7865 - val_loss: 0.4807 - val_acc: 0.7500\n",
      "Epoch 626/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4385 - acc: 0.7847 - val_loss: 0.4807 - val_acc: 0.7500\n",
      "Epoch 627/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4384 - acc: 0.7847 - val_loss: 0.4807 - val_acc: 0.7500\n",
      "Epoch 628/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4383 - acc: 0.7865 - val_loss: 0.4807 - val_acc: 0.7500\n",
      "Epoch 629/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4383 - acc: 0.7865 - val_loss: 0.4807 - val_acc: 0.7500\n",
      "Epoch 630/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4383 - acc: 0.7865 - val_loss: 0.4807 - val_acc: 0.7500\n",
      "Epoch 631/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4382 - acc: 0.7865 - val_loss: 0.4808 - val_acc: 0.7500\n",
      "Epoch 632/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4382 - acc: 0.7865 - val_loss: 0.4808 - val_acc: 0.7500\n",
      "Epoch 633/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4381 - acc: 0.7865 - val_loss: 0.4808 - val_acc: 0.7500\n",
      "Epoch 634/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4381 - acc: 0.7865 - val_loss: 0.4808 - val_acc: 0.7500\n",
      "Epoch 635/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4380 - acc: 0.7847 - val_loss: 0.4808 - val_acc: 0.7500\n",
      "Epoch 636/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4380 - acc: 0.7865 - val_loss: 0.4808 - val_acc: 0.7500\n",
      "Epoch 637/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4379 - acc: 0.7865 - val_loss: 0.4809 - val_acc: 0.7500\n",
      "Epoch 638/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4379 - acc: 0.7865 - val_loss: 0.4809 - val_acc: 0.7500\n",
      "Epoch 639/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4379 - acc: 0.7865 - val_loss: 0.4809 - val_acc: 0.7500\n",
      "Epoch 640/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4378 - acc: 0.7847 - val_loss: 0.4809 - val_acc: 0.7500\n",
      "Epoch 641/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4378 - acc: 0.7865 - val_loss: 0.4809 - val_acc: 0.7500\n",
      "Epoch 642/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4377 - acc: 0.7865 - val_loss: 0.4809 - val_acc: 0.7500\n",
      "Epoch 643/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4377 - acc: 0.7865 - val_loss: 0.4810 - val_acc: 0.7500\n",
      "Epoch 644/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4376 - acc: 0.7865 - val_loss: 0.4810 - val_acc: 0.7500\n",
      "Epoch 645/1000\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.4376 - acc: 0.7847 - val_loss: 0.4810 - val_acc: 0.7500\n",
      "Epoch 646/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4375 - acc: 0.7865 - val_loss: 0.4810 - val_acc: 0.7500\n",
      "Epoch 647/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4375 - acc: 0.7865 - val_loss: 0.4810 - val_acc: 0.7500\n",
      "Epoch 648/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4375 - acc: 0.7865 - val_loss: 0.4810 - val_acc: 0.7500\n",
      "Epoch 649/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4374 - acc: 0.7865 - val_loss: 0.4810 - val_acc: 0.7500\n",
      "Epoch 650/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4373 - acc: 0.7865 - val_loss: 0.4810 - val_acc: 0.7500\n",
      "Epoch 651/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4373 - acc: 0.7865 - val_loss: 0.4811 - val_acc: 0.7500\n",
      "Epoch 652/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4373 - acc: 0.7865 - val_loss: 0.4811 - val_acc: 0.7500\n",
      "Epoch 653/1000\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.4372 - acc: 0.7865 - val_loss: 0.4811 - val_acc: 0.7500\n",
      "Epoch 654/1000\n",
      "576/576 [==============================] - 0s 98us/step - loss: 0.4372 - acc: 0.7865 - val_loss: 0.4811 - val_acc: 0.7500\n",
      "Epoch 655/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4371 - acc: 0.7865 - val_loss: 0.4811 - val_acc: 0.7500\n",
      "Epoch 656/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4371 - acc: 0.7865 - val_loss: 0.4811 - val_acc: 0.7500\n",
      "Epoch 657/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 54us/step - loss: 0.4371 - acc: 0.7865 - val_loss: 0.4811 - val_acc: 0.7500\n",
      "Epoch 658/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4370 - acc: 0.7865 - val_loss: 0.4812 - val_acc: 0.7500\n",
      "Epoch 659/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4370 - acc: 0.7865 - val_loss: 0.4812 - val_acc: 0.7552\n",
      "Epoch 660/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4370 - acc: 0.7865 - val_loss: 0.4812 - val_acc: 0.7552\n",
      "Epoch 661/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4369 - acc: 0.7865 - val_loss: 0.4812 - val_acc: 0.7552\n",
      "Epoch 662/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4369 - acc: 0.7865 - val_loss: 0.4812 - val_acc: 0.7500\n",
      "Epoch 663/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4369 - acc: 0.7865 - val_loss: 0.4812 - val_acc: 0.7500\n",
      "Epoch 664/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4368 - acc: 0.7865 - val_loss: 0.4812 - val_acc: 0.7500\n",
      "Epoch 665/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4367 - acc: 0.7865 - val_loss: 0.4813 - val_acc: 0.7500\n",
      "Epoch 666/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4367 - acc: 0.7865 - val_loss: 0.4813 - val_acc: 0.7500\n",
      "Epoch 667/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4366 - acc: 0.7865 - val_loss: 0.4813 - val_acc: 0.7500\n",
      "Epoch 668/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4366 - acc: 0.7865 - val_loss: 0.4813 - val_acc: 0.7500\n",
      "Epoch 669/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4366 - acc: 0.7865 - val_loss: 0.4813 - val_acc: 0.7500\n",
      "Epoch 670/1000\n",
      "576/576 [==============================] - 0s 79us/step - loss: 0.4365 - acc: 0.7865 - val_loss: 0.4813 - val_acc: 0.7500\n",
      "Epoch 671/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4365 - acc: 0.7865 - val_loss: 0.4813 - val_acc: 0.7500\n",
      "Epoch 672/1000\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.4365 - acc: 0.7865 - val_loss: 0.4814 - val_acc: 0.7500\n",
      "Epoch 673/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4364 - acc: 0.7865 - val_loss: 0.4814 - val_acc: 0.7500\n",
      "Epoch 674/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4364 - acc: 0.7865 - val_loss: 0.4814 - val_acc: 0.7500\n",
      "Epoch 675/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4363 - acc: 0.7865 - val_loss: 0.4814 - val_acc: 0.7500\n",
      "Epoch 676/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4363 - acc: 0.7865 - val_loss: 0.4814 - val_acc: 0.7500\n",
      "Epoch 677/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4363 - acc: 0.7865 - val_loss: 0.4814 - val_acc: 0.7500\n",
      "Epoch 678/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4362 - acc: 0.7865 - val_loss: 0.4815 - val_acc: 0.7500\n",
      "Epoch 679/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4362 - acc: 0.7865 - val_loss: 0.4815 - val_acc: 0.7500\n",
      "Epoch 680/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4362 - acc: 0.7865 - val_loss: 0.4815 - val_acc: 0.7500\n",
      "Epoch 681/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4361 - acc: 0.7865 - val_loss: 0.4815 - val_acc: 0.7500\n",
      "Epoch 682/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4361 - acc: 0.7865 - val_loss: 0.4815 - val_acc: 0.7500\n",
      "Epoch 683/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4360 - acc: 0.7865 - val_loss: 0.4815 - val_acc: 0.7500\n",
      "Epoch 684/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4360 - acc: 0.7865 - val_loss: 0.4815 - val_acc: 0.7500\n",
      "Epoch 685/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4360 - acc: 0.7847 - val_loss: 0.4816 - val_acc: 0.7500\n",
      "Epoch 686/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4359 - acc: 0.7847 - val_loss: 0.4816 - val_acc: 0.7500\n",
      "Epoch 687/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4359 - acc: 0.7847 - val_loss: 0.4816 - val_acc: 0.7448\n",
      "Epoch 688/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4359 - acc: 0.7865 - val_loss: 0.4816 - val_acc: 0.7448\n",
      "Epoch 689/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4358 - acc: 0.7847 - val_loss: 0.4816 - val_acc: 0.7448\n",
      "Epoch 690/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4358 - acc: 0.7847 - val_loss: 0.4816 - val_acc: 0.7448\n",
      "Epoch 691/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4357 - acc: 0.7865 - val_loss: 0.4817 - val_acc: 0.7448\n",
      "Epoch 692/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4357 - acc: 0.7847 - val_loss: 0.4817 - val_acc: 0.7448\n",
      "Epoch 693/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4357 - acc: 0.7847 - val_loss: 0.4817 - val_acc: 0.7396\n",
      "Epoch 694/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4356 - acc: 0.7847 - val_loss: 0.4817 - val_acc: 0.7396\n",
      "Epoch 695/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4356 - acc: 0.7865 - val_loss: 0.4817 - val_acc: 0.7396\n",
      "Epoch 696/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4356 - acc: 0.7865 - val_loss: 0.4817 - val_acc: 0.7396\n",
      "Epoch 697/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4355 - acc: 0.7865 - val_loss: 0.4818 - val_acc: 0.7396\n",
      "Epoch 698/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4355 - acc: 0.7882 - val_loss: 0.4818 - val_acc: 0.7396\n",
      "Epoch 699/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4354 - acc: 0.7865 - val_loss: 0.4818 - val_acc: 0.7396\n",
      "Epoch 700/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4354 - acc: 0.7865 - val_loss: 0.4818 - val_acc: 0.7396\n",
      "Epoch 701/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4354 - acc: 0.7865 - val_loss: 0.4818 - val_acc: 0.7396\n",
      "Epoch 702/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4354 - acc: 0.7865 - val_loss: 0.4818 - val_acc: 0.7396\n",
      "Epoch 703/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4353 - acc: 0.7882 - val_loss: 0.4818 - val_acc: 0.7396\n",
      "Epoch 704/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4353 - acc: 0.7882 - val_loss: 0.4819 - val_acc: 0.7396\n",
      "Epoch 705/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4352 - acc: 0.7899 - val_loss: 0.4819 - val_acc: 0.7396\n",
      "Epoch 706/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4352 - acc: 0.7899 - val_loss: 0.4819 - val_acc: 0.7396\n",
      "Epoch 707/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4352 - acc: 0.7899 - val_loss: 0.4819 - val_acc: 0.7396\n",
      "Epoch 708/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4351 - acc: 0.7899 - val_loss: 0.4819 - val_acc: 0.7396\n",
      "Epoch 709/1000\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.3667 - acc: 0.875 - 0s 53us/step - loss: 0.4351 - acc: 0.7899 - val_loss: 0.4819 - val_acc: 0.7396\n",
      "Epoch 710/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4351 - acc: 0.7899 - val_loss: 0.4820 - val_acc: 0.7396\n",
      "Epoch 711/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4350 - acc: 0.7899 - val_loss: 0.4820 - val_acc: 0.7396\n",
      "Epoch 712/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4350 - acc: 0.7882 - val_loss: 0.4820 - val_acc: 0.7396\n",
      "Epoch 713/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4350 - acc: 0.7899 - val_loss: 0.4820 - val_acc: 0.7396\n",
      "Epoch 714/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4349 - acc: 0.7899 - val_loss: 0.4820 - val_acc: 0.7396\n",
      "Epoch 715/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4349 - acc: 0.7899 - val_loss: 0.4820 - val_acc: 0.7396\n",
      "Epoch 716/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 50us/step - loss: 0.4349 - acc: 0.7899 - val_loss: 0.4820 - val_acc: 0.7396\n",
      "Epoch 717/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4349 - acc: 0.7899 - val_loss: 0.4821 - val_acc: 0.7396\n",
      "Epoch 718/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4348 - acc: 0.7899 - val_loss: 0.4821 - val_acc: 0.7396\n",
      "Epoch 719/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4348 - acc: 0.7899 - val_loss: 0.4821 - val_acc: 0.7396\n",
      "Epoch 720/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4348 - acc: 0.7899 - val_loss: 0.4821 - val_acc: 0.7396\n",
      "Epoch 721/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4347 - acc: 0.7899 - val_loss: 0.4821 - val_acc: 0.7396\n",
      "Epoch 722/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4347 - acc: 0.7899 - val_loss: 0.4822 - val_acc: 0.7396\n",
      "Epoch 723/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4346 - acc: 0.7899 - val_loss: 0.4822 - val_acc: 0.7396\n",
      "Epoch 724/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4346 - acc: 0.7899 - val_loss: 0.4822 - val_acc: 0.7396\n",
      "Epoch 725/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4346 - acc: 0.7899 - val_loss: 0.4822 - val_acc: 0.7396\n",
      "Epoch 726/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4346 - acc: 0.7899 - val_loss: 0.4822 - val_acc: 0.7396\n",
      "Epoch 727/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4345 - acc: 0.7899 - val_loss: 0.4822 - val_acc: 0.7396\n",
      "Epoch 728/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4345 - acc: 0.7899 - val_loss: 0.4823 - val_acc: 0.7396\n",
      "Epoch 729/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4345 - acc: 0.7899 - val_loss: 0.4823 - val_acc: 0.7396\n",
      "Epoch 730/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4344 - acc: 0.7899 - val_loss: 0.4823 - val_acc: 0.7396\n",
      "Epoch 731/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4344 - acc: 0.7899 - val_loss: 0.4823 - val_acc: 0.7396\n",
      "Epoch 732/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4344 - acc: 0.7899 - val_loss: 0.4823 - val_acc: 0.7396\n",
      "Epoch 733/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4343 - acc: 0.7899 - val_loss: 0.4823 - val_acc: 0.7396\n",
      "Epoch 734/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4343 - acc: 0.7917 - val_loss: 0.4824 - val_acc: 0.7396\n",
      "Epoch 735/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4343 - acc: 0.7899 - val_loss: 0.4824 - val_acc: 0.7396\n",
      "Epoch 736/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4343 - acc: 0.7899 - val_loss: 0.4824 - val_acc: 0.7396\n",
      "Epoch 737/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4342 - acc: 0.7917 - val_loss: 0.4824 - val_acc: 0.7396\n",
      "Epoch 738/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4342 - acc: 0.7899 - val_loss: 0.4824 - val_acc: 0.7396\n",
      "Epoch 739/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4342 - acc: 0.7899 - val_loss: 0.4825 - val_acc: 0.7396\n",
      "Epoch 740/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4341 - acc: 0.7899 - val_loss: 0.4825 - val_acc: 0.7396\n",
      "Epoch 741/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4341 - acc: 0.7917 - val_loss: 0.4825 - val_acc: 0.7396\n",
      "Epoch 742/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4341 - acc: 0.7917 - val_loss: 0.4825 - val_acc: 0.7396\n",
      "Epoch 743/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4340 - acc: 0.7917 - val_loss: 0.4825 - val_acc: 0.7396\n",
      "Epoch 744/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4340 - acc: 0.7917 - val_loss: 0.4826 - val_acc: 0.7396\n",
      "Epoch 745/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4340 - acc: 0.7917 - val_loss: 0.4826 - val_acc: 0.7396\n",
      "Epoch 746/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4340 - acc: 0.7917 - val_loss: 0.4826 - val_acc: 0.7396\n",
      "Epoch 747/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4339 - acc: 0.7917 - val_loss: 0.4826 - val_acc: 0.7396\n",
      "Epoch 748/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4339 - acc: 0.7917 - val_loss: 0.4826 - val_acc: 0.7396\n",
      "Epoch 749/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4339 - acc: 0.7917 - val_loss: 0.4827 - val_acc: 0.7396\n",
      "Epoch 750/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4339 - acc: 0.7917 - val_loss: 0.4827 - val_acc: 0.7396\n",
      "Epoch 751/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4338 - acc: 0.7917 - val_loss: 0.4827 - val_acc: 0.7396\n",
      "Epoch 752/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4338 - acc: 0.7917 - val_loss: 0.4827 - val_acc: 0.7396\n",
      "Epoch 753/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4338 - acc: 0.7917 - val_loss: 0.4827 - val_acc: 0.7396\n",
      "Epoch 754/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4337 - acc: 0.7917 - val_loss: 0.4828 - val_acc: 0.7396\n",
      "Epoch 755/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4337 - acc: 0.7917 - val_loss: 0.4828 - val_acc: 0.7396\n",
      "Epoch 756/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4337 - acc: 0.7917 - val_loss: 0.4828 - val_acc: 0.7396\n",
      "Epoch 757/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4337 - acc: 0.7917 - val_loss: 0.4828 - val_acc: 0.7396\n",
      "Epoch 758/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4336 - acc: 0.7917 - val_loss: 0.4828 - val_acc: 0.7396\n",
      "Epoch 759/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4336 - acc: 0.7917 - val_loss: 0.4829 - val_acc: 0.7396\n",
      "Epoch 760/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4336 - acc: 0.7917 - val_loss: 0.4829 - val_acc: 0.7396\n",
      "Epoch 761/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4335 - acc: 0.7917 - val_loss: 0.4829 - val_acc: 0.7396\n",
      "Epoch 762/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4335 - acc: 0.7917 - val_loss: 0.4829 - val_acc: 0.7396\n",
      "Epoch 763/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4335 - acc: 0.7917 - val_loss: 0.4830 - val_acc: 0.7396\n",
      "Epoch 764/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4335 - acc: 0.7917 - val_loss: 0.4830 - val_acc: 0.7396\n",
      "Epoch 765/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4334 - acc: 0.7917 - val_loss: 0.4830 - val_acc: 0.7396\n",
      "Epoch 766/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4334 - acc: 0.7917 - val_loss: 0.4830 - val_acc: 0.7396\n",
      "Epoch 767/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4334 - acc: 0.7917 - val_loss: 0.4830 - val_acc: 0.7396\n",
      "Epoch 768/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4334 - acc: 0.7917 - val_loss: 0.4831 - val_acc: 0.7396\n",
      "Epoch 769/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4333 - acc: 0.7917 - val_loss: 0.4831 - val_acc: 0.7396\n",
      "Epoch 770/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4333 - acc: 0.7917 - val_loss: 0.4831 - val_acc: 0.7396\n",
      "Epoch 771/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4333 - acc: 0.7917 - val_loss: 0.4831 - val_acc: 0.7396\n",
      "Epoch 772/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4332 - acc: 0.7917 - val_loss: 0.4832 - val_acc: 0.7396\n",
      "Epoch 773/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4332 - acc: 0.7917 - val_loss: 0.4832 - val_acc: 0.7396\n",
      "Epoch 774/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4332 - acc: 0.7917 - val_loss: 0.4832 - val_acc: 0.7396\n",
      "Epoch 775/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4332 - acc: 0.7917 - val_loss: 0.4832 - val_acc: 0.7396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 776/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4331 - acc: 0.7917 - val_loss: 0.4832 - val_acc: 0.7396\n",
      "Epoch 777/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4331 - acc: 0.7917 - val_loss: 0.4833 - val_acc: 0.7396\n",
      "Epoch 778/1000\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.4331 - acc: 0.7917 - val_loss: 0.4833 - val_acc: 0.7396\n",
      "Epoch 779/1000\n",
      "576/576 [==============================] - 0s 154us/step - loss: 0.4330 - acc: 0.7917 - val_loss: 0.4833 - val_acc: 0.7396\n",
      "Epoch 780/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4330 - acc: 0.7917 - val_loss: 0.4833 - val_acc: 0.7396\n",
      "Epoch 781/1000\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4330 - acc: 0.7934 - val_loss: 0.4834 - val_acc: 0.7396\n",
      "Epoch 782/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4329 - acc: 0.7917 - val_loss: 0.4834 - val_acc: 0.7396\n",
      "Epoch 783/1000\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.4329 - acc: 0.7917 - val_loss: 0.4834 - val_acc: 0.7396\n",
      "Epoch 784/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4329 - acc: 0.7917 - val_loss: 0.4834 - val_acc: 0.7396\n",
      "Epoch 785/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4329 - acc: 0.7917 - val_loss: 0.4835 - val_acc: 0.7396\n",
      "Epoch 786/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4328 - acc: 0.7917 - val_loss: 0.4835 - val_acc: 0.7396\n",
      "Epoch 787/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4328 - acc: 0.7934 - val_loss: 0.4835 - val_acc: 0.7396\n",
      "Epoch 788/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4328 - acc: 0.7917 - val_loss: 0.4835 - val_acc: 0.7396\n",
      "Epoch 789/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4328 - acc: 0.7917 - val_loss: 0.4836 - val_acc: 0.7396\n",
      "Epoch 790/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4327 - acc: 0.7917 - val_loss: 0.4836 - val_acc: 0.7396\n",
      "Epoch 791/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4327 - acc: 0.7934 - val_loss: 0.4836 - val_acc: 0.7396\n",
      "Epoch 792/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4327 - acc: 0.7934 - val_loss: 0.4836 - val_acc: 0.7396\n",
      "Epoch 793/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4326 - acc: 0.7934 - val_loss: 0.4836 - val_acc: 0.7396\n",
      "Epoch 794/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4326 - acc: 0.7934 - val_loss: 0.4837 - val_acc: 0.7396\n",
      "Epoch 795/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4326 - acc: 0.7917 - val_loss: 0.4837 - val_acc: 0.7396\n",
      "Epoch 796/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4326 - acc: 0.7934 - val_loss: 0.4837 - val_acc: 0.7396\n",
      "Epoch 797/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4326 - acc: 0.7917 - val_loss: 0.4837 - val_acc: 0.7396\n",
      "Epoch 798/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4325 - acc: 0.7934 - val_loss: 0.4837 - val_acc: 0.7396\n",
      "Epoch 799/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4325 - acc: 0.7934 - val_loss: 0.4838 - val_acc: 0.7396\n",
      "Epoch 800/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4324 - acc: 0.7934 - val_loss: 0.4838 - val_acc: 0.7396\n",
      "Epoch 801/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4324 - acc: 0.7934 - val_loss: 0.4838 - val_acc: 0.7396\n",
      "Epoch 802/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4324 - acc: 0.7934 - val_loss: 0.4838 - val_acc: 0.7396\n",
      "Epoch 803/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4324 - acc: 0.7934 - val_loss: 0.4838 - val_acc: 0.7396\n",
      "Epoch 804/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4324 - acc: 0.7934 - val_loss: 0.4839 - val_acc: 0.7396\n",
      "Epoch 805/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4323 - acc: 0.7934 - val_loss: 0.4839 - val_acc: 0.7396\n",
      "Epoch 806/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4323 - acc: 0.7934 - val_loss: 0.4839 - val_acc: 0.7396\n",
      "Epoch 807/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4323 - acc: 0.7934 - val_loss: 0.4839 - val_acc: 0.7396\n",
      "Epoch 808/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4323 - acc: 0.7934 - val_loss: 0.4840 - val_acc: 0.7396\n",
      "Epoch 809/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4322 - acc: 0.7934 - val_loss: 0.4840 - val_acc: 0.7396\n",
      "Epoch 810/1000\n",
      "576/576 [==============================] - 0s 199us/step - loss: 0.4322 - acc: 0.7934 - val_loss: 0.4840 - val_acc: 0.7396\n",
      "Epoch 811/1000\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.4322 - acc: 0.7934 - val_loss: 0.4840 - val_acc: 0.7396\n",
      "Epoch 812/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4322 - acc: 0.7934 - val_loss: 0.4841 - val_acc: 0.7396\n",
      "Epoch 813/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4321 - acc: 0.7934 - val_loss: 0.4841 - val_acc: 0.7396\n",
      "Epoch 814/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4321 - acc: 0.7934 - val_loss: 0.4841 - val_acc: 0.7396\n",
      "Epoch 815/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4321 - acc: 0.7934 - val_loss: 0.4841 - val_acc: 0.7396\n",
      "Epoch 816/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4321 - acc: 0.7934 - val_loss: 0.4841 - val_acc: 0.7396\n",
      "Epoch 817/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4320 - acc: 0.7934 - val_loss: 0.4842 - val_acc: 0.7448\n",
      "Epoch 818/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4320 - acc: 0.7934 - val_loss: 0.4842 - val_acc: 0.7448\n",
      "Epoch 819/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4320 - acc: 0.7934 - val_loss: 0.4842 - val_acc: 0.7448\n",
      "Epoch 820/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4319 - acc: 0.7934 - val_loss: 0.4843 - val_acc: 0.7448\n",
      "Epoch 821/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4319 - acc: 0.7934 - val_loss: 0.4843 - val_acc: 0.7448\n",
      "Epoch 822/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4319 - acc: 0.7934 - val_loss: 0.4843 - val_acc: 0.7448\n",
      "Epoch 823/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4319 - acc: 0.7934 - val_loss: 0.4843 - val_acc: 0.7448\n",
      "Epoch 824/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4318 - acc: 0.7934 - val_loss: 0.4843 - val_acc: 0.7448\n",
      "Epoch 825/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4318 - acc: 0.7934 - val_loss: 0.4844 - val_acc: 0.7448\n",
      "Epoch 826/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4318 - acc: 0.7934 - val_loss: 0.4844 - val_acc: 0.7448\n",
      "Epoch 827/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4318 - acc: 0.7934 - val_loss: 0.4844 - val_acc: 0.7448\n",
      "Epoch 828/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4317 - acc: 0.7934 - val_loss: 0.4844 - val_acc: 0.7448\n",
      "Epoch 829/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4317 - acc: 0.7934 - val_loss: 0.4845 - val_acc: 0.7448\n",
      "Epoch 830/1000\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.4317 - acc: 0.7917 - val_loss: 0.4845 - val_acc: 0.7448\n",
      "Epoch 831/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4317 - acc: 0.7917 - val_loss: 0.4845 - val_acc: 0.7448\n",
      "Epoch 832/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4316 - acc: 0.7934 - val_loss: 0.4845 - val_acc: 0.7448\n",
      "Epoch 833/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4316 - acc: 0.7917 - val_loss: 0.4846 - val_acc: 0.7448\n",
      "Epoch 834/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4316 - acc: 0.7917 - val_loss: 0.4846 - val_acc: 0.7448\n",
      "Epoch 835/1000\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.4374 - acc: 0.789 - 0s 225us/step - loss: 0.4316 - acc: 0.7917 - val_loss: 0.4846 - val_acc: 0.7448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 836/1000\n",
      "576/576 [==============================] - 0s 90us/step - loss: 0.4315 - acc: 0.7917 - val_loss: 0.4846 - val_acc: 0.7448\n",
      "Epoch 837/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4315 - acc: 0.7917 - val_loss: 0.4847 - val_acc: 0.7448\n",
      "Epoch 838/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4315 - acc: 0.7917 - val_loss: 0.4847 - val_acc: 0.7448\n",
      "Epoch 839/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4315 - acc: 0.7917 - val_loss: 0.4847 - val_acc: 0.7448\n",
      "Epoch 840/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4314 - acc: 0.7917 - val_loss: 0.4847 - val_acc: 0.7448\n",
      "Epoch 841/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4314 - acc: 0.7917 - val_loss: 0.4848 - val_acc: 0.7448\n",
      "Epoch 842/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4314 - acc: 0.7917 - val_loss: 0.4848 - val_acc: 0.7448\n",
      "Epoch 843/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4314 - acc: 0.7917 - val_loss: 0.4848 - val_acc: 0.7448\n",
      "Epoch 844/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4314 - acc: 0.7917 - val_loss: 0.4848 - val_acc: 0.7448\n",
      "Epoch 845/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4313 - acc: 0.7917 - val_loss: 0.4849 - val_acc: 0.7448\n",
      "Epoch 846/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4313 - acc: 0.7917 - val_loss: 0.4849 - val_acc: 0.7448\n",
      "Epoch 847/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4312 - acc: 0.7917 - val_loss: 0.4849 - val_acc: 0.7448\n",
      "Epoch 848/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4313 - acc: 0.7917 - val_loss: 0.4849 - val_acc: 0.7448\n",
      "Epoch 849/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4312 - acc: 0.7917 - val_loss: 0.4850 - val_acc: 0.7448\n",
      "Epoch 850/1000\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.5197 - acc: 0.750 - 0s 53us/step - loss: 0.4312 - acc: 0.7917 - val_loss: 0.4850 - val_acc: 0.7448\n",
      "Epoch 851/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4312 - acc: 0.7917 - val_loss: 0.4850 - val_acc: 0.7448\n",
      "Epoch 852/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4311 - acc: 0.7917 - val_loss: 0.4850 - val_acc: 0.7448\n",
      "Epoch 853/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4311 - acc: 0.7917 - val_loss: 0.4851 - val_acc: 0.7448\n",
      "Epoch 854/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4311 - acc: 0.7917 - val_loss: 0.4851 - val_acc: 0.7448\n",
      "Epoch 855/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4311 - acc: 0.7917 - val_loss: 0.4851 - val_acc: 0.7448\n",
      "Epoch 856/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4311 - acc: 0.7917 - val_loss: 0.4851 - val_acc: 0.7448\n",
      "Epoch 857/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4310 - acc: 0.7917 - val_loss: 0.4852 - val_acc: 0.7448\n",
      "Epoch 858/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4310 - acc: 0.7917 - val_loss: 0.4852 - val_acc: 0.7448\n",
      "Epoch 859/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4310 - acc: 0.7917 - val_loss: 0.4852 - val_acc: 0.7448\n",
      "Epoch 860/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4310 - acc: 0.7934 - val_loss: 0.4852 - val_acc: 0.7448\n",
      "Epoch 861/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4309 - acc: 0.7917 - val_loss: 0.4853 - val_acc: 0.7448\n",
      "Epoch 862/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4309 - acc: 0.7917 - val_loss: 0.4853 - val_acc: 0.7448\n",
      "Epoch 863/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4309 - acc: 0.7917 - val_loss: 0.4853 - val_acc: 0.7448\n",
      "Epoch 864/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4309 - acc: 0.7917 - val_loss: 0.4853 - val_acc: 0.7448\n",
      "Epoch 865/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4309 - acc: 0.7934 - val_loss: 0.4854 - val_acc: 0.7448\n",
      "Epoch 866/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4308 - acc: 0.7934 - val_loss: 0.4854 - val_acc: 0.7448\n",
      "Epoch 867/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4308 - acc: 0.7934 - val_loss: 0.4854 - val_acc: 0.7448\n",
      "Epoch 868/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4308 - acc: 0.7934 - val_loss: 0.4854 - val_acc: 0.7448\n",
      "Epoch 869/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4308 - acc: 0.7934 - val_loss: 0.4855 - val_acc: 0.7448\n",
      "Epoch 870/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4307 - acc: 0.7934 - val_loss: 0.4855 - val_acc: 0.7448\n",
      "Epoch 871/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4307 - acc: 0.7934 - val_loss: 0.4855 - val_acc: 0.7448\n",
      "Epoch 872/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4307 - acc: 0.7934 - val_loss: 0.4855 - val_acc: 0.7448\n",
      "Epoch 873/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4307 - acc: 0.7934 - val_loss: 0.4856 - val_acc: 0.7448\n",
      "Epoch 874/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4307 - acc: 0.7934 - val_loss: 0.4856 - val_acc: 0.7448\n",
      "Epoch 875/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4306 - acc: 0.7934 - val_loss: 0.4856 - val_acc: 0.7448\n",
      "Epoch 876/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4306 - acc: 0.7934 - val_loss: 0.4856 - val_acc: 0.7448\n",
      "Epoch 877/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4306 - acc: 0.7934 - val_loss: 0.4857 - val_acc: 0.7448\n",
      "Epoch 878/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4306 - acc: 0.7934 - val_loss: 0.4857 - val_acc: 0.7448\n",
      "Epoch 879/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4305 - acc: 0.7934 - val_loss: 0.4857 - val_acc: 0.7448\n",
      "Epoch 880/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4305 - acc: 0.7934 - val_loss: 0.4857 - val_acc: 0.7448\n",
      "Epoch 881/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4305 - acc: 0.7934 - val_loss: 0.4858 - val_acc: 0.7448\n",
      "Epoch 882/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4305 - acc: 0.7934 - val_loss: 0.4858 - val_acc: 0.7448\n",
      "Epoch 883/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4304 - acc: 0.7934 - val_loss: 0.4858 - val_acc: 0.7448\n",
      "Epoch 884/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4304 - acc: 0.7934 - val_loss: 0.4858 - val_acc: 0.7448\n",
      "Epoch 885/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4304 - acc: 0.7934 - val_loss: 0.4859 - val_acc: 0.7448\n",
      "Epoch 886/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4304 - acc: 0.7934 - val_loss: 0.4859 - val_acc: 0.7448\n",
      "Epoch 887/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4304 - acc: 0.7934 - val_loss: 0.4859 - val_acc: 0.7448\n",
      "Epoch 888/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4303 - acc: 0.7934 - val_loss: 0.4859 - val_acc: 0.7448\n",
      "Epoch 889/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4303 - acc: 0.7934 - val_loss: 0.4860 - val_acc: 0.7448\n",
      "Epoch 890/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4303 - acc: 0.7934 - val_loss: 0.4860 - val_acc: 0.7448\n",
      "Epoch 891/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4303 - acc: 0.7934 - val_loss: 0.4860 - val_acc: 0.7448\n",
      "Epoch 892/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4302 - acc: 0.7934 - val_loss: 0.4860 - val_acc: 0.7448\n",
      "Epoch 893/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4302 - acc: 0.7934 - val_loss: 0.4861 - val_acc: 0.7448\n",
      "Epoch 894/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4302 - acc: 0.7934 - val_loss: 0.4861 - val_acc: 0.7448\n",
      "Epoch 895/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 54us/step - loss: 0.4302 - acc: 0.7934 - val_loss: 0.4861 - val_acc: 0.7448\n",
      "Epoch 896/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4302 - acc: 0.7934 - val_loss: 0.4862 - val_acc: 0.7448\n",
      "Epoch 897/1000\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4301 - acc: 0.7934 - val_loss: 0.4862 - val_acc: 0.7448\n",
      "Epoch 898/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4302 - acc: 0.7934 - val_loss: 0.4862 - val_acc: 0.7448\n",
      "Epoch 899/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4301 - acc: 0.7934 - val_loss: 0.4862 - val_acc: 0.7448\n",
      "Epoch 900/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4301 - acc: 0.7934 - val_loss: 0.4863 - val_acc: 0.7448\n",
      "Epoch 901/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4300 - acc: 0.7934 - val_loss: 0.4863 - val_acc: 0.7448\n",
      "Epoch 902/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4300 - acc: 0.7934 - val_loss: 0.4863 - val_acc: 0.7448\n",
      "Epoch 903/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4300 - acc: 0.7934 - val_loss: 0.4863 - val_acc: 0.7448\n",
      "Epoch 904/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4300 - acc: 0.7934 - val_loss: 0.4864 - val_acc: 0.7448\n",
      "Epoch 905/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4300 - acc: 0.7934 - val_loss: 0.4864 - val_acc: 0.7448\n",
      "Epoch 906/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4299 - acc: 0.7934 - val_loss: 0.4864 - val_acc: 0.7448\n",
      "Epoch 907/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4299 - acc: 0.7934 - val_loss: 0.4865 - val_acc: 0.7448\n",
      "Epoch 908/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4299 - acc: 0.7934 - val_loss: 0.4865 - val_acc: 0.7448\n",
      "Epoch 909/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4299 - acc: 0.7934 - val_loss: 0.4865 - val_acc: 0.7448\n",
      "Epoch 910/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4299 - acc: 0.7934 - val_loss: 0.4865 - val_acc: 0.7448\n",
      "Epoch 911/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4298 - acc: 0.7934 - val_loss: 0.4866 - val_acc: 0.7448\n",
      "Epoch 912/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4298 - acc: 0.7934 - val_loss: 0.4866 - val_acc: 0.7448\n",
      "Epoch 913/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4298 - acc: 0.7934 - val_loss: 0.4866 - val_acc: 0.7448\n",
      "Epoch 914/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4298 - acc: 0.7934 - val_loss: 0.4867 - val_acc: 0.7448\n",
      "Epoch 915/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4298 - acc: 0.7934 - val_loss: 0.4867 - val_acc: 0.7448\n",
      "Epoch 916/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4297 - acc: 0.7934 - val_loss: 0.4867 - val_acc: 0.7448\n",
      "Epoch 917/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4297 - acc: 0.7934 - val_loss: 0.4868 - val_acc: 0.7448\n",
      "Epoch 918/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4297 - acc: 0.7934 - val_loss: 0.4868 - val_acc: 0.7448\n",
      "Epoch 919/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4297 - acc: 0.7934 - val_loss: 0.4868 - val_acc: 0.7448\n",
      "Epoch 920/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4296 - acc: 0.7934 - val_loss: 0.4868 - val_acc: 0.7448\n",
      "Epoch 921/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4296 - acc: 0.7934 - val_loss: 0.4869 - val_acc: 0.7448\n",
      "Epoch 922/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4296 - acc: 0.7934 - val_loss: 0.4869 - val_acc: 0.7448\n",
      "Epoch 923/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4296 - acc: 0.7934 - val_loss: 0.4869 - val_acc: 0.7448\n",
      "Epoch 924/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4295 - acc: 0.7934 - val_loss: 0.4870 - val_acc: 0.7448\n",
      "Epoch 925/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4295 - acc: 0.7934 - val_loss: 0.4870 - val_acc: 0.7448\n",
      "Epoch 926/1000\n",
      "576/576 [==============================] - 0s 92us/step - loss: 0.4295 - acc: 0.7934 - val_loss: 0.4870 - val_acc: 0.7448\n",
      "Epoch 927/1000\n",
      "576/576 [==============================] - 0s 142us/step - loss: 0.4295 - acc: 0.7934 - val_loss: 0.4870 - val_acc: 0.7448\n",
      "Epoch 928/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4294 - acc: 0.7934 - val_loss: 0.4871 - val_acc: 0.7448\n",
      "Epoch 929/1000\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4294 - acc: 0.7934 - val_loss: 0.4871 - val_acc: 0.7448\n",
      "Epoch 930/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4294 - acc: 0.7934 - val_loss: 0.4871 - val_acc: 0.7448\n",
      "Epoch 931/1000\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4294 - acc: 0.7934 - val_loss: 0.4872 - val_acc: 0.7448\n",
      "Epoch 932/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4294 - acc: 0.7934 - val_loss: 0.4872 - val_acc: 0.7448\n",
      "Epoch 933/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4293 - acc: 0.7934 - val_loss: 0.4872 - val_acc: 0.7448\n",
      "Epoch 934/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4293 - acc: 0.7934 - val_loss: 0.4873 - val_acc: 0.7448\n",
      "Epoch 935/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4293 - acc: 0.7934 - val_loss: 0.4873 - val_acc: 0.7448\n",
      "Epoch 936/1000\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.4293 - acc: 0.7934 - val_loss: 0.4873 - val_acc: 0.7448\n",
      "Epoch 937/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4292 - acc: 0.7934 - val_loss: 0.4873 - val_acc: 0.7448\n",
      "Epoch 938/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4293 - acc: 0.7934 - val_loss: 0.4874 - val_acc: 0.7448\n",
      "Epoch 939/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4292 - acc: 0.7934 - val_loss: 0.4874 - val_acc: 0.7448\n",
      "Epoch 940/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4292 - acc: 0.7934 - val_loss: 0.4874 - val_acc: 0.7448\n",
      "Epoch 941/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4292 - acc: 0.7934 - val_loss: 0.4874 - val_acc: 0.7448\n",
      "Epoch 942/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4292 - acc: 0.7934 - val_loss: 0.4875 - val_acc: 0.7448\n",
      "Epoch 943/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4291 - acc: 0.7934 - val_loss: 0.4875 - val_acc: 0.7448\n",
      "Epoch 944/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4291 - acc: 0.7934 - val_loss: 0.4875 - val_acc: 0.7448\n",
      "Epoch 945/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4291 - acc: 0.7934 - val_loss: 0.4876 - val_acc: 0.7448\n",
      "Epoch 946/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4290 - acc: 0.7934 - val_loss: 0.4876 - val_acc: 0.7448\n",
      "Epoch 947/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4290 - acc: 0.7934 - val_loss: 0.4876 - val_acc: 0.7448\n",
      "Epoch 948/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4290 - acc: 0.7934 - val_loss: 0.4876 - val_acc: 0.7448\n",
      "Epoch 949/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4290 - acc: 0.7934 - val_loss: 0.4877 - val_acc: 0.7448\n",
      "Epoch 950/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4290 - acc: 0.7934 - val_loss: 0.4877 - val_acc: 0.7448\n",
      "Epoch 951/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4289 - acc: 0.7934 - val_loss: 0.4877 - val_acc: 0.7448\n",
      "Epoch 952/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4289 - acc: 0.7934 - val_loss: 0.4877 - val_acc: 0.7448\n",
      "Epoch 953/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4289 - acc: 0.7934 - val_loss: 0.4878 - val_acc: 0.7448\n",
      "Epoch 954/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4289 - acc: 0.7934 - val_loss: 0.4878 - val_acc: 0.7448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 955/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4289 - acc: 0.7934 - val_loss: 0.4878 - val_acc: 0.7500\n",
      "Epoch 956/1000\n",
      "576/576 [==============================] - 0s 88us/step - loss: 0.4288 - acc: 0.7934 - val_loss: 0.4878 - val_acc: 0.7500\n",
      "Epoch 957/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4288 - acc: 0.7934 - val_loss: 0.4879 - val_acc: 0.7500\n",
      "Epoch 958/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4288 - acc: 0.7934 - val_loss: 0.4879 - val_acc: 0.7500\n",
      "Epoch 959/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4288 - acc: 0.7934 - val_loss: 0.4879 - val_acc: 0.7500\n",
      "Epoch 960/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4288 - acc: 0.7934 - val_loss: 0.4879 - val_acc: 0.7500\n",
      "Epoch 961/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4288 - acc: 0.7934 - val_loss: 0.4880 - val_acc: 0.7500\n",
      "Epoch 962/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4287 - acc: 0.7934 - val_loss: 0.4880 - val_acc: 0.7500\n",
      "Epoch 963/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4287 - acc: 0.7934 - val_loss: 0.4880 - val_acc: 0.7500\n",
      "Epoch 964/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4287 - acc: 0.7934 - val_loss: 0.4881 - val_acc: 0.7500\n",
      "Epoch 965/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4287 - acc: 0.7934 - val_loss: 0.4881 - val_acc: 0.7500\n",
      "Epoch 966/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4286 - acc: 0.7934 - val_loss: 0.4881 - val_acc: 0.7500\n",
      "Epoch 967/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4286 - acc: 0.7934 - val_loss: 0.4881 - val_acc: 0.7500\n",
      "Epoch 968/1000\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.4286 - acc: 0.7934 - val_loss: 0.4882 - val_acc: 0.7500\n",
      "Epoch 969/1000\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.4722 - acc: 0.750 - 0s 83us/step - loss: 0.4286 - acc: 0.7934 - val_loss: 0.4882 - val_acc: 0.7500\n",
      "Epoch 970/1000\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4286 - acc: 0.7934 - val_loss: 0.4882 - val_acc: 0.7500\n",
      "Epoch 971/1000\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.4285 - acc: 0.7934 - val_loss: 0.4882 - val_acc: 0.7500\n",
      "Epoch 972/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4285 - acc: 0.7934 - val_loss: 0.4883 - val_acc: 0.7500\n",
      "Epoch 973/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4285 - acc: 0.7934 - val_loss: 0.4883 - val_acc: 0.7500\n",
      "Epoch 974/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4285 - acc: 0.7934 - val_loss: 0.4883 - val_acc: 0.7500\n",
      "Epoch 975/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4285 - acc: 0.7934 - val_loss: 0.4883 - val_acc: 0.7500\n",
      "Epoch 976/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4285 - acc: 0.7934 - val_loss: 0.4884 - val_acc: 0.7500\n",
      "Epoch 977/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4284 - acc: 0.7934 - val_loss: 0.4884 - val_acc: 0.7500\n",
      "Epoch 978/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4284 - acc: 0.7934 - val_loss: 0.4884 - val_acc: 0.7500\n",
      "Epoch 979/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4284 - acc: 0.7934 - val_loss: 0.4884 - val_acc: 0.7500\n",
      "Epoch 980/1000\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4284 - acc: 0.7934 - val_loss: 0.4885 - val_acc: 0.7500\n",
      "Epoch 981/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4283 - acc: 0.7934 - val_loss: 0.4885 - val_acc: 0.7500\n",
      "Epoch 982/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4283 - acc: 0.7934 - val_loss: 0.4885 - val_acc: 0.7500\n",
      "Epoch 983/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4283 - acc: 0.7934 - val_loss: 0.4885 - val_acc: 0.7500\n",
      "Epoch 984/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4283 - acc: 0.7934 - val_loss: 0.4885 - val_acc: 0.7500\n",
      "Epoch 985/1000\n",
      "576/576 [==============================] - 0s 83us/step - loss: 0.4283 - acc: 0.7934 - val_loss: 0.4886 - val_acc: 0.7500\n",
      "Epoch 986/1000\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4283 - acc: 0.7934 - val_loss: 0.4886 - val_acc: 0.7500\n",
      "Epoch 987/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4282 - acc: 0.7934 - val_loss: 0.4886 - val_acc: 0.7500\n",
      "Epoch 988/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4282 - acc: 0.7934 - val_loss: 0.4886 - val_acc: 0.7500\n",
      "Epoch 989/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4282 - acc: 0.7934 - val_loss: 0.4887 - val_acc: 0.7500\n",
      "Epoch 990/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4282 - acc: 0.7934 - val_loss: 0.4887 - val_acc: 0.7500\n",
      "Epoch 991/1000\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4281 - acc: 0.7934 - val_loss: 0.4887 - val_acc: 0.7500\n",
      "Epoch 992/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4281 - acc: 0.7934 - val_loss: 0.4887 - val_acc: 0.7500\n",
      "Epoch 993/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4281 - acc: 0.7934 - val_loss: 0.4888 - val_acc: 0.7500\n",
      "Epoch 994/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4281 - acc: 0.7934 - val_loss: 0.4888 - val_acc: 0.7500\n",
      "Epoch 995/1000\n",
      "576/576 [==============================] - 0s 95us/step - loss: 0.4281 - acc: 0.7934 - val_loss: 0.4888 - val_acc: 0.7500\n",
      "Epoch 996/1000\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4280 - acc: 0.7934 - val_loss: 0.4888 - val_acc: 0.7500\n",
      "Epoch 997/1000\n",
      "576/576 [==============================] - 0s 103us/step - loss: 0.4280 - acc: 0.7934 - val_loss: 0.4888 - val_acc: 0.7500\n",
      "Epoch 998/1000\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4280 - acc: 0.7934 - val_loss: 0.4889 - val_acc: 0.7500\n",
      "Epoch 999/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4280 - acc: 0.7934 - val_loss: 0.4889 - val_acc: 0.7500\n",
      "Epoch 1000/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4280 - acc: 0.7934 - val_loss: 0.4889 - val_acc: 0.7500\n"
     ]
    }
   ],
   "source": [
    "## Note that when we call \"fit\" again, it picks up where it left off\n",
    "run_hist_1b = model_1.fit(X_train_norm, y_train, validation_data=(X_test_norm, y_test), epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1a27493910>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = len(run_hist_1.history[\"loss\"])\n",
    "m = len(run_hist_1b.history['loss'])\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "ax.plot(range(n), run_hist_1.history[\"loss\"],'r', marker='.', label=\"Train Loss - Run 1\")\n",
    "ax.plot(range(n, n+m), run_hist_1b.history[\"loss\"], 'hotpink', marker='.', label=\"Train Loss - Run 2\")\n",
    "\n",
    "ax.plot(range(n), run_hist_1.history[\"val_loss\"],'b', marker='.', label=\"Validation Loss - Run 1\")\n",
    "ax.plot(range(n, n+m), run_hist_1b.history[\"val_loss\"], 'LightSkyBlue', marker='.',  label=\"Validation Loss - Run 2\")\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this graph begins where the other left off.  While the training loss is still going down, it looks like the validation loss has stabilized (or even gotten worse!).  This suggests that our network will not benefit from further training.  What is the appropriate number of epochs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "Now it's your turn.  Do the following in the cells below:\n",
    "- Build a model with two hidden layers, each with 6 nodes\n",
    "- Use the \"relu\" activation function for the hidden layers, and \"sigmoid\" for the final layer\n",
    "- Use a learning rate of .003 and train for 1500 epochs\n",
    "- Graph the trajectory of the loss functions, accuracy on both train and test set\n",
    "- Plot the roc curve for the predictions\n",
    "\n",
    "Experiment with different learning rates, numbers of epochs, and network structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 576 samples, validate on 192 samples\n",
      "Epoch 1/1500\n",
      "576/576 [==============================] - 0s 617us/step - loss: 0.8348 - acc: 0.5799 - val_loss: 0.8796 - val_acc: 0.5885\n",
      "Epoch 2/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.8247 - acc: 0.5816 - val_loss: 0.8687 - val_acc: 0.5938\n",
      "Epoch 3/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.8150 - acc: 0.5885 - val_loss: 0.8584 - val_acc: 0.5938\n",
      "Epoch 4/1500\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.8057 - acc: 0.5903 - val_loss: 0.8484 - val_acc: 0.6094\n",
      "Epoch 5/1500\n",
      "576/576 [==============================] - 0s 86us/step - loss: 0.7967 - acc: 0.5920 - val_loss: 0.8389 - val_acc: 0.6146\n",
      "Epoch 6/1500\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.7881 - acc: 0.5972 - val_loss: 0.8298 - val_acc: 0.6094\n",
      "Epoch 7/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.7799 - acc: 0.5955 - val_loss: 0.8211 - val_acc: 0.6042\n",
      "Epoch 8/1500\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.7721 - acc: 0.5955 - val_loss: 0.8127 - val_acc: 0.6042\n",
      "Epoch 9/1500\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.7645 - acc: 0.5972 - val_loss: 0.8047 - val_acc: 0.5990\n",
      "Epoch 10/1500\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.7572 - acc: 0.5990 - val_loss: 0.7971 - val_acc: 0.5990\n",
      "Epoch 11/1500\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.7504 - acc: 0.6059 - val_loss: 0.7897 - val_acc: 0.6042\n",
      "Epoch 12/1500\n",
      "576/576 [==============================] - 0s 84us/step - loss: 0.7437 - acc: 0.6076 - val_loss: 0.7825 - val_acc: 0.6094\n",
      "Epoch 13/1500\n",
      "576/576 [==============================] - 0s 85us/step - loss: 0.7374 - acc: 0.6076 - val_loss: 0.7757 - val_acc: 0.6198\n",
      "Epoch 14/1500\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.7313 - acc: 0.6128 - val_loss: 0.7691 - val_acc: 0.6198\n",
      "Epoch 15/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.7254 - acc: 0.6128 - val_loss: 0.7627 - val_acc: 0.6198\n",
      "Epoch 16/1500\n",
      "576/576 [==============================] - 0s 84us/step - loss: 0.7197 - acc: 0.6128 - val_loss: 0.7565 - val_acc: 0.6250\n",
      "Epoch 17/1500\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.7142 - acc: 0.6146 - val_loss: 0.7506 - val_acc: 0.6354\n",
      "Epoch 18/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.7090 - acc: 0.6198 - val_loss: 0.7449 - val_acc: 0.6354\n",
      "Epoch 19/1500\n",
      "576/576 [==============================] - 0s 87us/step - loss: 0.7039 - acc: 0.6233 - val_loss: 0.7393 - val_acc: 0.6354\n",
      "Epoch 20/1500\n",
      "576/576 [==============================] - 0s 83us/step - loss: 0.6989 - acc: 0.6267 - val_loss: 0.7339 - val_acc: 0.6354\n",
      "Epoch 21/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.6942 - acc: 0.6372 - val_loss: 0.7286 - val_acc: 0.6406\n",
      "Epoch 22/1500\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.6896 - acc: 0.6424 - val_loss: 0.7235 - val_acc: 0.6406\n",
      "Epoch 23/1500\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.6850 - acc: 0.6458 - val_loss: 0.7186 - val_acc: 0.6458\n",
      "Epoch 24/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.6807 - acc: 0.6476 - val_loss: 0.7138 - val_acc: 0.6510\n",
      "Epoch 25/1500\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.6764 - acc: 0.6476 - val_loss: 0.7092 - val_acc: 0.6510\n",
      "Epoch 26/1500\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.6722 - acc: 0.6493 - val_loss: 0.7047 - val_acc: 0.6406\n",
      "Epoch 27/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.6683 - acc: 0.6510 - val_loss: 0.7003 - val_acc: 0.6458\n",
      "Epoch 28/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.6643 - acc: 0.6510 - val_loss: 0.6961 - val_acc: 0.6458\n",
      "Epoch 29/1500\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.6606 - acc: 0.6528 - val_loss: 0.6920 - val_acc: 0.6458\n",
      "Epoch 30/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.6569 - acc: 0.6510 - val_loss: 0.6881 - val_acc: 0.6510\n",
      "Epoch 31/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.6534 - acc: 0.6562 - val_loss: 0.6842 - val_acc: 0.6562\n",
      "Epoch 32/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.6499 - acc: 0.6580 - val_loss: 0.6804 - val_acc: 0.6562\n",
      "Epoch 33/1500\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.6465 - acc: 0.6562 - val_loss: 0.6767 - val_acc: 0.6615\n",
      "Epoch 34/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.6432 - acc: 0.6615 - val_loss: 0.6731 - val_acc: 0.6667\n",
      "Epoch 35/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.6400 - acc: 0.6597 - val_loss: 0.6696 - val_acc: 0.6562\n",
      "Epoch 36/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.6368 - acc: 0.6667 - val_loss: 0.6661 - val_acc: 0.6615\n",
      "Epoch 37/1500\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.6337 - acc: 0.6771 - val_loss: 0.6627 - val_acc: 0.6615\n",
      "Epoch 38/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.6307 - acc: 0.6771 - val_loss: 0.6593 - val_acc: 0.6615\n",
      "Epoch 39/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.6277 - acc: 0.6753 - val_loss: 0.6561 - val_acc: 0.6615\n",
      "Epoch 40/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.6248 - acc: 0.6753 - val_loss: 0.6529 - val_acc: 0.6615\n",
      "Epoch 41/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.6220 - acc: 0.6771 - val_loss: 0.6499 - val_acc: 0.6667\n",
      "Epoch 42/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.6193 - acc: 0.6771 - val_loss: 0.6468 - val_acc: 0.6667\n",
      "Epoch 43/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.6165 - acc: 0.6771 - val_loss: 0.6439 - val_acc: 0.6667\n",
      "Epoch 44/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.6139 - acc: 0.6771 - val_loss: 0.6410 - val_acc: 0.6667\n",
      "Epoch 45/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.6113 - acc: 0.6823 - val_loss: 0.6382 - val_acc: 0.6667\n",
      "Epoch 46/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.6087 - acc: 0.6858 - val_loss: 0.6354 - val_acc: 0.6719\n",
      "Epoch 47/1500\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.6061 - acc: 0.6858 - val_loss: 0.6326 - val_acc: 0.6771\n",
      "Epoch 48/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.6036 - acc: 0.6875 - val_loss: 0.6299 - val_acc: 0.6771\n",
      "Epoch 49/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.6011 - acc: 0.6944 - val_loss: 0.6273 - val_acc: 0.6771\n",
      "Epoch 50/1500\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.7120 - acc: 0.531 - 0s 67us/step - loss: 0.5987 - acc: 0.6944 - val_loss: 0.6247 - val_acc: 0.6875\n",
      "Epoch 51/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.5963 - acc: 0.6997 - val_loss: 0.6221 - val_acc: 0.6875\n",
      "Epoch 52/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.5939 - acc: 0.6997 - val_loss: 0.6196 - val_acc: 0.6927\n",
      "Epoch 53/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.5917 - acc: 0.6997 - val_loss: 0.6172 - val_acc: 0.6927\n",
      "Epoch 54/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.5894 - acc: 0.6997 - val_loss: 0.6148 - val_acc: 0.6927\n",
      "Epoch 55/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.5872 - acc: 0.7031 - val_loss: 0.6125 - val_acc: 0.6979\n",
      "Epoch 56/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.5850 - acc: 0.7031 - val_loss: 0.6102 - val_acc: 0.7031\n",
      "Epoch 57/1500\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.5828 - acc: 0.7049 - val_loss: 0.6080 - val_acc: 0.7083\n",
      "Epoch 58/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.5807 - acc: 0.7101 - val_loss: 0.6058 - val_acc: 0.7083\n",
      "Epoch 59/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.5786 - acc: 0.7083 - val_loss: 0.6036 - val_acc: 0.7135\n",
      "Epoch 60/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 68us/step - loss: 0.5766 - acc: 0.7118 - val_loss: 0.6015 - val_acc: 0.7188\n",
      "Epoch 61/1500\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.5746 - acc: 0.7135 - val_loss: 0.5995 - val_acc: 0.7188\n",
      "Epoch 62/1500\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.5726 - acc: 0.7153 - val_loss: 0.5975 - val_acc: 0.7240\n",
      "Epoch 63/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.5706 - acc: 0.7170 - val_loss: 0.5955 - val_acc: 0.7240\n",
      "Epoch 64/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.5687 - acc: 0.7188 - val_loss: 0.5935 - val_acc: 0.7240\n",
      "Epoch 65/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.5668 - acc: 0.7170 - val_loss: 0.5916 - val_acc: 0.7240\n",
      "Epoch 66/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.5649 - acc: 0.7205 - val_loss: 0.5898 - val_acc: 0.7240\n",
      "Epoch 67/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.5631 - acc: 0.7205 - val_loss: 0.5879 - val_acc: 0.7292\n",
      "Epoch 68/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.5613 - acc: 0.7240 - val_loss: 0.5861 - val_acc: 0.7292\n",
      "Epoch 69/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.5596 - acc: 0.7257 - val_loss: 0.5844 - val_acc: 0.7344\n",
      "Epoch 70/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.5579 - acc: 0.7274 - val_loss: 0.5827 - val_acc: 0.7344\n",
      "Epoch 71/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.5562 - acc: 0.7326 - val_loss: 0.5810 - val_acc: 0.7292\n",
      "Epoch 72/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.5545 - acc: 0.7344 - val_loss: 0.5794 - val_acc: 0.7344\n",
      "Epoch 73/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.5529 - acc: 0.7344 - val_loss: 0.5778 - val_acc: 0.7292\n",
      "Epoch 74/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.5513 - acc: 0.7361 - val_loss: 0.5762 - val_acc: 0.7292\n",
      "Epoch 75/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.5497 - acc: 0.7378 - val_loss: 0.5747 - val_acc: 0.7396\n",
      "Epoch 76/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.5482 - acc: 0.7396 - val_loss: 0.5732 - val_acc: 0.7396\n",
      "Epoch 77/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.5467 - acc: 0.7396 - val_loss: 0.5717 - val_acc: 0.7396\n",
      "Epoch 78/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.5451 - acc: 0.7431 - val_loss: 0.5702 - val_acc: 0.7396\n",
      "Epoch 79/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.5437 - acc: 0.7448 - val_loss: 0.5688 - val_acc: 0.7396\n",
      "Epoch 80/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.5423 - acc: 0.7465 - val_loss: 0.5673 - val_acc: 0.7396\n",
      "Epoch 81/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.5408 - acc: 0.7465 - val_loss: 0.5660 - val_acc: 0.7396\n",
      "Epoch 82/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.5394 - acc: 0.7465 - val_loss: 0.5646 - val_acc: 0.7344\n",
      "Epoch 83/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.5381 - acc: 0.7448 - val_loss: 0.5633 - val_acc: 0.7344\n",
      "Epoch 84/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.5368 - acc: 0.7431 - val_loss: 0.5620 - val_acc: 0.7344\n",
      "Epoch 85/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.5354 - acc: 0.7431 - val_loss: 0.5607 - val_acc: 0.7344\n",
      "Epoch 86/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.5341 - acc: 0.7483 - val_loss: 0.5595 - val_acc: 0.7344\n",
      "Epoch 87/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.5329 - acc: 0.7483 - val_loss: 0.5583 - val_acc: 0.7344\n",
      "Epoch 88/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.5316 - acc: 0.7483 - val_loss: 0.5571 - val_acc: 0.7344\n",
      "Epoch 89/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.5303 - acc: 0.7465 - val_loss: 0.5559 - val_acc: 0.7344\n",
      "Epoch 90/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.5291 - acc: 0.7483 - val_loss: 0.5547 - val_acc: 0.7344\n",
      "Epoch 91/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.5279 - acc: 0.7483 - val_loss: 0.5536 - val_acc: 0.7448\n",
      "Epoch 92/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.5267 - acc: 0.7500 - val_loss: 0.5524 - val_acc: 0.7448\n",
      "Epoch 93/1500\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.5256 - acc: 0.7535 - val_loss: 0.5513 - val_acc: 0.7448\n",
      "Epoch 94/1500\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.5244 - acc: 0.7517 - val_loss: 0.5502 - val_acc: 0.7448\n",
      "Epoch 95/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.5233 - acc: 0.7569 - val_loss: 0.5491 - val_acc: 0.7448\n",
      "Epoch 96/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.5222 - acc: 0.7569 - val_loss: 0.5480 - val_acc: 0.7448\n",
      "Epoch 97/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.5211 - acc: 0.7569 - val_loss: 0.5470 - val_acc: 0.7448\n",
      "Epoch 98/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.5200 - acc: 0.7552 - val_loss: 0.5460 - val_acc: 0.7448\n",
      "Epoch 99/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.5190 - acc: 0.7535 - val_loss: 0.5450 - val_acc: 0.7448\n",
      "Epoch 100/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.5179 - acc: 0.7535 - val_loss: 0.5440 - val_acc: 0.7448\n",
      "Epoch 101/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.5170 - acc: 0.7552 - val_loss: 0.5431 - val_acc: 0.7500\n",
      "Epoch 102/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.5160 - acc: 0.7552 - val_loss: 0.5422 - val_acc: 0.7500\n",
      "Epoch 103/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.5150 - acc: 0.7535 - val_loss: 0.5413 - val_acc: 0.7500\n",
      "Epoch 104/1500\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.5141 - acc: 0.7535 - val_loss: 0.5404 - val_acc: 0.7552\n",
      "Epoch 105/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.5132 - acc: 0.7569 - val_loss: 0.5396 - val_acc: 0.7552\n",
      "Epoch 106/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.5123 - acc: 0.7587 - val_loss: 0.5387 - val_acc: 0.7552\n",
      "Epoch 107/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.5114 - acc: 0.7587 - val_loss: 0.5379 - val_acc: 0.7604\n",
      "Epoch 108/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.5106 - acc: 0.7604 - val_loss: 0.5371 - val_acc: 0.7604\n",
      "Epoch 109/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.5097 - acc: 0.7587 - val_loss: 0.5363 - val_acc: 0.7604\n",
      "Epoch 110/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.5088 - acc: 0.7569 - val_loss: 0.5355 - val_acc: 0.7604\n",
      "Epoch 111/1500\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.5080 - acc: 0.7552 - val_loss: 0.5348 - val_acc: 0.7604\n",
      "Epoch 112/1500\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.5072 - acc: 0.7552 - val_loss: 0.5340 - val_acc: 0.7604\n",
      "Epoch 113/1500\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.5065 - acc: 0.7552 - val_loss: 0.5333 - val_acc: 0.7604\n",
      "Epoch 114/1500\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.5057 - acc: 0.7552 - val_loss: 0.5326 - val_acc: 0.7604\n",
      "Epoch 115/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.5049 - acc: 0.7639 - val_loss: 0.5320 - val_acc: 0.7604\n",
      "Epoch 116/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.5042 - acc: 0.7639 - val_loss: 0.5313 - val_acc: 0.7604\n",
      "Epoch 117/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.5035 - acc: 0.7639 - val_loss: 0.5307 - val_acc: 0.7604\n",
      "Epoch 118/1500\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.5027 - acc: 0.7622 - val_loss: 0.5300 - val_acc: 0.7604\n",
      "Epoch 119/1500\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.5020 - acc: 0.7622 - val_loss: 0.5294 - val_acc: 0.7604\n",
      "Epoch 120/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 77us/step - loss: 0.5014 - acc: 0.7622 - val_loss: 0.5288 - val_acc: 0.7604\n",
      "Epoch 121/1500\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.5007 - acc: 0.7604 - val_loss: 0.5282 - val_acc: 0.7604\n",
      "Epoch 122/1500\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.5000 - acc: 0.7604 - val_loss: 0.5276 - val_acc: 0.7604\n",
      "Epoch 123/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4994 - acc: 0.7622 - val_loss: 0.5271 - val_acc: 0.7604\n",
      "Epoch 124/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4988 - acc: 0.7622 - val_loss: 0.5265 - val_acc: 0.7604\n",
      "Epoch 125/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4982 - acc: 0.7622 - val_loss: 0.5260 - val_acc: 0.7656\n",
      "Epoch 126/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4975 - acc: 0.7604 - val_loss: 0.5255 - val_acc: 0.7656\n",
      "Epoch 127/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4969 - acc: 0.7604 - val_loss: 0.5250 - val_acc: 0.7656\n",
      "Epoch 128/1500\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4964 - acc: 0.7604 - val_loss: 0.5245 - val_acc: 0.7656\n",
      "Epoch 129/1500\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4958 - acc: 0.7622 - val_loss: 0.5240 - val_acc: 0.7656\n",
      "Epoch 130/1500\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.4952 - acc: 0.7622 - val_loss: 0.5235 - val_acc: 0.7708\n",
      "Epoch 131/1500\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4947 - acc: 0.7656 - val_loss: 0.5231 - val_acc: 0.7708\n",
      "Epoch 132/1500\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4941 - acc: 0.7639 - val_loss: 0.5226 - val_acc: 0.7708\n",
      "Epoch 133/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4936 - acc: 0.7639 - val_loss: 0.5222 - val_acc: 0.7708\n",
      "Epoch 134/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4931 - acc: 0.7639 - val_loss: 0.5218 - val_acc: 0.7760\n",
      "Epoch 135/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4926 - acc: 0.7639 - val_loss: 0.5213 - val_acc: 0.7760\n",
      "Epoch 136/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4921 - acc: 0.7622 - val_loss: 0.5209 - val_acc: 0.7760\n",
      "Epoch 137/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4916 - acc: 0.7622 - val_loss: 0.5205 - val_acc: 0.7760\n",
      "Epoch 138/1500\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4911 - acc: 0.7622 - val_loss: 0.5201 - val_acc: 0.7760\n",
      "Epoch 139/1500\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4906 - acc: 0.7622 - val_loss: 0.5197 - val_acc: 0.7760\n",
      "Epoch 140/1500\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.4901 - acc: 0.7656 - val_loss: 0.5193 - val_acc: 0.7760\n",
      "Epoch 141/1500\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4896 - acc: 0.7656 - val_loss: 0.5190 - val_acc: 0.7760\n",
      "Epoch 142/1500\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4892 - acc: 0.7674 - val_loss: 0.5186 - val_acc: 0.7760\n",
      "Epoch 143/1500\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4887 - acc: 0.7674 - val_loss: 0.5182 - val_acc: 0.7760\n",
      "Epoch 144/1500\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.5328 - acc: 0.687 - 0s 107us/step - loss: 0.4883 - acc: 0.7674 - val_loss: 0.5179 - val_acc: 0.7760\n",
      "Epoch 145/1500\n",
      "576/576 [==============================] - 0s 99us/step - loss: 0.4879 - acc: 0.7674 - val_loss: 0.5175 - val_acc: 0.7812\n",
      "Epoch 146/1500\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.4874 - acc: 0.7708 - val_loss: 0.5172 - val_acc: 0.7812\n",
      "Epoch 147/1500\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4870 - acc: 0.7726 - val_loss: 0.5168 - val_acc: 0.7812\n",
      "Epoch 148/1500\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4866 - acc: 0.7726 - val_loss: 0.5165 - val_acc: 0.7812\n",
      "Epoch 149/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4862 - acc: 0.7726 - val_loss: 0.5161 - val_acc: 0.7812\n",
      "Epoch 150/1500\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4857 - acc: 0.7708 - val_loss: 0.5158 - val_acc: 0.7812\n",
      "Epoch 151/1500\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.4853 - acc: 0.7708 - val_loss: 0.5155 - val_acc: 0.7865\n",
      "Epoch 152/1500\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4849 - acc: 0.7708 - val_loss: 0.5152 - val_acc: 0.7865\n",
      "Epoch 153/1500\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4846 - acc: 0.7743 - val_loss: 0.5149 - val_acc: 0.7865\n",
      "Epoch 154/1500\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4841 - acc: 0.7743 - val_loss: 0.5145 - val_acc: 0.7812\n",
      "Epoch 155/1500\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4838 - acc: 0.7743 - val_loss: 0.5142 - val_acc: 0.7812\n",
      "Epoch 156/1500\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.4834 - acc: 0.7743 - val_loss: 0.5140 - val_acc: 0.7865\n",
      "Epoch 157/1500\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4830 - acc: 0.7743 - val_loss: 0.5137 - val_acc: 0.7865\n",
      "Epoch 158/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4826 - acc: 0.7760 - val_loss: 0.5134 - val_acc: 0.7865\n",
      "Epoch 159/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4823 - acc: 0.7726 - val_loss: 0.5131 - val_acc: 0.7865\n",
      "Epoch 160/1500\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4819 - acc: 0.7743 - val_loss: 0.5129 - val_acc: 0.7865\n",
      "Epoch 161/1500\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4816 - acc: 0.7743 - val_loss: 0.5126 - val_acc: 0.7865\n",
      "Epoch 162/1500\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4812 - acc: 0.7743 - val_loss: 0.5123 - val_acc: 0.7865\n",
      "Epoch 163/1500\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4809 - acc: 0.7743 - val_loss: 0.5121 - val_acc: 0.7865\n",
      "Epoch 164/1500\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4806 - acc: 0.7743 - val_loss: 0.5118 - val_acc: 0.7865\n",
      "Epoch 165/1500\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.4802 - acc: 0.7760 - val_loss: 0.5116 - val_acc: 0.7865\n",
      "Epoch 166/1500\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4799 - acc: 0.7760 - val_loss: 0.5114 - val_acc: 0.7865\n",
      "Epoch 167/1500\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.4796 - acc: 0.7760 - val_loss: 0.5111 - val_acc: 0.7865\n",
      "Epoch 168/1500\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.4793 - acc: 0.7760 - val_loss: 0.5109 - val_acc: 0.7917\n",
      "Epoch 169/1500\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.4790 - acc: 0.7743 - val_loss: 0.5107 - val_acc: 0.7917\n",
      "Epoch 170/1500\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4787 - acc: 0.7760 - val_loss: 0.5105 - val_acc: 0.7917\n",
      "Epoch 171/1500\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.4784 - acc: 0.7760 - val_loss: 0.5103 - val_acc: 0.7865\n",
      "Epoch 172/1500\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4781 - acc: 0.7760 - val_loss: 0.5101 - val_acc: 0.7865\n",
      "Epoch 173/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4778 - acc: 0.7743 - val_loss: 0.5099 - val_acc: 0.7865\n",
      "Epoch 174/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4775 - acc: 0.7760 - val_loss: 0.5097 - val_acc: 0.7865\n",
      "Epoch 175/1500\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4773 - acc: 0.7760 - val_loss: 0.5095 - val_acc: 0.7865\n",
      "Epoch 176/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4770 - acc: 0.7778 - val_loss: 0.5093 - val_acc: 0.7865\n",
      "Epoch 177/1500\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4767 - acc: 0.7778 - val_loss: 0.5091 - val_acc: 0.7865\n",
      "Epoch 178/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4765 - acc: 0.7760 - val_loss: 0.5089 - val_acc: 0.7865\n",
      "Epoch 179/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 70us/step - loss: 0.4762 - acc: 0.7760 - val_loss: 0.5088 - val_acc: 0.7865\n",
      "Epoch 180/1500\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.4760 - acc: 0.7760 - val_loss: 0.5086 - val_acc: 0.7865\n",
      "Epoch 181/1500\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4757 - acc: 0.7760 - val_loss: 0.5084 - val_acc: 0.7865\n",
      "Epoch 182/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4755 - acc: 0.7760 - val_loss: 0.5083 - val_acc: 0.7865\n",
      "Epoch 183/1500\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4753 - acc: 0.7778 - val_loss: 0.5081 - val_acc: 0.7865\n",
      "Epoch 184/1500\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4750 - acc: 0.7760 - val_loss: 0.5080 - val_acc: 0.7812\n",
      "Epoch 185/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4748 - acc: 0.7743 - val_loss: 0.5079 - val_acc: 0.7812\n",
      "Epoch 186/1500\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4746 - acc: 0.7760 - val_loss: 0.5077 - val_acc: 0.7812\n",
      "Epoch 187/1500\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4743 - acc: 0.7743 - val_loss: 0.5076 - val_acc: 0.7760\n",
      "Epoch 188/1500\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.4741 - acc: 0.7743 - val_loss: 0.5074 - val_acc: 0.7865\n",
      "Epoch 189/1500\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4739 - acc: 0.7726 - val_loss: 0.5073 - val_acc: 0.7865\n",
      "Epoch 190/1500\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4737 - acc: 0.7726 - val_loss: 0.5072 - val_acc: 0.7865\n",
      "Epoch 191/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4734 - acc: 0.7743 - val_loss: 0.5071 - val_acc: 0.7865\n",
      "Epoch 192/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4733 - acc: 0.7726 - val_loss: 0.5069 - val_acc: 0.7812\n",
      "Epoch 193/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4730 - acc: 0.7726 - val_loss: 0.5068 - val_acc: 0.7812\n",
      "Epoch 194/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4728 - acc: 0.7726 - val_loss: 0.5067 - val_acc: 0.7812\n",
      "Epoch 195/1500\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4726 - acc: 0.7743 - val_loss: 0.5065 - val_acc: 0.7812\n",
      "Epoch 196/1500\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4724 - acc: 0.7726 - val_loss: 0.5064 - val_acc: 0.7812\n",
      "Epoch 197/1500\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4723 - acc: 0.7726 - val_loss: 0.5063 - val_acc: 0.7812\n",
      "Epoch 198/1500\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.4721 - acc: 0.7743 - val_loss: 0.5061 - val_acc: 0.7812\n",
      "Epoch 199/1500\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.4719 - acc: 0.7726 - val_loss: 0.5060 - val_acc: 0.7812\n",
      "Epoch 200/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4717 - acc: 0.7760 - val_loss: 0.5059 - val_acc: 0.7812\n",
      "Epoch 201/1500\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.4715 - acc: 0.7726 - val_loss: 0.5058 - val_acc: 0.7812\n",
      "Epoch 202/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4713 - acc: 0.7743 - val_loss: 0.5057 - val_acc: 0.7812\n",
      "Epoch 203/1500\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4712 - acc: 0.7726 - val_loss: 0.5056 - val_acc: 0.7812\n",
      "Epoch 204/1500\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.4710 - acc: 0.7743 - val_loss: 0.5054 - val_acc: 0.7812\n",
      "Epoch 205/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4708 - acc: 0.7726 - val_loss: 0.5053 - val_acc: 0.7812\n",
      "Epoch 206/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4706 - acc: 0.7726 - val_loss: 0.5052 - val_acc: 0.7812\n",
      "Epoch 207/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4705 - acc: 0.7726 - val_loss: 0.5051 - val_acc: 0.7812\n",
      "Epoch 208/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4703 - acc: 0.7743 - val_loss: 0.5050 - val_acc: 0.7812\n",
      "Epoch 209/1500\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4702 - acc: 0.7726 - val_loss: 0.5049 - val_acc: 0.7812\n",
      "Epoch 210/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4700 - acc: 0.7726 - val_loss: 0.5048 - val_acc: 0.7812\n",
      "Epoch 211/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4699 - acc: 0.7726 - val_loss: 0.5048 - val_acc: 0.7812\n",
      "Epoch 212/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4697 - acc: 0.7726 - val_loss: 0.5047 - val_acc: 0.7812\n",
      "Epoch 213/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4696 - acc: 0.7726 - val_loss: 0.5046 - val_acc: 0.7812\n",
      "Epoch 214/1500\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4694 - acc: 0.7726 - val_loss: 0.5045 - val_acc: 0.7812\n",
      "Epoch 215/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4693 - acc: 0.7726 - val_loss: 0.5044 - val_acc: 0.7812\n",
      "Epoch 216/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4691 - acc: 0.7726 - val_loss: 0.5043 - val_acc: 0.7812\n",
      "Epoch 217/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4690 - acc: 0.7726 - val_loss: 0.5042 - val_acc: 0.7812\n",
      "Epoch 218/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4688 - acc: 0.7726 - val_loss: 0.5042 - val_acc: 0.7812\n",
      "Epoch 219/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4686 - acc: 0.7726 - val_loss: 0.5041 - val_acc: 0.7812\n",
      "Epoch 220/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4685 - acc: 0.7726 - val_loss: 0.5040 - val_acc: 0.7812\n",
      "Epoch 221/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4684 - acc: 0.7726 - val_loss: 0.5039 - val_acc: 0.7812\n",
      "Epoch 222/1500\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4683 - acc: 0.7726 - val_loss: 0.5039 - val_acc: 0.7812\n",
      "Epoch 223/1500\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4681 - acc: 0.7743 - val_loss: 0.5038 - val_acc: 0.7812\n",
      "Epoch 224/1500\n",
      "576/576 [==============================] - 0s 79us/step - loss: 0.4680 - acc: 0.7743 - val_loss: 0.5037 - val_acc: 0.7812\n",
      "Epoch 225/1500\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4679 - acc: 0.7743 - val_loss: 0.5037 - val_acc: 0.7812\n",
      "Epoch 226/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4677 - acc: 0.7760 - val_loss: 0.5036 - val_acc: 0.7812\n",
      "Epoch 227/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4676 - acc: 0.7743 - val_loss: 0.5035 - val_acc: 0.7760\n",
      "Epoch 228/1500\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4675 - acc: 0.7743 - val_loss: 0.5035 - val_acc: 0.7760\n",
      "Epoch 229/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4674 - acc: 0.7760 - val_loss: 0.5034 - val_acc: 0.7760\n",
      "Epoch 230/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4673 - acc: 0.7760 - val_loss: 0.5034 - val_acc: 0.7760\n",
      "Epoch 231/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4671 - acc: 0.7760 - val_loss: 0.5033 - val_acc: 0.7760\n",
      "Epoch 232/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4670 - acc: 0.7760 - val_loss: 0.5032 - val_acc: 0.7760\n",
      "Epoch 233/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4669 - acc: 0.7760 - val_loss: 0.5032 - val_acc: 0.7760\n",
      "Epoch 234/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4668 - acc: 0.7778 - val_loss: 0.5031 - val_acc: 0.7760\n",
      "Epoch 235/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4667 - acc: 0.7760 - val_loss: 0.5031 - val_acc: 0.7760\n",
      "Epoch 236/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4666 - acc: 0.7778 - val_loss: 0.5030 - val_acc: 0.7760\n",
      "Epoch 237/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4665 - acc: 0.7778 - val_loss: 0.5030 - val_acc: 0.7760\n",
      "Epoch 238/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4664 - acc: 0.7760 - val_loss: 0.5029 - val_acc: 0.7760\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 239/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4663 - acc: 0.7778 - val_loss: 0.5028 - val_acc: 0.7760\n",
      "Epoch 240/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4662 - acc: 0.7778 - val_loss: 0.5028 - val_acc: 0.7760\n",
      "Epoch 241/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4661 - acc: 0.7795 - val_loss: 0.5027 - val_acc: 0.7760\n",
      "Epoch 242/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4660 - acc: 0.7760 - val_loss: 0.5027 - val_acc: 0.7708\n",
      "Epoch 243/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4659 - acc: 0.7778 - val_loss: 0.5026 - val_acc: 0.7708\n",
      "Epoch 244/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4658 - acc: 0.7760 - val_loss: 0.5026 - val_acc: 0.7708\n",
      "Epoch 245/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4657 - acc: 0.7778 - val_loss: 0.5026 - val_acc: 0.7656\n",
      "Epoch 246/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4656 - acc: 0.7778 - val_loss: 0.5025 - val_acc: 0.7656\n",
      "Epoch 247/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4655 - acc: 0.7760 - val_loss: 0.5025 - val_acc: 0.7656\n",
      "Epoch 248/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4654 - acc: 0.7778 - val_loss: 0.5024 - val_acc: 0.7656\n",
      "Epoch 249/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4653 - acc: 0.7778 - val_loss: 0.5024 - val_acc: 0.7656\n",
      "Epoch 250/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4652 - acc: 0.7778 - val_loss: 0.5023 - val_acc: 0.7656\n",
      "Epoch 251/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4651 - acc: 0.7778 - val_loss: 0.5023 - val_acc: 0.7656\n",
      "Epoch 252/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4651 - acc: 0.7795 - val_loss: 0.5023 - val_acc: 0.7656\n",
      "Epoch 253/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4650 - acc: 0.7778 - val_loss: 0.5023 - val_acc: 0.7656\n",
      "Epoch 254/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4649 - acc: 0.7795 - val_loss: 0.5022 - val_acc: 0.7656\n",
      "Epoch 255/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4648 - acc: 0.7795 - val_loss: 0.5022 - val_acc: 0.7708\n",
      "Epoch 256/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4648 - acc: 0.7795 - val_loss: 0.5022 - val_acc: 0.7708\n",
      "Epoch 257/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4646 - acc: 0.7795 - val_loss: 0.5021 - val_acc: 0.7708\n",
      "Epoch 258/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4646 - acc: 0.7795 - val_loss: 0.5021 - val_acc: 0.7708\n",
      "Epoch 259/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4645 - acc: 0.7795 - val_loss: 0.5021 - val_acc: 0.7708\n",
      "Epoch 260/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4644 - acc: 0.7795 - val_loss: 0.5020 - val_acc: 0.7708\n",
      "Epoch 261/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4643 - acc: 0.7795 - val_loss: 0.5020 - val_acc: 0.7708\n",
      "Epoch 262/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4642 - acc: 0.7795 - val_loss: 0.5020 - val_acc: 0.7708\n",
      "Epoch 263/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4642 - acc: 0.7795 - val_loss: 0.5020 - val_acc: 0.7708\n",
      "Epoch 264/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4641 - acc: 0.7795 - val_loss: 0.5020 - val_acc: 0.7708\n",
      "Epoch 265/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4640 - acc: 0.7795 - val_loss: 0.5019 - val_acc: 0.7708\n",
      "Epoch 266/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4640 - acc: 0.7795 - val_loss: 0.5019 - val_acc: 0.7708\n",
      "Epoch 267/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4639 - acc: 0.7795 - val_loss: 0.5019 - val_acc: 0.7708\n",
      "Epoch 268/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4638 - acc: 0.7795 - val_loss: 0.5019 - val_acc: 0.7708\n",
      "Epoch 269/1500\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4637 - acc: 0.7795 - val_loss: 0.5019 - val_acc: 0.7708\n",
      "Epoch 270/1500\n",
      "576/576 [==============================] - 0s 87us/step - loss: 0.4637 - acc: 0.7795 - val_loss: 0.5018 - val_acc: 0.7708\n",
      "Epoch 271/1500\n",
      "576/576 [==============================] - 0s 84us/step - loss: 0.4636 - acc: 0.7795 - val_loss: 0.5018 - val_acc: 0.7708\n",
      "Epoch 272/1500\n",
      "576/576 [==============================] - 0s 91us/step - loss: 0.4635 - acc: 0.7795 - val_loss: 0.5018 - val_acc: 0.7708\n",
      "Epoch 273/1500\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4634 - acc: 0.7795 - val_loss: 0.5018 - val_acc: 0.7708\n",
      "Epoch 274/1500\n",
      "576/576 [==============================] - 0s 93us/step - loss: 0.4634 - acc: 0.7795 - val_loss: 0.5018 - val_acc: 0.7708\n",
      "Epoch 275/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4633 - acc: 0.7795 - val_loss: 0.5018 - val_acc: 0.7708\n",
      "Epoch 276/1500\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4632 - acc: 0.7795 - val_loss: 0.5018 - val_acc: 0.7708\n",
      "Epoch 277/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4631 - acc: 0.7795 - val_loss: 0.5018 - val_acc: 0.7708\n",
      "Epoch 278/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4631 - acc: 0.7778 - val_loss: 0.5017 - val_acc: 0.7708\n",
      "Epoch 279/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4630 - acc: 0.7778 - val_loss: 0.5017 - val_acc: 0.7708\n",
      "Epoch 280/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4629 - acc: 0.7778 - val_loss: 0.5017 - val_acc: 0.7708\n",
      "Epoch 281/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4629 - acc: 0.7778 - val_loss: 0.5017 - val_acc: 0.7708\n",
      "Epoch 282/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4628 - acc: 0.7778 - val_loss: 0.5017 - val_acc: 0.7708\n",
      "Epoch 283/1500\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.4627 - acc: 0.7760 - val_loss: 0.5017 - val_acc: 0.7708\n",
      "Epoch 284/1500\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4627 - acc: 0.7778 - val_loss: 0.5017 - val_acc: 0.7708\n",
      "Epoch 285/1500\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4626 - acc: 0.7760 - val_loss: 0.5017 - val_acc: 0.7708\n",
      "Epoch 286/1500\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4625 - acc: 0.7760 - val_loss: 0.5017 - val_acc: 0.7708\n",
      "Epoch 287/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4625 - acc: 0.7760 - val_loss: 0.5017 - val_acc: 0.7708\n",
      "Epoch 288/1500\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.4624 - acc: 0.7760 - val_loss: 0.5017 - val_acc: 0.7708\n",
      "Epoch 289/1500\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4623 - acc: 0.7760 - val_loss: 0.5017 - val_acc: 0.7708\n",
      "Epoch 290/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4623 - acc: 0.7760 - val_loss: 0.5016 - val_acc: 0.7708\n",
      "Epoch 291/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4622 - acc: 0.7760 - val_loss: 0.5016 - val_acc: 0.7708\n",
      "Epoch 292/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4622 - acc: 0.7760 - val_loss: 0.5016 - val_acc: 0.7708\n",
      "Epoch 293/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4621 - acc: 0.7760 - val_loss: 0.5016 - val_acc: 0.7708\n",
      "Epoch 294/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4620 - acc: 0.7760 - val_loss: 0.5016 - val_acc: 0.7708\n",
      "Epoch 295/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4619 - acc: 0.7760 - val_loss: 0.5016 - val_acc: 0.7708\n",
      "Epoch 296/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4619 - acc: 0.7760 - val_loss: 0.5016 - val_acc: 0.7708\n",
      "Epoch 297/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4618 - acc: 0.7760 - val_loss: 0.5016 - val_acc: 0.7708\n",
      "Epoch 298/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4618 - acc: 0.7760 - val_loss: 0.5016 - val_acc: 0.7708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 299/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4617 - acc: 0.7760 - val_loss: 0.5016 - val_acc: 0.7708\n",
      "Epoch 300/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4617 - acc: 0.7760 - val_loss: 0.5016 - val_acc: 0.7708\n",
      "Epoch 301/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4616 - acc: 0.7760 - val_loss: 0.5016 - val_acc: 0.7708\n",
      "Epoch 302/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4615 - acc: 0.7760 - val_loss: 0.5016 - val_acc: 0.7708\n",
      "Epoch 303/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4615 - acc: 0.7760 - val_loss: 0.5016 - val_acc: 0.7708\n",
      "Epoch 304/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4614 - acc: 0.7760 - val_loss: 0.5016 - val_acc: 0.7708\n",
      "Epoch 305/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4614 - acc: 0.7760 - val_loss: 0.5016 - val_acc: 0.7708\n",
      "Epoch 306/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4613 - acc: 0.7760 - val_loss: 0.5016 - val_acc: 0.7708\n",
      "Epoch 307/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4612 - acc: 0.7760 - val_loss: 0.5016 - val_acc: 0.7708\n",
      "Epoch 308/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4612 - acc: 0.7760 - val_loss: 0.5016 - val_acc: 0.7708\n",
      "Epoch 309/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4611 - acc: 0.7743 - val_loss: 0.5016 - val_acc: 0.7708\n",
      "Epoch 310/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4611 - acc: 0.7743 - val_loss: 0.5016 - val_acc: 0.7708\n",
      "Epoch 311/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4610 - acc: 0.7760 - val_loss: 0.5016 - val_acc: 0.7708\n",
      "Epoch 312/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4610 - acc: 0.7743 - val_loss: 0.5016 - val_acc: 0.7708\n",
      "Epoch 313/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4609 - acc: 0.7743 - val_loss: 0.5016 - val_acc: 0.7708\n",
      "Epoch 314/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4608 - acc: 0.7743 - val_loss: 0.5016 - val_acc: 0.7708\n",
      "Epoch 315/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4608 - acc: 0.7743 - val_loss: 0.5016 - val_acc: 0.7708\n",
      "Epoch 316/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4608 - acc: 0.7743 - val_loss: 0.5016 - val_acc: 0.7708\n",
      "Epoch 317/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4607 - acc: 0.7726 - val_loss: 0.5016 - val_acc: 0.7708\n",
      "Epoch 318/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4606 - acc: 0.7743 - val_loss: 0.5016 - val_acc: 0.7708\n",
      "Epoch 319/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4606 - acc: 0.7726 - val_loss: 0.5016 - val_acc: 0.7708\n",
      "Epoch 320/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4605 - acc: 0.7743 - val_loss: 0.5016 - val_acc: 0.7708\n",
      "Epoch 321/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4605 - acc: 0.7726 - val_loss: 0.5016 - val_acc: 0.7708\n",
      "Epoch 322/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4604 - acc: 0.7726 - val_loss: 0.5016 - val_acc: 0.7708\n",
      "Epoch 323/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4604 - acc: 0.7726 - val_loss: 0.5016 - val_acc: 0.7708\n",
      "Epoch 324/1500\n",
      "576/576 [==============================] - 0s 86us/step - loss: 0.4603 - acc: 0.7726 - val_loss: 0.5016 - val_acc: 0.7708\n",
      "Epoch 325/1500\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4603 - acc: 0.7726 - val_loss: 0.5016 - val_acc: 0.7708\n",
      "Epoch 326/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4602 - acc: 0.7726 - val_loss: 0.5016 - val_acc: 0.7708\n",
      "Epoch 327/1500\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4602 - acc: 0.7726 - val_loss: 0.5016 - val_acc: 0.7708\n",
      "Epoch 328/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4601 - acc: 0.7726 - val_loss: 0.5016 - val_acc: 0.7708\n",
      "Epoch 329/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4600 - acc: 0.7726 - val_loss: 0.5016 - val_acc: 0.7708\n",
      "Epoch 330/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4600 - acc: 0.7726 - val_loss: 0.5016 - val_acc: 0.7708\n",
      "Epoch 331/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4600 - acc: 0.7726 - val_loss: 0.5016 - val_acc: 0.7708\n",
      "Epoch 332/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4599 - acc: 0.7726 - val_loss: 0.5016 - val_acc: 0.7708\n",
      "Epoch 333/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4598 - acc: 0.7726 - val_loss: 0.5016 - val_acc: 0.7708\n",
      "Epoch 334/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4598 - acc: 0.7726 - val_loss: 0.5016 - val_acc: 0.7708\n",
      "Epoch 335/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4597 - acc: 0.7726 - val_loss: 0.5016 - val_acc: 0.7708\n",
      "Epoch 336/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4597 - acc: 0.7726 - val_loss: 0.5016 - val_acc: 0.7708\n",
      "Epoch 337/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4596 - acc: 0.7726 - val_loss: 0.5017 - val_acc: 0.7708\n",
      "Epoch 338/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4596 - acc: 0.7743 - val_loss: 0.5017 - val_acc: 0.7708\n",
      "Epoch 339/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4595 - acc: 0.7726 - val_loss: 0.5017 - val_acc: 0.7708\n",
      "Epoch 340/1500\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4595 - acc: 0.7743 - val_loss: 0.5017 - val_acc: 0.7708\n",
      "Epoch 341/1500\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4594 - acc: 0.7726 - val_loss: 0.5017 - val_acc: 0.7708\n",
      "Epoch 342/1500\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4594 - acc: 0.7708 - val_loss: 0.5017 - val_acc: 0.7708\n",
      "Epoch 343/1500\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.4594 - acc: 0.7743 - val_loss: 0.5017 - val_acc: 0.7708\n",
      "Epoch 344/1500\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4593 - acc: 0.7726 - val_loss: 0.5017 - val_acc: 0.7708\n",
      "Epoch 345/1500\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.4593 - acc: 0.7726 - val_loss: 0.5017 - val_acc: 0.7708\n",
      "Epoch 346/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4592 - acc: 0.7726 - val_loss: 0.5017 - val_acc: 0.7708\n",
      "Epoch 347/1500\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4592 - acc: 0.7726 - val_loss: 0.5017 - val_acc: 0.7708\n",
      "Epoch 348/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4591 - acc: 0.7726 - val_loss: 0.5017 - val_acc: 0.7708\n",
      "Epoch 349/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4590 - acc: 0.7726 - val_loss: 0.5018 - val_acc: 0.7708\n",
      "Epoch 350/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4590 - acc: 0.7708 - val_loss: 0.5018 - val_acc: 0.7708\n",
      "Epoch 351/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4590 - acc: 0.7708 - val_loss: 0.5018 - val_acc: 0.7708\n",
      "Epoch 352/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4589 - acc: 0.7708 - val_loss: 0.5018 - val_acc: 0.7708\n",
      "Epoch 353/1500\n",
      "576/576 [==============================] - 0s 88us/step - loss: 0.4589 - acc: 0.7708 - val_loss: 0.5018 - val_acc: 0.7708\n",
      "Epoch 354/1500\n",
      "576/576 [==============================] - 0s 79us/step - loss: 0.4588 - acc: 0.7708 - val_loss: 0.5018 - val_acc: 0.7708\n",
      "Epoch 355/1500\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.4588 - acc: 0.7708 - val_loss: 0.5018 - val_acc: 0.7708\n",
      "Epoch 356/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4587 - acc: 0.7708 - val_loss: 0.5018 - val_acc: 0.7708\n",
      "Epoch 357/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4587 - acc: 0.7708 - val_loss: 0.5019 - val_acc: 0.7708\n",
      "Epoch 358/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4586 - acc: 0.7708 - val_loss: 0.5019 - val_acc: 0.7708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 359/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4586 - acc: 0.7708 - val_loss: 0.5019 - val_acc: 0.7708\n",
      "Epoch 360/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4585 - acc: 0.7708 - val_loss: 0.5019 - val_acc: 0.7708\n",
      "Epoch 361/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4585 - acc: 0.7708 - val_loss: 0.5019 - val_acc: 0.7708\n",
      "Epoch 362/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4584 - acc: 0.7708 - val_loss: 0.5019 - val_acc: 0.7708\n",
      "Epoch 363/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4584 - acc: 0.7691 - val_loss: 0.5020 - val_acc: 0.7708\n",
      "Epoch 364/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4583 - acc: 0.7708 - val_loss: 0.5020 - val_acc: 0.7708\n",
      "Epoch 365/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4583 - acc: 0.7708 - val_loss: 0.5020 - val_acc: 0.7708\n",
      "Epoch 366/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4582 - acc: 0.7708 - val_loss: 0.5020 - val_acc: 0.7708\n",
      "Epoch 367/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4582 - acc: 0.7726 - val_loss: 0.5020 - val_acc: 0.7708\n",
      "Epoch 368/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4582 - acc: 0.7726 - val_loss: 0.5020 - val_acc: 0.7708\n",
      "Epoch 369/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4581 - acc: 0.7708 - val_loss: 0.5021 - val_acc: 0.7708\n",
      "Epoch 370/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4581 - acc: 0.7708 - val_loss: 0.5021 - val_acc: 0.7708\n",
      "Epoch 371/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4580 - acc: 0.7726 - val_loss: 0.5021 - val_acc: 0.7708\n",
      "Epoch 372/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4580 - acc: 0.7708 - val_loss: 0.5021 - val_acc: 0.7708\n",
      "Epoch 373/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4580 - acc: 0.7708 - val_loss: 0.5021 - val_acc: 0.7708\n",
      "Epoch 374/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4579 - acc: 0.7726 - val_loss: 0.5022 - val_acc: 0.7708\n",
      "Epoch 375/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4578 - acc: 0.7691 - val_loss: 0.5022 - val_acc: 0.7708\n",
      "Epoch 376/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4578 - acc: 0.7708 - val_loss: 0.5022 - val_acc: 0.7708\n",
      "Epoch 377/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4578 - acc: 0.7708 - val_loss: 0.5022 - val_acc: 0.7708\n",
      "Epoch 378/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4577 - acc: 0.7674 - val_loss: 0.5022 - val_acc: 0.7708\n",
      "Epoch 379/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4577 - acc: 0.7708 - val_loss: 0.5023 - val_acc: 0.7708\n",
      "Epoch 380/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4577 - acc: 0.7708 - val_loss: 0.5023 - val_acc: 0.7708\n",
      "Epoch 381/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4576 - acc: 0.7674 - val_loss: 0.5023 - val_acc: 0.7708\n",
      "Epoch 382/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4575 - acc: 0.7691 - val_loss: 0.5023 - val_acc: 0.7708\n",
      "Epoch 383/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4575 - acc: 0.7674 - val_loss: 0.5023 - val_acc: 0.7708\n",
      "Epoch 384/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4575 - acc: 0.7674 - val_loss: 0.5023 - val_acc: 0.7656\n",
      "Epoch 385/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4574 - acc: 0.7691 - val_loss: 0.5024 - val_acc: 0.7656\n",
      "Epoch 386/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4574 - acc: 0.7674 - val_loss: 0.5024 - val_acc: 0.7656\n",
      "Epoch 387/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4573 - acc: 0.7691 - val_loss: 0.5024 - val_acc: 0.7656\n",
      "Epoch 388/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4573 - acc: 0.7674 - val_loss: 0.5024 - val_acc: 0.7656\n",
      "Epoch 389/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4572 - acc: 0.7674 - val_loss: 0.5024 - val_acc: 0.7656\n",
      "Epoch 390/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4572 - acc: 0.7674 - val_loss: 0.5025 - val_acc: 0.7656\n",
      "Epoch 391/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4571 - acc: 0.7674 - val_loss: 0.5025 - val_acc: 0.7656\n",
      "Epoch 392/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4571 - acc: 0.7656 - val_loss: 0.5025 - val_acc: 0.7656\n",
      "Epoch 393/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4570 - acc: 0.7656 - val_loss: 0.5025 - val_acc: 0.7656\n",
      "Epoch 394/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4570 - acc: 0.7656 - val_loss: 0.5025 - val_acc: 0.7656\n",
      "Epoch 395/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4569 - acc: 0.7674 - val_loss: 0.5026 - val_acc: 0.7656\n",
      "Epoch 396/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4569 - acc: 0.7674 - val_loss: 0.5026 - val_acc: 0.7656\n",
      "Epoch 397/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4569 - acc: 0.7674 - val_loss: 0.5026 - val_acc: 0.7656\n",
      "Epoch 398/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4568 - acc: 0.7656 - val_loss: 0.5026 - val_acc: 0.7656\n",
      "Epoch 399/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4568 - acc: 0.7674 - val_loss: 0.5026 - val_acc: 0.7656\n",
      "Epoch 400/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4568 - acc: 0.7656 - val_loss: 0.5027 - val_acc: 0.7656\n",
      "Epoch 401/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4567 - acc: 0.7656 - val_loss: 0.5027 - val_acc: 0.7656\n",
      "Epoch 402/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4567 - acc: 0.7639 - val_loss: 0.5027 - val_acc: 0.7656\n",
      "Epoch 403/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4566 - acc: 0.7639 - val_loss: 0.5027 - val_acc: 0.7656\n",
      "Epoch 404/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4566 - acc: 0.7639 - val_loss: 0.5027 - val_acc: 0.7656\n",
      "Epoch 405/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4565 - acc: 0.7639 - val_loss: 0.5028 - val_acc: 0.7656\n",
      "Epoch 406/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4565 - acc: 0.7656 - val_loss: 0.5028 - val_acc: 0.7656\n",
      "Epoch 407/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4564 - acc: 0.7639 - val_loss: 0.5028 - val_acc: 0.7656\n",
      "Epoch 408/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4564 - acc: 0.7656 - val_loss: 0.5028 - val_acc: 0.7656\n",
      "Epoch 409/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4563 - acc: 0.7639 - val_loss: 0.5028 - val_acc: 0.7656\n",
      "Epoch 410/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4563 - acc: 0.7639 - val_loss: 0.5029 - val_acc: 0.7656\n",
      "Epoch 411/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4562 - acc: 0.7639 - val_loss: 0.5029 - val_acc: 0.7656\n",
      "Epoch 412/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4562 - acc: 0.7639 - val_loss: 0.5029 - val_acc: 0.7656\n",
      "Epoch 413/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4562 - acc: 0.7639 - val_loss: 0.5029 - val_acc: 0.7656\n",
      "Epoch 414/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4561 - acc: 0.7639 - val_loss: 0.5029 - val_acc: 0.7656\n",
      "Epoch 415/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4561 - acc: 0.7639 - val_loss: 0.5029 - val_acc: 0.7656\n",
      "Epoch 416/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4561 - acc: 0.7639 - val_loss: 0.5030 - val_acc: 0.7656\n",
      "Epoch 417/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4560 - acc: 0.7639 - val_loss: 0.5030 - val_acc: 0.7656\n",
      "Epoch 418/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4560 - acc: 0.7639 - val_loss: 0.5030 - val_acc: 0.7656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 419/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4559 - acc: 0.7639 - val_loss: 0.5030 - val_acc: 0.7656\n",
      "Epoch 420/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4559 - acc: 0.7639 - val_loss: 0.5030 - val_acc: 0.7656\n",
      "Epoch 421/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4559 - acc: 0.7639 - val_loss: 0.5030 - val_acc: 0.7604\n",
      "Epoch 422/1500\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4558 - acc: 0.7656 - val_loss: 0.5030 - val_acc: 0.7604\n",
      "Epoch 423/1500\n",
      "576/576 [==============================] - 0s 79us/step - loss: 0.4558 - acc: 0.7639 - val_loss: 0.5031 - val_acc: 0.7604\n",
      "Epoch 424/1500\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4558 - acc: 0.7639 - val_loss: 0.5031 - val_acc: 0.7604\n",
      "Epoch 425/1500\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4557 - acc: 0.7656 - val_loss: 0.5031 - val_acc: 0.7604\n",
      "Epoch 426/1500\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.4557 - acc: 0.7639 - val_loss: 0.5031 - val_acc: 0.7604\n",
      "Epoch 427/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4556 - acc: 0.7639 - val_loss: 0.5031 - val_acc: 0.7604\n",
      "Epoch 428/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4556 - acc: 0.7639 - val_loss: 0.5031 - val_acc: 0.7604\n",
      "Epoch 429/1500\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4556 - acc: 0.7656 - val_loss: 0.5031 - val_acc: 0.7604\n",
      "Epoch 430/1500\n",
      "576/576 [==============================] - 0s 79us/step - loss: 0.4555 - acc: 0.7639 - val_loss: 0.5032 - val_acc: 0.7604\n",
      "Epoch 431/1500\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4555 - acc: 0.7656 - val_loss: 0.5032 - val_acc: 0.7604\n",
      "Epoch 432/1500\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4555 - acc: 0.7639 - val_loss: 0.5032 - val_acc: 0.7604\n",
      "Epoch 433/1500\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4554 - acc: 0.7639 - val_loss: 0.5032 - val_acc: 0.7604\n",
      "Epoch 434/1500\n",
      "576/576 [==============================] - 0s 79us/step - loss: 0.4554 - acc: 0.7656 - val_loss: 0.5032 - val_acc: 0.7604\n",
      "Epoch 435/1500\n",
      "576/576 [==============================] - 0s 89us/step - loss: 0.4554 - acc: 0.7639 - val_loss: 0.5032 - val_acc: 0.7604\n",
      "Epoch 436/1500\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.4553 - acc: 0.7656 - val_loss: 0.5032 - val_acc: 0.7604\n",
      "Epoch 437/1500\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.4553 - acc: 0.7639 - val_loss: 0.5032 - val_acc: 0.7604\n",
      "Epoch 438/1500\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4552 - acc: 0.7639 - val_loss: 0.5032 - val_acc: 0.7604\n",
      "Epoch 439/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4552 - acc: 0.7639 - val_loss: 0.5033 - val_acc: 0.7604\n",
      "Epoch 440/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4552 - acc: 0.7639 - val_loss: 0.5033 - val_acc: 0.7604\n",
      "Epoch 441/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4551 - acc: 0.7639 - val_loss: 0.5033 - val_acc: 0.7604\n",
      "Epoch 442/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4551 - acc: 0.7639 - val_loss: 0.5033 - val_acc: 0.7604\n",
      "Epoch 443/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4551 - acc: 0.7639 - val_loss: 0.5033 - val_acc: 0.7604\n",
      "Epoch 444/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4550 - acc: 0.7639 - val_loss: 0.5033 - val_acc: 0.7604\n",
      "Epoch 445/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4550 - acc: 0.7639 - val_loss: 0.5033 - val_acc: 0.7604\n",
      "Epoch 446/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4550 - acc: 0.7639 - val_loss: 0.5033 - val_acc: 0.7604\n",
      "Epoch 447/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4549 - acc: 0.7639 - val_loss: 0.5033 - val_acc: 0.7552\n",
      "Epoch 448/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4549 - acc: 0.7639 - val_loss: 0.5034 - val_acc: 0.7552\n",
      "Epoch 449/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4549 - acc: 0.7639 - val_loss: 0.5034 - val_acc: 0.7552\n",
      "Epoch 450/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4548 - acc: 0.7639 - val_loss: 0.5034 - val_acc: 0.7552\n",
      "Epoch 451/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4548 - acc: 0.7639 - val_loss: 0.5034 - val_acc: 0.7552\n",
      "Epoch 452/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4547 - acc: 0.7639 - val_loss: 0.5034 - val_acc: 0.7552\n",
      "Epoch 453/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4547 - acc: 0.7639 - val_loss: 0.5034 - val_acc: 0.7552\n",
      "Epoch 454/1500\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.4547 - acc: 0.7622 - val_loss: 0.5034 - val_acc: 0.7552\n",
      "Epoch 455/1500\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.4547 - acc: 0.7639 - val_loss: 0.5035 - val_acc: 0.7552\n",
      "Epoch 456/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4546 - acc: 0.7656 - val_loss: 0.5035 - val_acc: 0.7552\n",
      "Epoch 457/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4546 - acc: 0.7639 - val_loss: 0.5035 - val_acc: 0.7552\n",
      "Epoch 458/1500\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4545 - acc: 0.7639 - val_loss: 0.5035 - val_acc: 0.7552\n",
      "Epoch 459/1500\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.4545 - acc: 0.7639 - val_loss: 0.5035 - val_acc: 0.7500\n",
      "Epoch 460/1500\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4545 - acc: 0.7639 - val_loss: 0.5036 - val_acc: 0.7500\n",
      "Epoch 461/1500\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4545 - acc: 0.7622 - val_loss: 0.5036 - val_acc: 0.7500\n",
      "Epoch 462/1500\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.4545 - acc: 0.7622 - val_loss: 0.5036 - val_acc: 0.7500\n",
      "Epoch 463/1500\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4544 - acc: 0.7622 - val_loss: 0.5036 - val_acc: 0.7500\n",
      "Epoch 464/1500\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4544 - acc: 0.7639 - val_loss: 0.5036 - val_acc: 0.7500\n",
      "Epoch 465/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4543 - acc: 0.7622 - val_loss: 0.5036 - val_acc: 0.7500\n",
      "Epoch 466/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4543 - acc: 0.7639 - val_loss: 0.5037 - val_acc: 0.7500\n",
      "Epoch 467/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4543 - acc: 0.7622 - val_loss: 0.5037 - val_acc: 0.7500\n",
      "Epoch 468/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4542 - acc: 0.7622 - val_loss: 0.5037 - val_acc: 0.7500\n",
      "Epoch 469/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4542 - acc: 0.7622 - val_loss: 0.5037 - val_acc: 0.7500\n",
      "Epoch 470/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4542 - acc: 0.7622 - val_loss: 0.5037 - val_acc: 0.7500\n",
      "Epoch 471/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4542 - acc: 0.7639 - val_loss: 0.5038 - val_acc: 0.7500\n",
      "Epoch 472/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4541 - acc: 0.7622 - val_loss: 0.5038 - val_acc: 0.7500\n",
      "Epoch 473/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4541 - acc: 0.7639 - val_loss: 0.5038 - val_acc: 0.7500\n",
      "Epoch 474/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4541 - acc: 0.7639 - val_loss: 0.5038 - val_acc: 0.7500\n",
      "Epoch 475/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4540 - acc: 0.7622 - val_loss: 0.5038 - val_acc: 0.7500\n",
      "Epoch 476/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4540 - acc: 0.7639 - val_loss: 0.5038 - val_acc: 0.7500\n",
      "Epoch 477/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4540 - acc: 0.7639 - val_loss: 0.5039 - val_acc: 0.7500\n",
      "Epoch 478/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4540 - acc: 0.7639 - val_loss: 0.5039 - val_acc: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 479/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4539 - acc: 0.7639 - val_loss: 0.5039 - val_acc: 0.7500\n",
      "Epoch 480/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4539 - acc: 0.7639 - val_loss: 0.5039 - val_acc: 0.7500\n",
      "Epoch 481/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4539 - acc: 0.7639 - val_loss: 0.5039 - val_acc: 0.7500\n",
      "Epoch 482/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4538 - acc: 0.7639 - val_loss: 0.5040 - val_acc: 0.7500\n",
      "Epoch 483/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4538 - acc: 0.7639 - val_loss: 0.5040 - val_acc: 0.7500\n",
      "Epoch 484/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4538 - acc: 0.7639 - val_loss: 0.5040 - val_acc: 0.7500\n",
      "Epoch 485/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4538 - acc: 0.7639 - val_loss: 0.5040 - val_acc: 0.7500\n",
      "Epoch 486/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4537 - acc: 0.7639 - val_loss: 0.5040 - val_acc: 0.7500\n",
      "Epoch 487/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4537 - acc: 0.7656 - val_loss: 0.5041 - val_acc: 0.7500\n",
      "Epoch 488/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4537 - acc: 0.7639 - val_loss: 0.5041 - val_acc: 0.7500\n",
      "Epoch 489/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4536 - acc: 0.7639 - val_loss: 0.5041 - val_acc: 0.7500\n",
      "Epoch 490/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4536 - acc: 0.7639 - val_loss: 0.5041 - val_acc: 0.7500\n",
      "Epoch 491/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4536 - acc: 0.7639 - val_loss: 0.5041 - val_acc: 0.7500\n",
      "Epoch 492/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4536 - acc: 0.7656 - val_loss: 0.5041 - val_acc: 0.7500\n",
      "Epoch 493/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4535 - acc: 0.7656 - val_loss: 0.5042 - val_acc: 0.7500\n",
      "Epoch 494/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4535 - acc: 0.7656 - val_loss: 0.5042 - val_acc: 0.7500\n",
      "Epoch 495/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4535 - acc: 0.7656 - val_loss: 0.5042 - val_acc: 0.7500\n",
      "Epoch 496/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4534 - acc: 0.7656 - val_loss: 0.5042 - val_acc: 0.7500\n",
      "Epoch 497/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4534 - acc: 0.7656 - val_loss: 0.5042 - val_acc: 0.7500\n",
      "Epoch 498/1500\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.4694 - acc: 0.781 - 0s 51us/step - loss: 0.4534 - acc: 0.7656 - val_loss: 0.5043 - val_acc: 0.7500\n",
      "Epoch 499/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4533 - acc: 0.7674 - val_loss: 0.5043 - val_acc: 0.7500\n",
      "Epoch 500/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4533 - acc: 0.7656 - val_loss: 0.5043 - val_acc: 0.7500\n",
      "Epoch 501/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4533 - acc: 0.7656 - val_loss: 0.5043 - val_acc: 0.7500\n",
      "Epoch 502/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4533 - acc: 0.7656 - val_loss: 0.5043 - val_acc: 0.7500\n",
      "Epoch 503/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4532 - acc: 0.7656 - val_loss: 0.5044 - val_acc: 0.7500\n",
      "Epoch 504/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4532 - acc: 0.7674 - val_loss: 0.5044 - val_acc: 0.7500\n",
      "Epoch 505/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4532 - acc: 0.7656 - val_loss: 0.5044 - val_acc: 0.7500\n",
      "Epoch 506/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4532 - acc: 0.7656 - val_loss: 0.5044 - val_acc: 0.7500\n",
      "Epoch 507/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4531 - acc: 0.7674 - val_loss: 0.5044 - val_acc: 0.7500\n",
      "Epoch 508/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4531 - acc: 0.7656 - val_loss: 0.5045 - val_acc: 0.7500\n",
      "Epoch 509/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4531 - acc: 0.7656 - val_loss: 0.5045 - val_acc: 0.7500\n",
      "Epoch 510/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4531 - acc: 0.7656 - val_loss: 0.5045 - val_acc: 0.7500\n",
      "Epoch 511/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4531 - acc: 0.7656 - val_loss: 0.5045 - val_acc: 0.7500\n",
      "Epoch 512/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4530 - acc: 0.7656 - val_loss: 0.5045 - val_acc: 0.7500\n",
      "Epoch 513/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4530 - acc: 0.7656 - val_loss: 0.5045 - val_acc: 0.7500\n",
      "Epoch 514/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4530 - acc: 0.7656 - val_loss: 0.5046 - val_acc: 0.7500\n",
      "Epoch 515/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4529 - acc: 0.7656 - val_loss: 0.5046 - val_acc: 0.7500\n",
      "Epoch 516/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4529 - acc: 0.7656 - val_loss: 0.5046 - val_acc: 0.7500\n",
      "Epoch 517/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4529 - acc: 0.7656 - val_loss: 0.5046 - val_acc: 0.7500\n",
      "Epoch 518/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4529 - acc: 0.7656 - val_loss: 0.5046 - val_acc: 0.7500\n",
      "Epoch 519/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4529 - acc: 0.7656 - val_loss: 0.5047 - val_acc: 0.7552\n",
      "Epoch 520/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4528 - acc: 0.7656 - val_loss: 0.5047 - val_acc: 0.7552\n",
      "Epoch 521/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4528 - acc: 0.7656 - val_loss: 0.5047 - val_acc: 0.7552\n",
      "Epoch 522/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4528 - acc: 0.7656 - val_loss: 0.5047 - val_acc: 0.7552\n",
      "Epoch 523/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4527 - acc: 0.7656 - val_loss: 0.5047 - val_acc: 0.7552\n",
      "Epoch 524/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4528 - acc: 0.7674 - val_loss: 0.5048 - val_acc: 0.7552\n",
      "Epoch 525/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4527 - acc: 0.7656 - val_loss: 0.5048 - val_acc: 0.7552\n",
      "Epoch 526/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4527 - acc: 0.7656 - val_loss: 0.5048 - val_acc: 0.7552\n",
      "Epoch 527/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4527 - acc: 0.7656 - val_loss: 0.5048 - val_acc: 0.7552\n",
      "Epoch 528/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4526 - acc: 0.7674 - val_loss: 0.5048 - val_acc: 0.7552\n",
      "Epoch 529/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4526 - acc: 0.7674 - val_loss: 0.5048 - val_acc: 0.7552\n",
      "Epoch 530/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4526 - acc: 0.7656 - val_loss: 0.5049 - val_acc: 0.7552\n",
      "Epoch 531/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4526 - acc: 0.7656 - val_loss: 0.5049 - val_acc: 0.7552\n",
      "Epoch 532/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4526 - acc: 0.7656 - val_loss: 0.5049 - val_acc: 0.7552\n",
      "Epoch 533/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4525 - acc: 0.7656 - val_loss: 0.5049 - val_acc: 0.7552\n",
      "Epoch 534/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4525 - acc: 0.7656 - val_loss: 0.5049 - val_acc: 0.7552\n",
      "Epoch 535/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4525 - acc: 0.7674 - val_loss: 0.5050 - val_acc: 0.7552\n",
      "Epoch 536/1500\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.3444 - acc: 0.875 - 0s 52us/step - loss: 0.4525 - acc: 0.7656 - val_loss: 0.5050 - val_acc: 0.7552\n",
      "Epoch 537/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4525 - acc: 0.7656 - val_loss: 0.5050 - val_acc: 0.7552\n",
      "Epoch 538/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 59us/step - loss: 0.4524 - acc: 0.7656 - val_loss: 0.5050 - val_acc: 0.7552\n",
      "Epoch 539/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4524 - acc: 0.7656 - val_loss: 0.5050 - val_acc: 0.7552\n",
      "Epoch 540/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4524 - acc: 0.7656 - val_loss: 0.5050 - val_acc: 0.7552\n",
      "Epoch 541/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4524 - acc: 0.7656 - val_loss: 0.5051 - val_acc: 0.7552\n",
      "Epoch 542/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4524 - acc: 0.7656 - val_loss: 0.5051 - val_acc: 0.7552\n",
      "Epoch 543/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4523 - acc: 0.7656 - val_loss: 0.5051 - val_acc: 0.7552\n",
      "Epoch 544/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4523 - acc: 0.7656 - val_loss: 0.5051 - val_acc: 0.7552\n",
      "Epoch 545/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4523 - acc: 0.7656 - val_loss: 0.5051 - val_acc: 0.7552\n",
      "Epoch 546/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4523 - acc: 0.7656 - val_loss: 0.5052 - val_acc: 0.7552\n",
      "Epoch 547/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4523 - acc: 0.7656 - val_loss: 0.5052 - val_acc: 0.7552\n",
      "Epoch 548/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4523 - acc: 0.7656 - val_loss: 0.5052 - val_acc: 0.7552\n",
      "Epoch 549/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4522 - acc: 0.7656 - val_loss: 0.5052 - val_acc: 0.7552\n",
      "Epoch 550/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4522 - acc: 0.7656 - val_loss: 0.5052 - val_acc: 0.7552\n",
      "Epoch 551/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4522 - acc: 0.7656 - val_loss: 0.5053 - val_acc: 0.7552\n",
      "Epoch 552/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4522 - acc: 0.7656 - val_loss: 0.5053 - val_acc: 0.7552\n",
      "Epoch 553/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4522 - acc: 0.7656 - val_loss: 0.5053 - val_acc: 0.7552\n",
      "Epoch 554/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4522 - acc: 0.7656 - val_loss: 0.5053 - val_acc: 0.7552\n",
      "Epoch 555/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4521 - acc: 0.7656 - val_loss: 0.5053 - val_acc: 0.7552\n",
      "Epoch 556/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4521 - acc: 0.7656 - val_loss: 0.5054 - val_acc: 0.7552\n",
      "Epoch 557/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4521 - acc: 0.7656 - val_loss: 0.5054 - val_acc: 0.7552\n",
      "Epoch 558/1500\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4521 - acc: 0.7656 - val_loss: 0.5054 - val_acc: 0.7552\n",
      "Epoch 559/1500\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4521 - acc: 0.7656 - val_loss: 0.5054 - val_acc: 0.7552\n",
      "Epoch 560/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4520 - acc: 0.7656 - val_loss: 0.5054 - val_acc: 0.7552\n",
      "Epoch 561/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4520 - acc: 0.7656 - val_loss: 0.5055 - val_acc: 0.7552\n",
      "Epoch 562/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4520 - acc: 0.7656 - val_loss: 0.5055 - val_acc: 0.7552\n",
      "Epoch 563/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4520 - acc: 0.7656 - val_loss: 0.5055 - val_acc: 0.7552\n",
      "Epoch 564/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4520 - acc: 0.7656 - val_loss: 0.5055 - val_acc: 0.7552\n",
      "Epoch 565/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4520 - acc: 0.7656 - val_loss: 0.5055 - val_acc: 0.7552\n",
      "Epoch 566/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4519 - acc: 0.7656 - val_loss: 0.5056 - val_acc: 0.7552\n",
      "Epoch 567/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4519 - acc: 0.7656 - val_loss: 0.5056 - val_acc: 0.7552\n",
      "Epoch 568/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4519 - acc: 0.7656 - val_loss: 0.5056 - val_acc: 0.7552\n",
      "Epoch 569/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4519 - acc: 0.7656 - val_loss: 0.5056 - val_acc: 0.7552\n",
      "Epoch 570/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4519 - acc: 0.7656 - val_loss: 0.5056 - val_acc: 0.7552\n",
      "Epoch 571/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4519 - acc: 0.7656 - val_loss: 0.5056 - val_acc: 0.7552\n",
      "Epoch 572/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4518 - acc: 0.7656 - val_loss: 0.5057 - val_acc: 0.7552\n",
      "Epoch 573/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4518 - acc: 0.7656 - val_loss: 0.5057 - val_acc: 0.7552\n",
      "Epoch 574/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4518 - acc: 0.7656 - val_loss: 0.5057 - val_acc: 0.7552\n",
      "Epoch 575/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4518 - acc: 0.7656 - val_loss: 0.5057 - val_acc: 0.7552\n",
      "Epoch 576/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4518 - acc: 0.7656 - val_loss: 0.5058 - val_acc: 0.7552\n",
      "Epoch 577/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4518 - acc: 0.7656 - val_loss: 0.5058 - val_acc: 0.7552\n",
      "Epoch 578/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4517 - acc: 0.7656 - val_loss: 0.5058 - val_acc: 0.7552\n",
      "Epoch 579/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4517 - acc: 0.7656 - val_loss: 0.5058 - val_acc: 0.7552\n",
      "Epoch 580/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4517 - acc: 0.7639 - val_loss: 0.5059 - val_acc: 0.7552\n",
      "Epoch 581/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4517 - acc: 0.7656 - val_loss: 0.5059 - val_acc: 0.7552\n",
      "Epoch 582/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4517 - acc: 0.7656 - val_loss: 0.5059 - val_acc: 0.7552\n",
      "Epoch 583/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4517 - acc: 0.7656 - val_loss: 0.5060 - val_acc: 0.7552\n",
      "Epoch 584/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4516 - acc: 0.7639 - val_loss: 0.5060 - val_acc: 0.7552\n",
      "Epoch 585/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4516 - acc: 0.7639 - val_loss: 0.5060 - val_acc: 0.7552\n",
      "Epoch 586/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4516 - acc: 0.7639 - val_loss: 0.5060 - val_acc: 0.7552\n",
      "Epoch 587/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4516 - acc: 0.7639 - val_loss: 0.5061 - val_acc: 0.7552\n",
      "Epoch 588/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4516 - acc: 0.7639 - val_loss: 0.5061 - val_acc: 0.7552\n",
      "Epoch 589/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4515 - acc: 0.7639 - val_loss: 0.5061 - val_acc: 0.7552\n",
      "Epoch 590/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4515 - acc: 0.7639 - val_loss: 0.5061 - val_acc: 0.7552\n",
      "Epoch 591/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4515 - acc: 0.7639 - val_loss: 0.5062 - val_acc: 0.7552\n",
      "Epoch 592/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4515 - acc: 0.7639 - val_loss: 0.5062 - val_acc: 0.7552\n",
      "Epoch 593/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4515 - acc: 0.7639 - val_loss: 0.5062 - val_acc: 0.7552\n",
      "Epoch 594/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4514 - acc: 0.7656 - val_loss: 0.5062 - val_acc: 0.7552\n",
      "Epoch 595/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4515 - acc: 0.7656 - val_loss: 0.5063 - val_acc: 0.7552\n",
      "Epoch 596/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4514 - acc: 0.7656 - val_loss: 0.5063 - val_acc: 0.7552\n",
      "Epoch 597/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4514 - acc: 0.7656 - val_loss: 0.5063 - val_acc: 0.7552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 598/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4514 - acc: 0.7656 - val_loss: 0.5064 - val_acc: 0.7552\n",
      "Epoch 599/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4514 - acc: 0.7656 - val_loss: 0.5064 - val_acc: 0.7552\n",
      "Epoch 600/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4514 - acc: 0.7656 - val_loss: 0.5064 - val_acc: 0.7552\n",
      "Epoch 601/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4513 - acc: 0.7656 - val_loss: 0.5064 - val_acc: 0.7552\n",
      "Epoch 602/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4513 - acc: 0.7656 - val_loss: 0.5065 - val_acc: 0.7552\n",
      "Epoch 603/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4513 - acc: 0.7656 - val_loss: 0.5065 - val_acc: 0.7552\n",
      "Epoch 604/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4513 - acc: 0.7656 - val_loss: 0.5065 - val_acc: 0.7552\n",
      "Epoch 605/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4513 - acc: 0.7656 - val_loss: 0.5065 - val_acc: 0.7552\n",
      "Epoch 606/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4513 - acc: 0.7656 - val_loss: 0.5066 - val_acc: 0.7552\n",
      "Epoch 607/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4513 - acc: 0.7656 - val_loss: 0.5066 - val_acc: 0.7552\n",
      "Epoch 608/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4512 - acc: 0.7656 - val_loss: 0.5066 - val_acc: 0.7552\n",
      "Epoch 609/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4512 - acc: 0.7656 - val_loss: 0.5066 - val_acc: 0.7552\n",
      "Epoch 610/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4512 - acc: 0.7656 - val_loss: 0.5067 - val_acc: 0.7552\n",
      "Epoch 611/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4512 - acc: 0.7656 - val_loss: 0.5067 - val_acc: 0.7552\n",
      "Epoch 612/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4511 - acc: 0.7656 - val_loss: 0.5067 - val_acc: 0.7552\n",
      "Epoch 613/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4511 - acc: 0.7656 - val_loss: 0.5067 - val_acc: 0.7552\n",
      "Epoch 614/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4511 - acc: 0.7656 - val_loss: 0.5068 - val_acc: 0.7552\n",
      "Epoch 615/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4511 - acc: 0.7656 - val_loss: 0.5068 - val_acc: 0.7552\n",
      "Epoch 616/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4511 - acc: 0.7656 - val_loss: 0.5068 - val_acc: 0.7552\n",
      "Epoch 617/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4510 - acc: 0.7656 - val_loss: 0.5069 - val_acc: 0.7552\n",
      "Epoch 618/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4510 - acc: 0.7656 - val_loss: 0.5069 - val_acc: 0.7604\n",
      "Epoch 619/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4510 - acc: 0.7656 - val_loss: 0.5069 - val_acc: 0.7604\n",
      "Epoch 620/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4510 - acc: 0.7656 - val_loss: 0.5069 - val_acc: 0.7604\n",
      "Epoch 621/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4510 - acc: 0.7656 - val_loss: 0.5069 - val_acc: 0.7604\n",
      "Epoch 622/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4510 - acc: 0.7656 - val_loss: 0.5070 - val_acc: 0.7552\n",
      "Epoch 623/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4509 - acc: 0.7656 - val_loss: 0.5070 - val_acc: 0.7552\n",
      "Epoch 624/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4509 - acc: 0.7656 - val_loss: 0.5070 - val_acc: 0.7552\n",
      "Epoch 625/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4509 - acc: 0.7656 - val_loss: 0.5070 - val_acc: 0.7552\n",
      "Epoch 626/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4509 - acc: 0.7656 - val_loss: 0.5071 - val_acc: 0.7552\n",
      "Epoch 627/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4509 - acc: 0.7656 - val_loss: 0.5071 - val_acc: 0.7552\n",
      "Epoch 628/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4508 - acc: 0.7656 - val_loss: 0.5071 - val_acc: 0.7552\n",
      "Epoch 629/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4508 - acc: 0.7656 - val_loss: 0.5071 - val_acc: 0.7552\n",
      "Epoch 630/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4508 - acc: 0.7656 - val_loss: 0.5072 - val_acc: 0.7552\n",
      "Epoch 631/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4508 - acc: 0.7639 - val_loss: 0.5072 - val_acc: 0.7552\n",
      "Epoch 632/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4508 - acc: 0.7639 - val_loss: 0.5072 - val_acc: 0.7552\n",
      "Epoch 633/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4507 - acc: 0.7656 - val_loss: 0.5072 - val_acc: 0.7552\n",
      "Epoch 634/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4507 - acc: 0.7639 - val_loss: 0.5073 - val_acc: 0.7552\n",
      "Epoch 635/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4507 - acc: 0.7639 - val_loss: 0.5073 - val_acc: 0.7552\n",
      "Epoch 636/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4507 - acc: 0.7639 - val_loss: 0.5073 - val_acc: 0.7552\n",
      "Epoch 637/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4507 - acc: 0.7639 - val_loss: 0.5073 - val_acc: 0.7552\n",
      "Epoch 638/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4507 - acc: 0.7639 - val_loss: 0.5073 - val_acc: 0.7552\n",
      "Epoch 639/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4506 - acc: 0.7639 - val_loss: 0.5074 - val_acc: 0.7552\n",
      "Epoch 640/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4506 - acc: 0.7639 - val_loss: 0.5074 - val_acc: 0.7552\n",
      "Epoch 641/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4506 - acc: 0.7639 - val_loss: 0.5074 - val_acc: 0.7552\n",
      "Epoch 642/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4506 - acc: 0.7639 - val_loss: 0.5074 - val_acc: 0.7552\n",
      "Epoch 643/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4506 - acc: 0.7639 - val_loss: 0.5074 - val_acc: 0.7552\n",
      "Epoch 644/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4506 - acc: 0.7639 - val_loss: 0.5075 - val_acc: 0.7552\n",
      "Epoch 645/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4506 - acc: 0.7639 - val_loss: 0.5075 - val_acc: 0.7552\n",
      "Epoch 646/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4505 - acc: 0.7639 - val_loss: 0.5075 - val_acc: 0.7552\n",
      "Epoch 647/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4505 - acc: 0.7639 - val_loss: 0.5075 - val_acc: 0.7552\n",
      "Epoch 648/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4505 - acc: 0.7639 - val_loss: 0.5075 - val_acc: 0.7552\n",
      "Epoch 649/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4504 - acc: 0.7639 - val_loss: 0.5075 - val_acc: 0.7552\n",
      "Epoch 650/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4505 - acc: 0.7639 - val_loss: 0.5076 - val_acc: 0.7552\n",
      "Epoch 651/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4504 - acc: 0.7639 - val_loss: 0.5076 - val_acc: 0.7552\n",
      "Epoch 652/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4504 - acc: 0.7639 - val_loss: 0.5076 - val_acc: 0.7552\n",
      "Epoch 653/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4504 - acc: 0.7639 - val_loss: 0.5076 - val_acc: 0.7552\n",
      "Epoch 654/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4504 - acc: 0.7639 - val_loss: 0.5076 - val_acc: 0.7552\n",
      "Epoch 655/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4504 - acc: 0.7639 - val_loss: 0.5077 - val_acc: 0.7552\n",
      "Epoch 656/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4504 - acc: 0.7639 - val_loss: 0.5077 - val_acc: 0.7552\n",
      "Epoch 657/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4503 - acc: 0.7639 - val_loss: 0.5077 - val_acc: 0.7552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 658/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4503 - acc: 0.7639 - val_loss: 0.5077 - val_acc: 0.7552\n",
      "Epoch 659/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4503 - acc: 0.7639 - val_loss: 0.5077 - val_acc: 0.7552\n",
      "Epoch 660/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4503 - acc: 0.7639 - val_loss: 0.5077 - val_acc: 0.7552\n",
      "Epoch 661/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4503 - acc: 0.7639 - val_loss: 0.5078 - val_acc: 0.7552\n",
      "Epoch 662/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4503 - acc: 0.7639 - val_loss: 0.5078 - val_acc: 0.7552\n",
      "Epoch 663/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4502 - acc: 0.7639 - val_loss: 0.5078 - val_acc: 0.7552\n",
      "Epoch 664/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4503 - acc: 0.7639 - val_loss: 0.5078 - val_acc: 0.7552\n",
      "Epoch 665/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4502 - acc: 0.7639 - val_loss: 0.5078 - val_acc: 0.7552\n",
      "Epoch 666/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4502 - acc: 0.7639 - val_loss: 0.5079 - val_acc: 0.7552\n",
      "Epoch 667/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4502 - acc: 0.7639 - val_loss: 0.5079 - val_acc: 0.7552\n",
      "Epoch 668/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4502 - acc: 0.7639 - val_loss: 0.5079 - val_acc: 0.7552\n",
      "Epoch 669/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4502 - acc: 0.7639 - val_loss: 0.5079 - val_acc: 0.7552\n",
      "Epoch 670/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4501 - acc: 0.7639 - val_loss: 0.5079 - val_acc: 0.7552\n",
      "Epoch 671/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4501 - acc: 0.7639 - val_loss: 0.5080 - val_acc: 0.7552\n",
      "Epoch 672/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4501 - acc: 0.7639 - val_loss: 0.5080 - val_acc: 0.7552\n",
      "Epoch 673/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4502 - acc: 0.7639 - val_loss: 0.5080 - val_acc: 0.7552\n",
      "Epoch 674/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4501 - acc: 0.7639 - val_loss: 0.5080 - val_acc: 0.7552\n",
      "Epoch 675/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4501 - acc: 0.7639 - val_loss: 0.5080 - val_acc: 0.7552\n",
      "Epoch 676/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4500 - acc: 0.7639 - val_loss: 0.5081 - val_acc: 0.7552\n",
      "Epoch 677/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4500 - acc: 0.7639 - val_loss: 0.5081 - val_acc: 0.7552\n",
      "Epoch 678/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4500 - acc: 0.7639 - val_loss: 0.5081 - val_acc: 0.7552\n",
      "Epoch 679/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4500 - acc: 0.7639 - val_loss: 0.5081 - val_acc: 0.7552\n",
      "Epoch 680/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4500 - acc: 0.7639 - val_loss: 0.5082 - val_acc: 0.7552\n",
      "Epoch 681/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4500 - acc: 0.7639 - val_loss: 0.5082 - val_acc: 0.7552\n",
      "Epoch 682/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4500 - acc: 0.7639 - val_loss: 0.5082 - val_acc: 0.7552\n",
      "Epoch 683/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4499 - acc: 0.7639 - val_loss: 0.5082 - val_acc: 0.7604\n",
      "Epoch 684/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4499 - acc: 0.7639 - val_loss: 0.5082 - val_acc: 0.7604\n",
      "Epoch 685/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4499 - acc: 0.7639 - val_loss: 0.5083 - val_acc: 0.7604\n",
      "Epoch 686/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4499 - acc: 0.7639 - val_loss: 0.5083 - val_acc: 0.7656\n",
      "Epoch 687/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4499 - acc: 0.7639 - val_loss: 0.5083 - val_acc: 0.7656\n",
      "Epoch 688/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4499 - acc: 0.7639 - val_loss: 0.5083 - val_acc: 0.7656\n",
      "Epoch 689/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4498 - acc: 0.7639 - val_loss: 0.5084 - val_acc: 0.7656\n",
      "Epoch 690/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4498 - acc: 0.7639 - val_loss: 0.5084 - val_acc: 0.7656\n",
      "Epoch 691/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4498 - acc: 0.7639 - val_loss: 0.5084 - val_acc: 0.7656\n",
      "Epoch 692/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4498 - acc: 0.7639 - val_loss: 0.5084 - val_acc: 0.7656\n",
      "Epoch 693/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4498 - acc: 0.7639 - val_loss: 0.5084 - val_acc: 0.7656\n",
      "Epoch 694/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4498 - acc: 0.7639 - val_loss: 0.5085 - val_acc: 0.7656\n",
      "Epoch 695/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4498 - acc: 0.7639 - val_loss: 0.5085 - val_acc: 0.7656\n",
      "Epoch 696/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4497 - acc: 0.7656 - val_loss: 0.5085 - val_acc: 0.7656\n",
      "Epoch 697/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4497 - acc: 0.7639 - val_loss: 0.5085 - val_acc: 0.7656\n",
      "Epoch 698/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4497 - acc: 0.7639 - val_loss: 0.5085 - val_acc: 0.7656\n",
      "Epoch 699/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4497 - acc: 0.7639 - val_loss: 0.5086 - val_acc: 0.7656\n",
      "Epoch 700/1500\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4497 - acc: 0.7639 - val_loss: 0.5086 - val_acc: 0.7656\n",
      "Epoch 701/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4496 - acc: 0.7639 - val_loss: 0.5086 - val_acc: 0.7656\n",
      "Epoch 702/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4497 - acc: 0.7656 - val_loss: 0.5086 - val_acc: 0.7656\n",
      "Epoch 703/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4496 - acc: 0.7639 - val_loss: 0.5086 - val_acc: 0.7656\n",
      "Epoch 704/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4496 - acc: 0.7639 - val_loss: 0.5087 - val_acc: 0.7656\n",
      "Epoch 705/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4496 - acc: 0.7656 - val_loss: 0.5087 - val_acc: 0.7656\n",
      "Epoch 706/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4496 - acc: 0.7656 - val_loss: 0.5087 - val_acc: 0.7656\n",
      "Epoch 707/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4495 - acc: 0.7656 - val_loss: 0.5087 - val_acc: 0.7656\n",
      "Epoch 708/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4495 - acc: 0.7656 - val_loss: 0.5087 - val_acc: 0.7656\n",
      "Epoch 709/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4495 - acc: 0.7656 - val_loss: 0.5088 - val_acc: 0.7656\n",
      "Epoch 710/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4495 - acc: 0.7656 - val_loss: 0.5088 - val_acc: 0.7656\n",
      "Epoch 711/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4495 - acc: 0.7656 - val_loss: 0.5088 - val_acc: 0.7656\n",
      "Epoch 712/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4495 - acc: 0.7656 - val_loss: 0.5088 - val_acc: 0.7656\n",
      "Epoch 713/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4494 - acc: 0.7656 - val_loss: 0.5088 - val_acc: 0.7656\n",
      "Epoch 714/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4495 - acc: 0.7656 - val_loss: 0.5088 - val_acc: 0.7656\n",
      "Epoch 715/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4494 - acc: 0.7656 - val_loss: 0.5088 - val_acc: 0.7656\n",
      "Epoch 716/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4495 - acc: 0.7656 - val_loss: 0.5088 - val_acc: 0.7656\n",
      "Epoch 717/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4494 - acc: 0.7656 - val_loss: 0.5088 - val_acc: 0.7656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 718/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4494 - acc: 0.7656 - val_loss: 0.5089 - val_acc: 0.7656\n",
      "Epoch 719/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4493 - acc: 0.7656 - val_loss: 0.5089 - val_acc: 0.7656\n",
      "Epoch 720/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4493 - acc: 0.7656 - val_loss: 0.5089 - val_acc: 0.7656\n",
      "Epoch 721/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4493 - acc: 0.7656 - val_loss: 0.5089 - val_acc: 0.7656\n",
      "Epoch 722/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4493 - acc: 0.7656 - val_loss: 0.5089 - val_acc: 0.7656\n",
      "Epoch 723/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4493 - acc: 0.7656 - val_loss: 0.5089 - val_acc: 0.7656\n",
      "Epoch 724/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4493 - acc: 0.7656 - val_loss: 0.5089 - val_acc: 0.7656\n",
      "Epoch 725/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4492 - acc: 0.7656 - val_loss: 0.5090 - val_acc: 0.7656\n",
      "Epoch 726/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4492 - acc: 0.7656 - val_loss: 0.5090 - val_acc: 0.7656\n",
      "Epoch 727/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4492 - acc: 0.7656 - val_loss: 0.5090 - val_acc: 0.7656\n",
      "Epoch 728/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4492 - acc: 0.7656 - val_loss: 0.5090 - val_acc: 0.7656\n",
      "Epoch 729/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4492 - acc: 0.7656 - val_loss: 0.5090 - val_acc: 0.7656\n",
      "Epoch 730/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4492 - acc: 0.7656 - val_loss: 0.5090 - val_acc: 0.7656\n",
      "Epoch 731/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4491 - acc: 0.7656 - val_loss: 0.5091 - val_acc: 0.7656\n",
      "Epoch 732/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4491 - acc: 0.7656 - val_loss: 0.5091 - val_acc: 0.7656\n",
      "Epoch 733/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4491 - acc: 0.7656 - val_loss: 0.5091 - val_acc: 0.7656\n",
      "Epoch 734/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4491 - acc: 0.7656 - val_loss: 0.5091 - val_acc: 0.7656\n",
      "Epoch 735/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4491 - acc: 0.7656 - val_loss: 0.5091 - val_acc: 0.7656\n",
      "Epoch 736/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4491 - acc: 0.7656 - val_loss: 0.5091 - val_acc: 0.7656\n",
      "Epoch 737/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4490 - acc: 0.7656 - val_loss: 0.5092 - val_acc: 0.7656\n",
      "Epoch 738/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4490 - acc: 0.7656 - val_loss: 0.5092 - val_acc: 0.7656\n",
      "Epoch 739/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4490 - acc: 0.7656 - val_loss: 0.5092 - val_acc: 0.7656\n",
      "Epoch 740/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4490 - acc: 0.7656 - val_loss: 0.5092 - val_acc: 0.7656\n",
      "Epoch 741/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4489 - acc: 0.7656 - val_loss: 0.5092 - val_acc: 0.7656\n",
      "Epoch 742/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4490 - acc: 0.7656 - val_loss: 0.5092 - val_acc: 0.7656\n",
      "Epoch 743/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4489 - acc: 0.7656 - val_loss: 0.5092 - val_acc: 0.7656\n",
      "Epoch 744/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4489 - acc: 0.7656 - val_loss: 0.5093 - val_acc: 0.7656\n",
      "Epoch 745/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4489 - acc: 0.7656 - val_loss: 0.5093 - val_acc: 0.7656\n",
      "Epoch 746/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4489 - acc: 0.7656 - val_loss: 0.5093 - val_acc: 0.7656\n",
      "Epoch 747/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4488 - acc: 0.7656 - val_loss: 0.5093 - val_acc: 0.7656\n",
      "Epoch 748/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4488 - acc: 0.7656 - val_loss: 0.5093 - val_acc: 0.7656\n",
      "Epoch 749/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4488 - acc: 0.7656 - val_loss: 0.5093 - val_acc: 0.7656\n",
      "Epoch 750/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4488 - acc: 0.7656 - val_loss: 0.5093 - val_acc: 0.7656\n",
      "Epoch 751/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4488 - acc: 0.7656 - val_loss: 0.5093 - val_acc: 0.7656\n",
      "Epoch 752/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4487 - acc: 0.7656 - val_loss: 0.5094 - val_acc: 0.7656\n",
      "Epoch 753/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4487 - acc: 0.7639 - val_loss: 0.5094 - val_acc: 0.7656\n",
      "Epoch 754/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4487 - acc: 0.7639 - val_loss: 0.5094 - val_acc: 0.7656\n",
      "Epoch 755/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4487 - acc: 0.7639 - val_loss: 0.5094 - val_acc: 0.7656\n",
      "Epoch 756/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4487 - acc: 0.7639 - val_loss: 0.5094 - val_acc: 0.7656\n",
      "Epoch 757/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4486 - acc: 0.7639 - val_loss: 0.5094 - val_acc: 0.7656\n",
      "Epoch 758/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4486 - acc: 0.7639 - val_loss: 0.5094 - val_acc: 0.7656\n",
      "Epoch 759/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4486 - acc: 0.7639 - val_loss: 0.5095 - val_acc: 0.7656\n",
      "Epoch 760/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4486 - acc: 0.7639 - val_loss: 0.5095 - val_acc: 0.7656\n",
      "Epoch 761/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4486 - acc: 0.7639 - val_loss: 0.5095 - val_acc: 0.7656\n",
      "Epoch 762/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4486 - acc: 0.7622 - val_loss: 0.5095 - val_acc: 0.7656\n",
      "Epoch 763/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4486 - acc: 0.7639 - val_loss: 0.5095 - val_acc: 0.7656\n",
      "Epoch 764/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4485 - acc: 0.7639 - val_loss: 0.5096 - val_acc: 0.7656\n",
      "Epoch 765/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4485 - acc: 0.7639 - val_loss: 0.5096 - val_acc: 0.7604\n",
      "Epoch 766/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4485 - acc: 0.7622 - val_loss: 0.5096 - val_acc: 0.7604\n",
      "Epoch 767/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4485 - acc: 0.7639 - val_loss: 0.5096 - val_acc: 0.7604\n",
      "Epoch 768/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4485 - acc: 0.7622 - val_loss: 0.5096 - val_acc: 0.7604\n",
      "Epoch 769/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4484 - acc: 0.7639 - val_loss: 0.5096 - val_acc: 0.7604\n",
      "Epoch 770/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4484 - acc: 0.7639 - val_loss: 0.5097 - val_acc: 0.7604\n",
      "Epoch 771/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4484 - acc: 0.7622 - val_loss: 0.5097 - val_acc: 0.7604\n",
      "Epoch 772/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4484 - acc: 0.7622 - val_loss: 0.5097 - val_acc: 0.7604\n",
      "Epoch 773/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4484 - acc: 0.7622 - val_loss: 0.5097 - val_acc: 0.7604\n",
      "Epoch 774/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4483 - acc: 0.7622 - val_loss: 0.5097 - val_acc: 0.7604\n",
      "Epoch 775/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4483 - acc: 0.7622 - val_loss: 0.5097 - val_acc: 0.7604\n",
      "Epoch 776/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4483 - acc: 0.7639 - val_loss: 0.5097 - val_acc: 0.7604\n",
      "Epoch 777/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4483 - acc: 0.7622 - val_loss: 0.5097 - val_acc: 0.7604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 778/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4483 - acc: 0.7639 - val_loss: 0.5097 - val_acc: 0.7604\n",
      "Epoch 779/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4482 - acc: 0.7622 - val_loss: 0.5097 - val_acc: 0.7604\n",
      "Epoch 780/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4483 - acc: 0.7622 - val_loss: 0.5098 - val_acc: 0.7604\n",
      "Epoch 781/1500\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4482 - acc: 0.7622 - val_loss: 0.5098 - val_acc: 0.7604\n",
      "Epoch 782/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4482 - acc: 0.7622 - val_loss: 0.5098 - val_acc: 0.7604\n",
      "Epoch 783/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4482 - acc: 0.7622 - val_loss: 0.5098 - val_acc: 0.7604\n",
      "Epoch 784/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4482 - acc: 0.7622 - val_loss: 0.5097 - val_acc: 0.7604\n",
      "Epoch 785/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4482 - acc: 0.7622 - val_loss: 0.5097 - val_acc: 0.7604\n",
      "Epoch 786/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4482 - acc: 0.7604 - val_loss: 0.5097 - val_acc: 0.7604\n",
      "Epoch 787/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4481 - acc: 0.7622 - val_loss: 0.5097 - val_acc: 0.7656\n",
      "Epoch 788/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4481 - acc: 0.7604 - val_loss: 0.5097 - val_acc: 0.7656\n",
      "Epoch 789/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4481 - acc: 0.7622 - val_loss: 0.5098 - val_acc: 0.7656\n",
      "Epoch 790/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4481 - acc: 0.7622 - val_loss: 0.5098 - val_acc: 0.7656\n",
      "Epoch 791/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4481 - acc: 0.7604 - val_loss: 0.5098 - val_acc: 0.7656\n",
      "Epoch 792/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4481 - acc: 0.7604 - val_loss: 0.5098 - val_acc: 0.7656\n",
      "Epoch 793/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4481 - acc: 0.7604 - val_loss: 0.5098 - val_acc: 0.7656\n",
      "Epoch 794/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4480 - acc: 0.7604 - val_loss: 0.5098 - val_acc: 0.7656\n",
      "Epoch 795/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4480 - acc: 0.7604 - val_loss: 0.5098 - val_acc: 0.7656\n",
      "Epoch 796/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4480 - acc: 0.7604 - val_loss: 0.5098 - val_acc: 0.7656\n",
      "Epoch 797/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4480 - acc: 0.7604 - val_loss: 0.5097 - val_acc: 0.7656\n",
      "Epoch 798/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4480 - acc: 0.7604 - val_loss: 0.5097 - val_acc: 0.7656\n",
      "Epoch 799/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4480 - acc: 0.7604 - val_loss: 0.5097 - val_acc: 0.7656\n",
      "Epoch 800/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4480 - acc: 0.7604 - val_loss: 0.5097 - val_acc: 0.7656\n",
      "Epoch 801/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4480 - acc: 0.7604 - val_loss: 0.5097 - val_acc: 0.7656\n",
      "Epoch 802/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4480 - acc: 0.7604 - val_loss: 0.5097 - val_acc: 0.7656\n",
      "Epoch 803/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4480 - acc: 0.7604 - val_loss: 0.5097 - val_acc: 0.7656\n",
      "Epoch 804/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4479 - acc: 0.7604 - val_loss: 0.5097 - val_acc: 0.7656\n",
      "Epoch 805/1500\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4479 - acc: 0.7604 - val_loss: 0.5097 - val_acc: 0.7656\n",
      "Epoch 806/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4479 - acc: 0.7604 - val_loss: 0.5097 - val_acc: 0.7656\n",
      "Epoch 807/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4479 - acc: 0.7604 - val_loss: 0.5097 - val_acc: 0.7656\n",
      "Epoch 808/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4479 - acc: 0.7604 - val_loss: 0.5097 - val_acc: 0.7656\n",
      "Epoch 809/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4479 - acc: 0.7604 - val_loss: 0.5097 - val_acc: 0.7656\n",
      "Epoch 810/1500\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4478 - acc: 0.7604 - val_loss: 0.5097 - val_acc: 0.7656\n",
      "Epoch 811/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4478 - acc: 0.7604 - val_loss: 0.5097 - val_acc: 0.7656\n",
      "Epoch 812/1500\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4478 - acc: 0.7604 - val_loss: 0.5097 - val_acc: 0.7656\n",
      "Epoch 813/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4478 - acc: 0.7604 - val_loss: 0.5097 - val_acc: 0.7656\n",
      "Epoch 814/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4478 - acc: 0.7622 - val_loss: 0.5097 - val_acc: 0.7656\n",
      "Epoch 815/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4478 - acc: 0.7622 - val_loss: 0.5097 - val_acc: 0.7656\n",
      "Epoch 816/1500\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4478 - acc: 0.7622 - val_loss: 0.5097 - val_acc: 0.7656\n",
      "Epoch 817/1500\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4478 - acc: 0.7622 - val_loss: 0.5097 - val_acc: 0.7656\n",
      "Epoch 818/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4477 - acc: 0.7622 - val_loss: 0.5097 - val_acc: 0.7656\n",
      "Epoch 819/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4478 - acc: 0.7622 - val_loss: 0.5097 - val_acc: 0.7656\n",
      "Epoch 820/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4477 - acc: 0.7622 - val_loss: 0.5097 - val_acc: 0.7656\n",
      "Epoch 821/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4477 - acc: 0.7622 - val_loss: 0.5097 - val_acc: 0.7656\n",
      "Epoch 822/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4477 - acc: 0.7622 - val_loss: 0.5097 - val_acc: 0.7656\n",
      "Epoch 823/1500\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4477 - acc: 0.7622 - val_loss: 0.5097 - val_acc: 0.7656\n",
      "Epoch 824/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4477 - acc: 0.7622 - val_loss: 0.5097 - val_acc: 0.7656\n",
      "Epoch 825/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4477 - acc: 0.7622 - val_loss: 0.5097 - val_acc: 0.7656\n",
      "Epoch 826/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4477 - acc: 0.7622 - val_loss: 0.5097 - val_acc: 0.7656\n",
      "Epoch 827/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4477 - acc: 0.7622 - val_loss: 0.5097 - val_acc: 0.7656\n",
      "Epoch 828/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4477 - acc: 0.7622 - val_loss: 0.5096 - val_acc: 0.7656\n",
      "Epoch 829/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4476 - acc: 0.7622 - val_loss: 0.5096 - val_acc: 0.7656\n",
      "Epoch 830/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4476 - acc: 0.7622 - val_loss: 0.5096 - val_acc: 0.7656\n",
      "Epoch 831/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4476 - acc: 0.7622 - val_loss: 0.5096 - val_acc: 0.7656\n",
      "Epoch 832/1500\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.4476 - acc: 0.7622 - val_loss: 0.5096 - val_acc: 0.7656\n",
      "Epoch 833/1500\n",
      "576/576 [==============================] - 0s 83us/step - loss: 0.4476 - acc: 0.7622 - val_loss: 0.5096 - val_acc: 0.7656\n",
      "Epoch 834/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4476 - acc: 0.7622 - val_loss: 0.5096 - val_acc: 0.7656\n",
      "Epoch 835/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4476 - acc: 0.7622 - val_loss: 0.5096 - val_acc: 0.7656\n",
      "Epoch 836/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4476 - acc: 0.7622 - val_loss: 0.5096 - val_acc: 0.7656\n",
      "Epoch 837/1500\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4476 - acc: 0.7622 - val_loss: 0.5096 - val_acc: 0.7656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 838/1500\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4475 - acc: 0.7622 - val_loss: 0.5096 - val_acc: 0.7656\n",
      "Epoch 839/1500\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4475 - acc: 0.7622 - val_loss: 0.5096 - val_acc: 0.7656\n",
      "Epoch 840/1500\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.4475 - acc: 0.7622 - val_loss: 0.5096 - val_acc: 0.7656\n",
      "Epoch 841/1500\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.4475 - acc: 0.7622 - val_loss: 0.5096 - val_acc: 0.7656\n",
      "Epoch 842/1500\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4475 - acc: 0.7622 - val_loss: 0.5096 - val_acc: 0.7656\n",
      "Epoch 843/1500\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.4475 - acc: 0.7604 - val_loss: 0.5096 - val_acc: 0.7656\n",
      "Epoch 844/1500\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.4475 - acc: 0.7622 - val_loss: 0.5096 - val_acc: 0.7656\n",
      "Epoch 845/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4475 - acc: 0.7622 - val_loss: 0.5096 - val_acc: 0.7656\n",
      "Epoch 846/1500\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.4475 - acc: 0.7622 - val_loss: 0.5096 - val_acc: 0.7656\n",
      "Epoch 847/1500\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4475 - acc: 0.7622 - val_loss: 0.5096 - val_acc: 0.7656\n",
      "Epoch 848/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4475 - acc: 0.7622 - val_loss: 0.5096 - val_acc: 0.7656\n",
      "Epoch 849/1500\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.4475 - acc: 0.7622 - val_loss: 0.5096 - val_acc: 0.7656\n",
      "Epoch 850/1500\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4474 - acc: 0.7622 - val_loss: 0.5095 - val_acc: 0.7656\n",
      "Epoch 851/1500\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.4474 - acc: 0.7622 - val_loss: 0.5095 - val_acc: 0.7656\n",
      "Epoch 852/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4474 - acc: 0.7622 - val_loss: 0.5095 - val_acc: 0.7656\n",
      "Epoch 853/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4474 - acc: 0.7639 - val_loss: 0.5095 - val_acc: 0.7656\n",
      "Epoch 854/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4474 - acc: 0.7639 - val_loss: 0.5095 - val_acc: 0.7656\n",
      "Epoch 855/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4474 - acc: 0.7639 - val_loss: 0.5095 - val_acc: 0.7656\n",
      "Epoch 856/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4473 - acc: 0.7639 - val_loss: 0.5095 - val_acc: 0.7656\n",
      "Epoch 857/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4474 - acc: 0.7639 - val_loss: 0.5095 - val_acc: 0.7656\n",
      "Epoch 858/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4474 - acc: 0.7639 - val_loss: 0.5095 - val_acc: 0.7656\n",
      "Epoch 859/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4473 - acc: 0.7622 - val_loss: 0.5095 - val_acc: 0.7656\n",
      "Epoch 860/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4474 - acc: 0.7639 - val_loss: 0.5095 - val_acc: 0.7656\n",
      "Epoch 861/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4473 - acc: 0.7639 - val_loss: 0.5095 - val_acc: 0.7656\n",
      "Epoch 862/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4473 - acc: 0.7639 - val_loss: 0.5095 - val_acc: 0.7656\n",
      "Epoch 863/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4473 - acc: 0.7639 - val_loss: 0.5095 - val_acc: 0.7656\n",
      "Epoch 864/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4473 - acc: 0.7639 - val_loss: 0.5095 - val_acc: 0.7656\n",
      "Epoch 865/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4473 - acc: 0.7639 - val_loss: 0.5095 - val_acc: 0.7656\n",
      "Epoch 866/1500\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4473 - acc: 0.7639 - val_loss: 0.5095 - val_acc: 0.7656\n",
      "Epoch 867/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4473 - acc: 0.7639 - val_loss: 0.5095 - val_acc: 0.7656\n",
      "Epoch 868/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4473 - acc: 0.7639 - val_loss: 0.5095 - val_acc: 0.7656\n",
      "Epoch 869/1500\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4473 - acc: 0.7639 - val_loss: 0.5094 - val_acc: 0.7656\n",
      "Epoch 870/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4473 - acc: 0.7639 - val_loss: 0.5094 - val_acc: 0.7656\n",
      "Epoch 871/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4472 - acc: 0.7639 - val_loss: 0.5094 - val_acc: 0.7656\n",
      "Epoch 872/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4472 - acc: 0.7639 - val_loss: 0.5094 - val_acc: 0.7656\n",
      "Epoch 873/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4472 - acc: 0.7639 - val_loss: 0.5094 - val_acc: 0.7656\n",
      "Epoch 874/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4472 - acc: 0.7622 - val_loss: 0.5094 - val_acc: 0.7656\n",
      "Epoch 875/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4472 - acc: 0.7639 - val_loss: 0.5094 - val_acc: 0.7656\n",
      "Epoch 876/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4472 - acc: 0.7622 - val_loss: 0.5094 - val_acc: 0.7656\n",
      "Epoch 877/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4472 - acc: 0.7622 - val_loss: 0.5094 - val_acc: 0.7656\n",
      "Epoch 878/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4472 - acc: 0.7639 - val_loss: 0.5094 - val_acc: 0.7656\n",
      "Epoch 879/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4471 - acc: 0.7639 - val_loss: 0.5094 - val_acc: 0.7656\n",
      "Epoch 880/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4471 - acc: 0.7656 - val_loss: 0.5093 - val_acc: 0.7656\n",
      "Epoch 881/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4471 - acc: 0.7622 - val_loss: 0.5093 - val_acc: 0.7656\n",
      "Epoch 882/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4471 - acc: 0.7622 - val_loss: 0.5093 - val_acc: 0.7656\n",
      "Epoch 883/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4471 - acc: 0.7656 - val_loss: 0.5093 - val_acc: 0.7656\n",
      "Epoch 884/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4471 - acc: 0.7656 - val_loss: 0.5093 - val_acc: 0.7656\n",
      "Epoch 885/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4471 - acc: 0.7622 - val_loss: 0.5093 - val_acc: 0.7656\n",
      "Epoch 886/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4470 - acc: 0.7639 - val_loss: 0.5093 - val_acc: 0.7656\n",
      "Epoch 887/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4470 - acc: 0.7639 - val_loss: 0.5092 - val_acc: 0.7656\n",
      "Epoch 888/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4470 - acc: 0.7639 - val_loss: 0.5092 - val_acc: 0.7656\n",
      "Epoch 889/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4470 - acc: 0.7656 - val_loss: 0.5092 - val_acc: 0.7656\n",
      "Epoch 890/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4470 - acc: 0.7639 - val_loss: 0.5092 - val_acc: 0.7656\n",
      "Epoch 891/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4470 - acc: 0.7639 - val_loss: 0.5092 - val_acc: 0.7656\n",
      "Epoch 892/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4470 - acc: 0.7656 - val_loss: 0.5092 - val_acc: 0.7656\n",
      "Epoch 893/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4470 - acc: 0.7656 - val_loss: 0.5092 - val_acc: 0.7656\n",
      "Epoch 894/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4470 - acc: 0.7639 - val_loss: 0.5092 - val_acc: 0.7656\n",
      "Epoch 895/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4469 - acc: 0.7656 - val_loss: 0.5091 - val_acc: 0.7656\n",
      "Epoch 896/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4469 - acc: 0.7656 - val_loss: 0.5091 - val_acc: 0.7656\n",
      "Epoch 897/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4469 - acc: 0.7639 - val_loss: 0.5091 - val_acc: 0.7656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 898/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4469 - acc: 0.7639 - val_loss: 0.5091 - val_acc: 0.7656\n",
      "Epoch 899/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4469 - acc: 0.7639 - val_loss: 0.5091 - val_acc: 0.7656\n",
      "Epoch 900/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4469 - acc: 0.7639 - val_loss: 0.5091 - val_acc: 0.7656\n",
      "Epoch 901/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4468 - acc: 0.7639 - val_loss: 0.5091 - val_acc: 0.7656\n",
      "Epoch 902/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4468 - acc: 0.7639 - val_loss: 0.5091 - val_acc: 0.7656\n",
      "Epoch 903/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4468 - acc: 0.7656 - val_loss: 0.5090 - val_acc: 0.7656\n",
      "Epoch 904/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4468 - acc: 0.7656 - val_loss: 0.5090 - val_acc: 0.7656\n",
      "Epoch 905/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4468 - acc: 0.7639 - val_loss: 0.5090 - val_acc: 0.7656\n",
      "Epoch 906/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4468 - acc: 0.7639 - val_loss: 0.5090 - val_acc: 0.7656\n",
      "Epoch 907/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4468 - acc: 0.7639 - val_loss: 0.5090 - val_acc: 0.7656\n",
      "Epoch 908/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4468 - acc: 0.7639 - val_loss: 0.5090 - val_acc: 0.7656\n",
      "Epoch 909/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4468 - acc: 0.7639 - val_loss: 0.5090 - val_acc: 0.7656\n",
      "Epoch 910/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4468 - acc: 0.7639 - val_loss: 0.5090 - val_acc: 0.7656\n",
      "Epoch 911/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4468 - acc: 0.7639 - val_loss: 0.5089 - val_acc: 0.7656\n",
      "Epoch 912/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4467 - acc: 0.7639 - val_loss: 0.5089 - val_acc: 0.7604\n",
      "Epoch 913/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4467 - acc: 0.7639 - val_loss: 0.5089 - val_acc: 0.7604\n",
      "Epoch 914/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4467 - acc: 0.7639 - val_loss: 0.5089 - val_acc: 0.7604\n",
      "Epoch 915/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4467 - acc: 0.7639 - val_loss: 0.5089 - val_acc: 0.7604\n",
      "Epoch 916/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4467 - acc: 0.7622 - val_loss: 0.5089 - val_acc: 0.7604\n",
      "Epoch 917/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4467 - acc: 0.7639 - val_loss: 0.5089 - val_acc: 0.7604\n",
      "Epoch 918/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4467 - acc: 0.7656 - val_loss: 0.5089 - val_acc: 0.7604\n",
      "Epoch 919/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4467 - acc: 0.7622 - val_loss: 0.5089 - val_acc: 0.7604\n",
      "Epoch 920/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4466 - acc: 0.7639 - val_loss: 0.5088 - val_acc: 0.7604\n",
      "Epoch 921/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4467 - acc: 0.7622 - val_loss: 0.5088 - val_acc: 0.7604\n",
      "Epoch 922/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4466 - acc: 0.7622 - val_loss: 0.5088 - val_acc: 0.7604\n",
      "Epoch 923/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4466 - acc: 0.7622 - val_loss: 0.5088 - val_acc: 0.7604\n",
      "Epoch 924/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4466 - acc: 0.7622 - val_loss: 0.5088 - val_acc: 0.7604\n",
      "Epoch 925/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4466 - acc: 0.7604 - val_loss: 0.5088 - val_acc: 0.7604\n",
      "Epoch 926/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4466 - acc: 0.7622 - val_loss: 0.5088 - val_acc: 0.7604\n",
      "Epoch 927/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4466 - acc: 0.7604 - val_loss: 0.5088 - val_acc: 0.7604\n",
      "Epoch 928/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4466 - acc: 0.7604 - val_loss: 0.5088 - val_acc: 0.7604\n",
      "Epoch 929/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4466 - acc: 0.7604 - val_loss: 0.5088 - val_acc: 0.7604\n",
      "Epoch 930/1500\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4466 - acc: 0.7604 - val_loss: 0.5088 - val_acc: 0.7604\n",
      "Epoch 931/1500\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.4465 - acc: 0.7622 - val_loss: 0.5087 - val_acc: 0.7604\n",
      "Epoch 932/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4465 - acc: 0.7622 - val_loss: 0.5087 - val_acc: 0.7604\n",
      "Epoch 933/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4465 - acc: 0.7622 - val_loss: 0.5087 - val_acc: 0.7604\n",
      "Epoch 934/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4465 - acc: 0.7604 - val_loss: 0.5087 - val_acc: 0.7604\n",
      "Epoch 935/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4465 - acc: 0.7622 - val_loss: 0.5087 - val_acc: 0.7604\n",
      "Epoch 936/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4465 - acc: 0.7622 - val_loss: 0.5087 - val_acc: 0.7604\n",
      "Epoch 937/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4465 - acc: 0.7604 - val_loss: 0.5087 - val_acc: 0.7604\n",
      "Epoch 938/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4464 - acc: 0.7604 - val_loss: 0.5087 - val_acc: 0.7604\n",
      "Epoch 939/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4464 - acc: 0.7622 - val_loss: 0.5087 - val_acc: 0.7604\n",
      "Epoch 940/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4464 - acc: 0.7622 - val_loss: 0.5087 - val_acc: 0.7604\n",
      "Epoch 941/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4464 - acc: 0.7622 - val_loss: 0.5087 - val_acc: 0.7604\n",
      "Epoch 942/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4464 - acc: 0.7604 - val_loss: 0.5087 - val_acc: 0.7604\n",
      "Epoch 943/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4464 - acc: 0.7604 - val_loss: 0.5087 - val_acc: 0.7604\n",
      "Epoch 944/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4464 - acc: 0.7622 - val_loss: 0.5086 - val_acc: 0.7604\n",
      "Epoch 945/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4464 - acc: 0.7604 - val_loss: 0.5086 - val_acc: 0.7604\n",
      "Epoch 946/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4464 - acc: 0.7604 - val_loss: 0.5086 - val_acc: 0.7604\n",
      "Epoch 947/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4464 - acc: 0.7604 - val_loss: 0.5086 - val_acc: 0.7604\n",
      "Epoch 948/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4464 - acc: 0.7604 - val_loss: 0.5086 - val_acc: 0.7604\n",
      "Epoch 949/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4464 - acc: 0.7604 - val_loss: 0.5086 - val_acc: 0.7604\n",
      "Epoch 950/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4463 - acc: 0.7604 - val_loss: 0.5086 - val_acc: 0.7604\n",
      "Epoch 951/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4463 - acc: 0.7604 - val_loss: 0.5086 - val_acc: 0.7604\n",
      "Epoch 952/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4463 - acc: 0.7604 - val_loss: 0.5086 - val_acc: 0.7604\n",
      "Epoch 953/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4463 - acc: 0.7604 - val_loss: 0.5086 - val_acc: 0.7604\n",
      "Epoch 954/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4463 - acc: 0.7604 - val_loss: 0.5086 - val_acc: 0.7604\n",
      "Epoch 955/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4463 - acc: 0.7604 - val_loss: 0.5086 - val_acc: 0.7604\n",
      "Epoch 956/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4463 - acc: 0.7604 - val_loss: 0.5085 - val_acc: 0.7604\n",
      "Epoch 957/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4462 - acc: 0.7604 - val_loss: 0.5085 - val_acc: 0.7604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 958/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4463 - acc: 0.7604 - val_loss: 0.5085 - val_acc: 0.7604\n",
      "Epoch 959/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4462 - acc: 0.7604 - val_loss: 0.5085 - val_acc: 0.7604\n",
      "Epoch 960/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4462 - acc: 0.7604 - val_loss: 0.5085 - val_acc: 0.7604\n",
      "Epoch 961/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4462 - acc: 0.7604 - val_loss: 0.5085 - val_acc: 0.7604\n",
      "Epoch 962/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4462 - acc: 0.7604 - val_loss: 0.5085 - val_acc: 0.7604\n",
      "Epoch 963/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4462 - acc: 0.7604 - val_loss: 0.5085 - val_acc: 0.7604\n",
      "Epoch 964/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4462 - acc: 0.7604 - val_loss: 0.5085 - val_acc: 0.7604\n",
      "Epoch 965/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4462 - acc: 0.7604 - val_loss: 0.5085 - val_acc: 0.7604\n",
      "Epoch 966/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4462 - acc: 0.7604 - val_loss: 0.5085 - val_acc: 0.7604\n",
      "Epoch 967/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4462 - acc: 0.7604 - val_loss: 0.5085 - val_acc: 0.7604\n",
      "Epoch 968/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4462 - acc: 0.7604 - val_loss: 0.5085 - val_acc: 0.7604\n",
      "Epoch 969/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4462 - acc: 0.7604 - val_loss: 0.5085 - val_acc: 0.7604\n",
      "Epoch 970/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4462 - acc: 0.7604 - val_loss: 0.5084 - val_acc: 0.7604\n",
      "Epoch 971/1500\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4461 - acc: 0.7604 - val_loss: 0.5084 - val_acc: 0.7604\n",
      "Epoch 972/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4461 - acc: 0.7604 - val_loss: 0.5084 - val_acc: 0.7604\n",
      "Epoch 973/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4461 - acc: 0.7604 - val_loss: 0.5084 - val_acc: 0.7604\n",
      "Epoch 974/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4461 - acc: 0.7604 - val_loss: 0.5084 - val_acc: 0.7604\n",
      "Epoch 975/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4461 - acc: 0.7604 - val_loss: 0.5084 - val_acc: 0.7604\n",
      "Epoch 976/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4461 - acc: 0.7604 - val_loss: 0.5084 - val_acc: 0.7604\n",
      "Epoch 977/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4461 - acc: 0.7604 - val_loss: 0.5084 - val_acc: 0.7604\n",
      "Epoch 978/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4461 - acc: 0.7604 - val_loss: 0.5084 - val_acc: 0.7604\n",
      "Epoch 979/1500\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.4476 - acc: 0.812 - 0s 58us/step - loss: 0.4461 - acc: 0.7604 - val_loss: 0.5084 - val_acc: 0.7604\n",
      "Epoch 980/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4461 - acc: 0.7604 - val_loss: 0.5084 - val_acc: 0.7604\n",
      "Epoch 981/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4460 - acc: 0.7604 - val_loss: 0.5084 - val_acc: 0.7604\n",
      "Epoch 982/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4460 - acc: 0.7604 - val_loss: 0.5084 - val_acc: 0.7604\n",
      "Epoch 983/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4461 - acc: 0.7604 - val_loss: 0.5083 - val_acc: 0.7604\n",
      "Epoch 984/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4460 - acc: 0.7604 - val_loss: 0.5083 - val_acc: 0.7604\n",
      "Epoch 985/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4460 - acc: 0.7604 - val_loss: 0.5083 - val_acc: 0.7604\n",
      "Epoch 986/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4460 - acc: 0.7604 - val_loss: 0.5083 - val_acc: 0.7604\n",
      "Epoch 987/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4460 - acc: 0.7604 - val_loss: 0.5083 - val_acc: 0.7604\n",
      "Epoch 988/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4460 - acc: 0.7622 - val_loss: 0.5083 - val_acc: 0.7604\n",
      "Epoch 989/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4460 - acc: 0.7622 - val_loss: 0.5083 - val_acc: 0.7604\n",
      "Epoch 990/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4460 - acc: 0.7622 - val_loss: 0.5083 - val_acc: 0.7604\n",
      "Epoch 991/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4459 - acc: 0.7622 - val_loss: 0.5083 - val_acc: 0.7604\n",
      "Epoch 992/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4459 - acc: 0.7622 - val_loss: 0.5083 - val_acc: 0.7604\n",
      "Epoch 993/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4459 - acc: 0.7622 - val_loss: 0.5083 - val_acc: 0.7604\n",
      "Epoch 994/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4459 - acc: 0.7622 - val_loss: 0.5083 - val_acc: 0.7604\n",
      "Epoch 995/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4459 - acc: 0.7622 - val_loss: 0.5083 - val_acc: 0.7604\n",
      "Epoch 996/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4459 - acc: 0.7622 - val_loss: 0.5082 - val_acc: 0.7604\n",
      "Epoch 997/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4459 - acc: 0.7622 - val_loss: 0.5082 - val_acc: 0.7604\n",
      "Epoch 998/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4459 - acc: 0.7622 - val_loss: 0.5082 - val_acc: 0.7604\n",
      "Epoch 999/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4459 - acc: 0.7622 - val_loss: 0.5082 - val_acc: 0.7604\n",
      "Epoch 1000/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4459 - acc: 0.7622 - val_loss: 0.5082 - val_acc: 0.7604\n",
      "Epoch 1001/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4459 - acc: 0.7622 - val_loss: 0.5082 - val_acc: 0.7604\n",
      "Epoch 1002/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4459 - acc: 0.7656 - val_loss: 0.5082 - val_acc: 0.7604\n",
      "Epoch 1003/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4458 - acc: 0.7656 - val_loss: 0.5082 - val_acc: 0.7604\n",
      "Epoch 1004/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4458 - acc: 0.7656 - val_loss: 0.5082 - val_acc: 0.7604\n",
      "Epoch 1005/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4458 - acc: 0.7639 - val_loss: 0.5082 - val_acc: 0.7604\n",
      "Epoch 1006/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4458 - acc: 0.7622 - val_loss: 0.5082 - val_acc: 0.7604\n",
      "Epoch 1007/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4458 - acc: 0.7639 - val_loss: 0.5082 - val_acc: 0.7604\n",
      "Epoch 1008/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4458 - acc: 0.7674 - val_loss: 0.5082 - val_acc: 0.7604\n",
      "Epoch 1009/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4458 - acc: 0.7674 - val_loss: 0.5082 - val_acc: 0.7604\n",
      "Epoch 1010/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4458 - acc: 0.7656 - val_loss: 0.5081 - val_acc: 0.7604\n",
      "Epoch 1011/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4458 - acc: 0.7674 - val_loss: 0.5081 - val_acc: 0.7604\n",
      "Epoch 1012/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4457 - acc: 0.7674 - val_loss: 0.5081 - val_acc: 0.7604\n",
      "Epoch 1013/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4458 - acc: 0.7674 - val_loss: 0.5081 - val_acc: 0.7604\n",
      "Epoch 1014/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4457 - acc: 0.7674 - val_loss: 0.5081 - val_acc: 0.7604\n",
      "Epoch 1015/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4457 - acc: 0.7674 - val_loss: 0.5081 - val_acc: 0.7604\n",
      "Epoch 1016/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4457 - acc: 0.7674 - val_loss: 0.5081 - val_acc: 0.7604\n",
      "Epoch 1017/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 53us/step - loss: 0.4457 - acc: 0.7674 - val_loss: 0.5081 - val_acc: 0.7604\n",
      "Epoch 1018/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4457 - acc: 0.7691 - val_loss: 0.5081 - val_acc: 0.7604\n",
      "Epoch 1019/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4457 - acc: 0.7691 - val_loss: 0.5081 - val_acc: 0.7604\n",
      "Epoch 1020/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4457 - acc: 0.7691 - val_loss: 0.5081 - val_acc: 0.7604\n",
      "Epoch 1021/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4457 - acc: 0.7674 - val_loss: 0.5081 - val_acc: 0.7604\n",
      "Epoch 1022/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4457 - acc: 0.7674 - val_loss: 0.5081 - val_acc: 0.7604\n",
      "Epoch 1023/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4457 - acc: 0.7691 - val_loss: 0.5081 - val_acc: 0.7604\n",
      "Epoch 1024/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4456 - acc: 0.7691 - val_loss: 0.5080 - val_acc: 0.7604\n",
      "Epoch 1025/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4457 - acc: 0.7691 - val_loss: 0.5080 - val_acc: 0.7604\n",
      "Epoch 1026/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4456 - acc: 0.7691 - val_loss: 0.5080 - val_acc: 0.7604\n",
      "Epoch 1027/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4456 - acc: 0.7691 - val_loss: 0.5080 - val_acc: 0.7604\n",
      "Epoch 1028/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4456 - acc: 0.7691 - val_loss: 0.5080 - val_acc: 0.7604\n",
      "Epoch 1029/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4456 - acc: 0.7691 - val_loss: 0.5080 - val_acc: 0.7604\n",
      "Epoch 1030/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4456 - acc: 0.7691 - val_loss: 0.5080 - val_acc: 0.7604\n",
      "Epoch 1031/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4456 - acc: 0.7691 - val_loss: 0.5080 - val_acc: 0.7604\n",
      "Epoch 1032/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4456 - acc: 0.7691 - val_loss: 0.5080 - val_acc: 0.7604\n",
      "Epoch 1033/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4456 - acc: 0.7691 - val_loss: 0.5080 - val_acc: 0.7604\n",
      "Epoch 1034/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4456 - acc: 0.7708 - val_loss: 0.5080 - val_acc: 0.7604\n",
      "Epoch 1035/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4456 - acc: 0.7691 - val_loss: 0.5080 - val_acc: 0.7604\n",
      "Epoch 1036/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4456 - acc: 0.7708 - val_loss: 0.5080 - val_acc: 0.7604\n",
      "Epoch 1037/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4455 - acc: 0.7708 - val_loss: 0.5080 - val_acc: 0.7604\n",
      "Epoch 1038/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4455 - acc: 0.7708 - val_loss: 0.5080 - val_acc: 0.7604\n",
      "Epoch 1039/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4455 - acc: 0.7691 - val_loss: 0.5079 - val_acc: 0.7604\n",
      "Epoch 1040/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4455 - acc: 0.7708 - val_loss: 0.5079 - val_acc: 0.7604\n",
      "Epoch 1041/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4455 - acc: 0.7691 - val_loss: 0.5079 - val_acc: 0.7604\n",
      "Epoch 1042/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4455 - acc: 0.7691 - val_loss: 0.5079 - val_acc: 0.7604\n",
      "Epoch 1043/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4455 - acc: 0.7708 - val_loss: 0.5079 - val_acc: 0.7604\n",
      "Epoch 1044/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4455 - acc: 0.7708 - val_loss: 0.5079 - val_acc: 0.7604\n",
      "Epoch 1045/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4455 - acc: 0.7708 - val_loss: 0.5079 - val_acc: 0.7604\n",
      "Epoch 1046/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4455 - acc: 0.7691 - val_loss: 0.5079 - val_acc: 0.7604\n",
      "Epoch 1047/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4455 - acc: 0.7708 - val_loss: 0.5079 - val_acc: 0.7604\n",
      "Epoch 1048/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4455 - acc: 0.7691 - val_loss: 0.5079 - val_acc: 0.7604\n",
      "Epoch 1049/1500\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4455 - acc: 0.7691 - val_loss: 0.5079 - val_acc: 0.7604\n",
      "Epoch 1050/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4455 - acc: 0.7708 - val_loss: 0.5079 - val_acc: 0.7604\n",
      "Epoch 1051/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4454 - acc: 0.7708 - val_loss: 0.5079 - val_acc: 0.7604\n",
      "Epoch 1052/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4454 - acc: 0.7708 - val_loss: 0.5079 - val_acc: 0.7604\n",
      "Epoch 1053/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4454 - acc: 0.7708 - val_loss: 0.5079 - val_acc: 0.7604\n",
      "Epoch 1054/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4454 - acc: 0.7708 - val_loss: 0.5079 - val_acc: 0.7604\n",
      "Epoch 1055/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4454 - acc: 0.7708 - val_loss: 0.5079 - val_acc: 0.7604\n",
      "Epoch 1056/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4454 - acc: 0.7708 - val_loss: 0.5078 - val_acc: 0.7604\n",
      "Epoch 1057/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4454 - acc: 0.7708 - val_loss: 0.5078 - val_acc: 0.7604\n",
      "Epoch 1058/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4454 - acc: 0.7708 - val_loss: 0.5078 - val_acc: 0.7604\n",
      "Epoch 1059/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4454 - acc: 0.7708 - val_loss: 0.5078 - val_acc: 0.7604\n",
      "Epoch 1060/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4454 - acc: 0.7708 - val_loss: 0.5078 - val_acc: 0.7604\n",
      "Epoch 1061/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4454 - acc: 0.7708 - val_loss: 0.5078 - val_acc: 0.7604\n",
      "Epoch 1062/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4454 - acc: 0.7708 - val_loss: 0.5078 - val_acc: 0.7604\n",
      "Epoch 1063/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4454 - acc: 0.7708 - val_loss: 0.5078 - val_acc: 0.7604\n",
      "Epoch 1064/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4453 - acc: 0.7708 - val_loss: 0.5078 - val_acc: 0.7604\n",
      "Epoch 1065/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4453 - acc: 0.7708 - val_loss: 0.5078 - val_acc: 0.7604\n",
      "Epoch 1066/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4453 - acc: 0.7708 - val_loss: 0.5078 - val_acc: 0.7604\n",
      "Epoch 1067/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4453 - acc: 0.7708 - val_loss: 0.5078 - val_acc: 0.7604\n",
      "Epoch 1068/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4453 - acc: 0.7708 - val_loss: 0.5078 - val_acc: 0.7604\n",
      "Epoch 1069/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4453 - acc: 0.7708 - val_loss: 0.5078 - val_acc: 0.7604\n",
      "Epoch 1070/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4453 - acc: 0.7708 - val_loss: 0.5078 - val_acc: 0.7604\n",
      "Epoch 1071/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4453 - acc: 0.7708 - val_loss: 0.5078 - val_acc: 0.7604\n",
      "Epoch 1072/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4453 - acc: 0.7708 - val_loss: 0.5077 - val_acc: 0.7604\n",
      "Epoch 1073/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4453 - acc: 0.7708 - val_loss: 0.5077 - val_acc: 0.7604\n",
      "Epoch 1074/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4453 - acc: 0.7708 - val_loss: 0.5077 - val_acc: 0.7604\n",
      "Epoch 1075/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4453 - acc: 0.7708 - val_loss: 0.5077 - val_acc: 0.7604\n",
      "Epoch 1076/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 57us/step - loss: 0.4453 - acc: 0.7708 - val_loss: 0.5077 - val_acc: 0.7604\n",
      "Epoch 1077/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4452 - acc: 0.7708 - val_loss: 0.5077 - val_acc: 0.7604\n",
      "Epoch 1078/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4453 - acc: 0.7708 - val_loss: 0.5077 - val_acc: 0.7604\n",
      "Epoch 1079/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4453 - acc: 0.7708 - val_loss: 0.5077 - val_acc: 0.7604\n",
      "Epoch 1080/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4453 - acc: 0.7708 - val_loss: 0.5077 - val_acc: 0.7604\n",
      "Epoch 1081/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4452 - acc: 0.7708 - val_loss: 0.5077 - val_acc: 0.7604\n",
      "Epoch 1082/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4452 - acc: 0.7708 - val_loss: 0.5077 - val_acc: 0.7604\n",
      "Epoch 1083/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4452 - acc: 0.7708 - val_loss: 0.5077 - val_acc: 0.7604\n",
      "Epoch 1084/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4452 - acc: 0.7708 - val_loss: 0.5077 - val_acc: 0.7604\n",
      "Epoch 1085/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4452 - acc: 0.7708 - val_loss: 0.5077 - val_acc: 0.7604\n",
      "Epoch 1086/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4452 - acc: 0.7708 - val_loss: 0.5077 - val_acc: 0.7604\n",
      "Epoch 1087/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4452 - acc: 0.7708 - val_loss: 0.5076 - val_acc: 0.7604\n",
      "Epoch 1088/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4452 - acc: 0.7708 - val_loss: 0.5076 - val_acc: 0.7604\n",
      "Epoch 1089/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4452 - acc: 0.7708 - val_loss: 0.5076 - val_acc: 0.7604\n",
      "Epoch 1090/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4452 - acc: 0.7708 - val_loss: 0.5076 - val_acc: 0.7604\n",
      "Epoch 1091/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4452 - acc: 0.7708 - val_loss: 0.5076 - val_acc: 0.7604\n",
      "Epoch 1092/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4452 - acc: 0.7708 - val_loss: 0.5076 - val_acc: 0.7604\n",
      "Epoch 1093/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4452 - acc: 0.7708 - val_loss: 0.5076 - val_acc: 0.7604\n",
      "Epoch 1094/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4451 - acc: 0.7708 - val_loss: 0.5076 - val_acc: 0.7604\n",
      "Epoch 1095/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4451 - acc: 0.7708 - val_loss: 0.5076 - val_acc: 0.7604\n",
      "Epoch 1096/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4451 - acc: 0.7708 - val_loss: 0.5076 - val_acc: 0.7604\n",
      "Epoch 1097/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4451 - acc: 0.7708 - val_loss: 0.5076 - val_acc: 0.7604\n",
      "Epoch 1098/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4451 - acc: 0.7708 - val_loss: 0.5076 - val_acc: 0.7604\n",
      "Epoch 1099/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4451 - acc: 0.7708 - val_loss: 0.5076 - val_acc: 0.7604\n",
      "Epoch 1100/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4451 - acc: 0.7708 - val_loss: 0.5076 - val_acc: 0.7604\n",
      "Epoch 1101/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4451 - acc: 0.7708 - val_loss: 0.5076 - val_acc: 0.7604\n",
      "Epoch 1102/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4451 - acc: 0.7708 - val_loss: 0.5076 - val_acc: 0.7604\n",
      "Epoch 1103/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4451 - acc: 0.7708 - val_loss: 0.5076 - val_acc: 0.7604\n",
      "Epoch 1104/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4451 - acc: 0.7708 - val_loss: 0.5075 - val_acc: 0.7604\n",
      "Epoch 1105/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4451 - acc: 0.7708 - val_loss: 0.5075 - val_acc: 0.7604\n",
      "Epoch 1106/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4451 - acc: 0.7708 - val_loss: 0.5075 - val_acc: 0.7604\n",
      "Epoch 1107/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4451 - acc: 0.7708 - val_loss: 0.5075 - val_acc: 0.7604\n",
      "Epoch 1108/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4450 - acc: 0.7708 - val_loss: 0.5075 - val_acc: 0.7604\n",
      "Epoch 1109/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4451 - acc: 0.7708 - val_loss: 0.5075 - val_acc: 0.7604\n",
      "Epoch 1110/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4450 - acc: 0.7708 - val_loss: 0.5075 - val_acc: 0.7604\n",
      "Epoch 1111/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4450 - acc: 0.7708 - val_loss: 0.5075 - val_acc: 0.7604\n",
      "Epoch 1112/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4450 - acc: 0.7708 - val_loss: 0.5075 - val_acc: 0.7604\n",
      "Epoch 1113/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4451 - acc: 0.7708 - val_loss: 0.5075 - val_acc: 0.7604\n",
      "Epoch 1114/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4450 - acc: 0.7708 - val_loss: 0.5075 - val_acc: 0.7604\n",
      "Epoch 1115/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4450 - acc: 0.7708 - val_loss: 0.5075 - val_acc: 0.7604\n",
      "Epoch 1116/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4450 - acc: 0.7708 - val_loss: 0.5075 - val_acc: 0.7604\n",
      "Epoch 1117/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4450 - acc: 0.7708 - val_loss: 0.5075 - val_acc: 0.7604\n",
      "Epoch 1118/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4450 - acc: 0.7708 - val_loss: 0.5075 - val_acc: 0.7604\n",
      "Epoch 1119/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4450 - acc: 0.7708 - val_loss: 0.5075 - val_acc: 0.7604\n",
      "Epoch 1120/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4450 - acc: 0.7708 - val_loss: 0.5075 - val_acc: 0.7604\n",
      "Epoch 1121/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4450 - acc: 0.7708 - val_loss: 0.5075 - val_acc: 0.7604\n",
      "Epoch 1122/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4450 - acc: 0.7708 - val_loss: 0.5074 - val_acc: 0.7604\n",
      "Epoch 1123/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4450 - acc: 0.7708 - val_loss: 0.5074 - val_acc: 0.7604\n",
      "Epoch 1124/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4450 - acc: 0.7708 - val_loss: 0.5074 - val_acc: 0.7604\n",
      "Epoch 1125/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4450 - acc: 0.7708 - val_loss: 0.5074 - val_acc: 0.7604\n",
      "Epoch 1126/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4450 - acc: 0.7708 - val_loss: 0.5074 - val_acc: 0.7604\n",
      "Epoch 1127/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4450 - acc: 0.7708 - val_loss: 0.5074 - val_acc: 0.7604\n",
      "Epoch 1128/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4449 - acc: 0.7726 - val_loss: 0.5074 - val_acc: 0.7604\n",
      "Epoch 1129/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4449 - acc: 0.7708 - val_loss: 0.5074 - val_acc: 0.7604\n",
      "Epoch 1130/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4449 - acc: 0.7708 - val_loss: 0.5074 - val_acc: 0.7604\n",
      "Epoch 1131/1500\n",
      "576/576 [==============================] - 0s 88us/step - loss: 0.4449 - acc: 0.7708 - val_loss: 0.5074 - val_acc: 0.7604\n",
      "Epoch 1132/1500\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4449 - acc: 0.7708 - val_loss: 0.5074 - val_acc: 0.7604\n",
      "Epoch 1133/1500\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.4449 - acc: 0.7708 - val_loss: 0.5074 - val_acc: 0.7604\n",
      "Epoch 1134/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4449 - acc: 0.7708 - val_loss: 0.5074 - val_acc: 0.7604\n",
      "Epoch 1135/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 66us/step - loss: 0.4449 - acc: 0.7708 - val_loss: 0.5074 - val_acc: 0.7604\n",
      "Epoch 1136/1500\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4449 - acc: 0.7708 - val_loss: 0.5074 - val_acc: 0.7604\n",
      "Epoch 1137/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4449 - acc: 0.7708 - val_loss: 0.5074 - val_acc: 0.7604\n",
      "Epoch 1138/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4449 - acc: 0.7708 - val_loss: 0.5074 - val_acc: 0.7604\n",
      "Epoch 1139/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4449 - acc: 0.7708 - val_loss: 0.5074 - val_acc: 0.7604\n",
      "Epoch 1140/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4449 - acc: 0.7708 - val_loss: 0.5074 - val_acc: 0.7604\n",
      "Epoch 1141/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4449 - acc: 0.7708 - val_loss: 0.5074 - val_acc: 0.7604\n",
      "Epoch 1142/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4449 - acc: 0.7708 - val_loss: 0.5073 - val_acc: 0.7604\n",
      "Epoch 1143/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4449 - acc: 0.7708 - val_loss: 0.5073 - val_acc: 0.7604\n",
      "Epoch 1144/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4449 - acc: 0.7708 - val_loss: 0.5073 - val_acc: 0.7604\n",
      "Epoch 1145/1500\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.4407 - acc: 0.750 - 0s 72us/step - loss: 0.4449 - acc: 0.7708 - val_loss: 0.5073 - val_acc: 0.7604\n",
      "Epoch 1146/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4448 - acc: 0.7708 - val_loss: 0.5073 - val_acc: 0.7604\n",
      "Epoch 1147/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4449 - acc: 0.7708 - val_loss: 0.5073 - val_acc: 0.7604\n",
      "Epoch 1148/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4448 - acc: 0.7708 - val_loss: 0.5073 - val_acc: 0.7604\n",
      "Epoch 1149/1500\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4448 - acc: 0.7708 - val_loss: 0.5073 - val_acc: 0.7604\n",
      "Epoch 1150/1500\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4448 - acc: 0.7708 - val_loss: 0.5073 - val_acc: 0.7604\n",
      "Epoch 1151/1500\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4448 - acc: 0.7708 - val_loss: 0.5073 - val_acc: 0.7604\n",
      "Epoch 1152/1500\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4448 - acc: 0.7708 - val_loss: 0.5073 - val_acc: 0.7604\n",
      "Epoch 1153/1500\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.4448 - acc: 0.7708 - val_loss: 0.5073 - val_acc: 0.7604\n",
      "Epoch 1154/1500\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4448 - acc: 0.7708 - val_loss: 0.5073 - val_acc: 0.7604\n",
      "Epoch 1155/1500\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.4448 - acc: 0.7708 - val_loss: 0.5073 - val_acc: 0.7604\n",
      "Epoch 1156/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4448 - acc: 0.7708 - val_loss: 0.5073 - val_acc: 0.7604\n",
      "Epoch 1157/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4448 - acc: 0.7708 - val_loss: 0.5073 - val_acc: 0.7604\n",
      "Epoch 1158/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4448 - acc: 0.7708 - val_loss: 0.5072 - val_acc: 0.7604\n",
      "Epoch 1159/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4448 - acc: 0.7708 - val_loss: 0.5072 - val_acc: 0.7604\n",
      "Epoch 1160/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4448 - acc: 0.7726 - val_loss: 0.5072 - val_acc: 0.7604\n",
      "Epoch 1161/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4448 - acc: 0.7708 - val_loss: 0.5072 - val_acc: 0.7604\n",
      "Epoch 1162/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4448 - acc: 0.7708 - val_loss: 0.5072 - val_acc: 0.7604\n",
      "Epoch 1163/1500\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.4448 - acc: 0.7708 - val_loss: 0.5072 - val_acc: 0.7604\n",
      "Epoch 1164/1500\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.4447 - acc: 0.7708 - val_loss: 0.5072 - val_acc: 0.7604\n",
      "Epoch 1165/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4447 - acc: 0.7708 - val_loss: 0.5072 - val_acc: 0.7604\n",
      "Epoch 1166/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4447 - acc: 0.7708 - val_loss: 0.5072 - val_acc: 0.7604\n",
      "Epoch 1167/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4447 - acc: 0.7708 - val_loss: 0.5072 - val_acc: 0.7604\n",
      "Epoch 1168/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4447 - acc: 0.7708 - val_loss: 0.5072 - val_acc: 0.7604\n",
      "Epoch 1169/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4447 - acc: 0.7708 - val_loss: 0.5072 - val_acc: 0.7604\n",
      "Epoch 1170/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4447 - acc: 0.7726 - val_loss: 0.5072 - val_acc: 0.7604\n",
      "Epoch 1171/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4447 - acc: 0.7708 - val_loss: 0.5072 - val_acc: 0.7656\n",
      "Epoch 1172/1500\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.4447 - acc: 0.7708 - val_loss: 0.5072 - val_acc: 0.7656\n",
      "Epoch 1173/1500\n",
      "576/576 [==============================] - 0s 79us/step - loss: 0.4447 - acc: 0.7726 - val_loss: 0.5072 - val_acc: 0.7656\n",
      "Epoch 1174/1500\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4447 - acc: 0.7708 - val_loss: 0.5072 - val_acc: 0.7656\n",
      "Epoch 1175/1500\n",
      "576/576 [==============================] - 0s 83us/step - loss: 0.4447 - acc: 0.7708 - val_loss: 0.5072 - val_acc: 0.7656\n",
      "Epoch 1176/1500\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.4446 - acc: 0.7708 - val_loss: 0.5071 - val_acc: 0.7656\n",
      "Epoch 1177/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4447 - acc: 0.7708 - val_loss: 0.5071 - val_acc: 0.7656\n",
      "Epoch 1178/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4446 - acc: 0.7708 - val_loss: 0.5071 - val_acc: 0.7656\n",
      "Epoch 1179/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4446 - acc: 0.7708 - val_loss: 0.5071 - val_acc: 0.7656\n",
      "Epoch 1180/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4446 - acc: 0.7708 - val_loss: 0.5071 - val_acc: 0.7656\n",
      "Epoch 1181/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4446 - acc: 0.7726 - val_loss: 0.5071 - val_acc: 0.7656\n",
      "Epoch 1182/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4446 - acc: 0.7726 - val_loss: 0.5071 - val_acc: 0.7656\n",
      "Epoch 1183/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4446 - acc: 0.7691 - val_loss: 0.5071 - val_acc: 0.7656\n",
      "Epoch 1184/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4446 - acc: 0.7726 - val_loss: 0.5071 - val_acc: 0.7656\n",
      "Epoch 1185/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4446 - acc: 0.7726 - val_loss: 0.5071 - val_acc: 0.7656\n",
      "Epoch 1186/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4446 - acc: 0.7726 - val_loss: 0.5071 - val_acc: 0.7656\n",
      "Epoch 1187/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4446 - acc: 0.7726 - val_loss: 0.5071 - val_acc: 0.7656\n",
      "Epoch 1188/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4446 - acc: 0.7708 - val_loss: 0.5071 - val_acc: 0.7656\n",
      "Epoch 1189/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4446 - acc: 0.7726 - val_loss: 0.5071 - val_acc: 0.7656\n",
      "Epoch 1190/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4445 - acc: 0.7708 - val_loss: 0.5070 - val_acc: 0.7656\n",
      "Epoch 1191/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4445 - acc: 0.7708 - val_loss: 0.5070 - val_acc: 0.7656\n",
      "Epoch 1192/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4445 - acc: 0.7726 - val_loss: 0.5070 - val_acc: 0.7656\n",
      "Epoch 1193/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4445 - acc: 0.7726 - val_loss: 0.5070 - val_acc: 0.7656\n",
      "Epoch 1194/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 65us/step - loss: 0.4445 - acc: 0.7726 - val_loss: 0.5070 - val_acc: 0.7656\n",
      "Epoch 1195/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4445 - acc: 0.7708 - val_loss: 0.5070 - val_acc: 0.7656\n",
      "Epoch 1196/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4445 - acc: 0.7708 - val_loss: 0.5070 - val_acc: 0.7656\n",
      "Epoch 1197/1500\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.4445 - acc: 0.7708 - val_loss: 0.5070 - val_acc: 0.7656\n",
      "Epoch 1198/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4445 - acc: 0.7708 - val_loss: 0.5070 - val_acc: 0.7656\n",
      "Epoch 1199/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4445 - acc: 0.7708 - val_loss: 0.5070 - val_acc: 0.7656\n",
      "Epoch 1200/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4445 - acc: 0.7708 - val_loss: 0.5070 - val_acc: 0.7656\n",
      "Epoch 1201/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4445 - acc: 0.7708 - val_loss: 0.5070 - val_acc: 0.7656\n",
      "Epoch 1202/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4445 - acc: 0.7708 - val_loss: 0.5070 - val_acc: 0.7656\n",
      "Epoch 1203/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4445 - acc: 0.7708 - val_loss: 0.5069 - val_acc: 0.7656\n",
      "Epoch 1204/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4444 - acc: 0.7708 - val_loss: 0.5069 - val_acc: 0.7656\n",
      "Epoch 1205/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4445 - acc: 0.7708 - val_loss: 0.5069 - val_acc: 0.7656\n",
      "Epoch 1206/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4445 - acc: 0.7708 - val_loss: 0.5069 - val_acc: 0.7656\n",
      "Epoch 1207/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4444 - acc: 0.7708 - val_loss: 0.5069 - val_acc: 0.7656\n",
      "Epoch 1208/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4444 - acc: 0.7708 - val_loss: 0.5069 - val_acc: 0.7656\n",
      "Epoch 1209/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4444 - acc: 0.7708 - val_loss: 0.5069 - val_acc: 0.7656\n",
      "Epoch 1210/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4444 - acc: 0.7708 - val_loss: 0.5069 - val_acc: 0.7656\n",
      "Epoch 1211/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4444 - acc: 0.7708 - val_loss: 0.5069 - val_acc: 0.7656\n",
      "Epoch 1212/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4444 - acc: 0.7708 - val_loss: 0.5069 - val_acc: 0.7656\n",
      "Epoch 1213/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4444 - acc: 0.7708 - val_loss: 0.5069 - val_acc: 0.7656\n",
      "Epoch 1214/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4444 - acc: 0.7708 - val_loss: 0.5069 - val_acc: 0.7656\n",
      "Epoch 1215/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4444 - acc: 0.7708 - val_loss: 0.5069 - val_acc: 0.7656\n",
      "Epoch 1216/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4444 - acc: 0.7708 - val_loss: 0.5069 - val_acc: 0.7656\n",
      "Epoch 1217/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4444 - acc: 0.7708 - val_loss: 0.5068 - val_acc: 0.7656\n",
      "Epoch 1218/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4444 - acc: 0.7708 - val_loss: 0.5068 - val_acc: 0.7656\n",
      "Epoch 1219/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4444 - acc: 0.7708 - val_loss: 0.5068 - val_acc: 0.7656\n",
      "Epoch 1220/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4444 - acc: 0.7708 - val_loss: 0.5068 - val_acc: 0.7656\n",
      "Epoch 1221/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4444 - acc: 0.7708 - val_loss: 0.5068 - val_acc: 0.7656\n",
      "Epoch 1222/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4444 - acc: 0.7708 - val_loss: 0.5068 - val_acc: 0.7656\n",
      "Epoch 1223/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4443 - acc: 0.7708 - val_loss: 0.5068 - val_acc: 0.7656\n",
      "Epoch 1224/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4444 - acc: 0.7708 - val_loss: 0.5068 - val_acc: 0.7656\n",
      "Epoch 1225/1500\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4443 - acc: 0.7708 - val_loss: 0.5068 - val_acc: 0.7656\n",
      "Epoch 1226/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4443 - acc: 0.7708 - val_loss: 0.5068 - val_acc: 0.7656\n",
      "Epoch 1227/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4443 - acc: 0.7708 - val_loss: 0.5068 - val_acc: 0.7656\n",
      "Epoch 1228/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4443 - acc: 0.7708 - val_loss: 0.5068 - val_acc: 0.7656\n",
      "Epoch 1229/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4443 - acc: 0.7708 - val_loss: 0.5068 - val_acc: 0.7656\n",
      "Epoch 1230/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4443 - acc: 0.7708 - val_loss: 0.5068 - val_acc: 0.7656\n",
      "Epoch 1231/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4443 - acc: 0.7708 - val_loss: 0.5067 - val_acc: 0.7656\n",
      "Epoch 1232/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4443 - acc: 0.7708 - val_loss: 0.5067 - val_acc: 0.7656\n",
      "Epoch 1233/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4443 - acc: 0.7708 - val_loss: 0.5067 - val_acc: 0.7656\n",
      "Epoch 1234/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4443 - acc: 0.7708 - val_loss: 0.5067 - val_acc: 0.7656\n",
      "Epoch 1235/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4443 - acc: 0.7708 - val_loss: 0.5067 - val_acc: 0.7656\n",
      "Epoch 1236/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4443 - acc: 0.7708 - val_loss: 0.5067 - val_acc: 0.7656\n",
      "Epoch 1237/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4443 - acc: 0.7708 - val_loss: 0.5067 - val_acc: 0.7656\n",
      "Epoch 1238/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4443 - acc: 0.7708 - val_loss: 0.5067 - val_acc: 0.7656\n",
      "Epoch 1239/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4443 - acc: 0.7708 - val_loss: 0.5067 - val_acc: 0.7656\n",
      "Epoch 1240/1500\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4442 - acc: 0.7708 - val_loss: 0.5067 - val_acc: 0.7656\n",
      "Epoch 1241/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4442 - acc: 0.7708 - val_loss: 0.5067 - val_acc: 0.7656\n",
      "Epoch 1242/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4442 - acc: 0.7708 - val_loss: 0.5066 - val_acc: 0.7656\n",
      "Epoch 1243/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4442 - acc: 0.7708 - val_loss: 0.5066 - val_acc: 0.7656\n",
      "Epoch 1244/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4443 - acc: 0.7708 - val_loss: 0.5066 - val_acc: 0.7656\n",
      "Epoch 1245/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4442 - acc: 0.7708 - val_loss: 0.5066 - val_acc: 0.7656\n",
      "Epoch 1246/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4442 - acc: 0.7708 - val_loss: 0.5066 - val_acc: 0.7656\n",
      "Epoch 1247/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4442 - acc: 0.7708 - val_loss: 0.5066 - val_acc: 0.7656\n",
      "Epoch 1248/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4442 - acc: 0.7708 - val_loss: 0.5066 - val_acc: 0.7656\n",
      "Epoch 1249/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4442 - acc: 0.7708 - val_loss: 0.5066 - val_acc: 0.7656\n",
      "Epoch 1250/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4442 - acc: 0.7726 - val_loss: 0.5066 - val_acc: 0.7656\n",
      "Epoch 1251/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4442 - acc: 0.7708 - val_loss: 0.5066 - val_acc: 0.7656\n",
      "Epoch 1252/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4442 - acc: 0.7708 - val_loss: 0.5066 - val_acc: 0.7656\n",
      "Epoch 1253/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 58us/step - loss: 0.4442 - acc: 0.7708 - val_loss: 0.5066 - val_acc: 0.7656\n",
      "Epoch 1254/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4442 - acc: 0.7708 - val_loss: 0.5066 - val_acc: 0.7656\n",
      "Epoch 1255/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4442 - acc: 0.7708 - val_loss: 0.5065 - val_acc: 0.7656\n",
      "Epoch 1256/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4442 - acc: 0.7708 - val_loss: 0.5065 - val_acc: 0.7656\n",
      "Epoch 1257/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4441 - acc: 0.7708 - val_loss: 0.5065 - val_acc: 0.7656\n",
      "Epoch 1258/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4442 - acc: 0.7691 - val_loss: 0.5065 - val_acc: 0.7656\n",
      "Epoch 1259/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4442 - acc: 0.7708 - val_loss: 0.5065 - val_acc: 0.7656\n",
      "Epoch 1260/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4442 - acc: 0.7691 - val_loss: 0.5065 - val_acc: 0.7656\n",
      "Epoch 1261/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4442 - acc: 0.7708 - val_loss: 0.5065 - val_acc: 0.7656\n",
      "Epoch 1262/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4442 - acc: 0.7691 - val_loss: 0.5065 - val_acc: 0.7656\n",
      "Epoch 1263/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4442 - acc: 0.7691 - val_loss: 0.5065 - val_acc: 0.7656\n",
      "Epoch 1264/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4441 - acc: 0.7708 - val_loss: 0.5065 - val_acc: 0.7656\n",
      "Epoch 1265/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4441 - acc: 0.7691 - val_loss: 0.5065 - val_acc: 0.7656\n",
      "Epoch 1266/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4441 - acc: 0.7691 - val_loss: 0.5065 - val_acc: 0.7656\n",
      "Epoch 1267/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4441 - acc: 0.7691 - val_loss: 0.5064 - val_acc: 0.7656\n",
      "Epoch 1268/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4441 - acc: 0.7691 - val_loss: 0.5064 - val_acc: 0.7656\n",
      "Epoch 1269/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4441 - acc: 0.7691 - val_loss: 0.5064 - val_acc: 0.7656\n",
      "Epoch 1270/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4441 - acc: 0.7691 - val_loss: 0.5064 - val_acc: 0.7656\n",
      "Epoch 1271/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4441 - acc: 0.7691 - val_loss: 0.5064 - val_acc: 0.7656\n",
      "Epoch 1272/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4441 - acc: 0.7691 - val_loss: 0.5064 - val_acc: 0.7656\n",
      "Epoch 1273/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4441 - acc: 0.7691 - val_loss: 0.5064 - val_acc: 0.7656\n",
      "Epoch 1274/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4441 - acc: 0.7691 - val_loss: 0.5064 - val_acc: 0.7656\n",
      "Epoch 1275/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4441 - acc: 0.7691 - val_loss: 0.5064 - val_acc: 0.7656\n",
      "Epoch 1276/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4441 - acc: 0.7691 - val_loss: 0.5064 - val_acc: 0.7656\n",
      "Epoch 1277/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4441 - acc: 0.7691 - val_loss: 0.5064 - val_acc: 0.7656\n",
      "Epoch 1278/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4441 - acc: 0.7691 - val_loss: 0.5063 - val_acc: 0.7656\n",
      "Epoch 1279/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4440 - acc: 0.7691 - val_loss: 0.5063 - val_acc: 0.7656\n",
      "Epoch 1280/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4441 - acc: 0.7691 - val_loss: 0.5063 - val_acc: 0.7656\n",
      "Epoch 1281/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4440 - acc: 0.7691 - val_loss: 0.5063 - val_acc: 0.7656\n",
      "Epoch 1282/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4440 - acc: 0.7691 - val_loss: 0.5063 - val_acc: 0.7656\n",
      "Epoch 1283/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4440 - acc: 0.7691 - val_loss: 0.5063 - val_acc: 0.7656\n",
      "Epoch 1284/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4441 - acc: 0.7691 - val_loss: 0.5063 - val_acc: 0.7656\n",
      "Epoch 1285/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4440 - acc: 0.7691 - val_loss: 0.5063 - val_acc: 0.7656\n",
      "Epoch 1286/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4440 - acc: 0.7691 - val_loss: 0.5063 - val_acc: 0.7656\n",
      "Epoch 1287/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4440 - acc: 0.7691 - val_loss: 0.5063 - val_acc: 0.7656\n",
      "Epoch 1288/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4440 - acc: 0.7691 - val_loss: 0.5063 - val_acc: 0.7656\n",
      "Epoch 1289/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4440 - acc: 0.7691 - val_loss: 0.5063 - val_acc: 0.7656\n",
      "Epoch 1290/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4440 - acc: 0.7691 - val_loss: 0.5063 - val_acc: 0.7656\n",
      "Epoch 1291/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4440 - acc: 0.7691 - val_loss: 0.5062 - val_acc: 0.7656\n",
      "Epoch 1292/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4440 - acc: 0.7691 - val_loss: 0.5062 - val_acc: 0.7656\n",
      "Epoch 1293/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4440 - acc: 0.7691 - val_loss: 0.5062 - val_acc: 0.7656\n",
      "Epoch 1294/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4440 - acc: 0.7691 - val_loss: 0.5062 - val_acc: 0.7656\n",
      "Epoch 1295/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4440 - acc: 0.7691 - val_loss: 0.5062 - val_acc: 0.7656\n",
      "Epoch 1296/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4440 - acc: 0.7691 - val_loss: 0.5062 - val_acc: 0.7656\n",
      "Epoch 1297/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4440 - acc: 0.7691 - val_loss: 0.5062 - val_acc: 0.7656\n",
      "Epoch 1298/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4440 - acc: 0.7691 - val_loss: 0.5062 - val_acc: 0.7656\n",
      "Epoch 1299/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4440 - acc: 0.7708 - val_loss: 0.5062 - val_acc: 0.7656\n",
      "Epoch 1300/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4440 - acc: 0.7691 - val_loss: 0.5062 - val_acc: 0.7656\n",
      "Epoch 1301/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4439 - acc: 0.7691 - val_loss: 0.5062 - val_acc: 0.7656\n",
      "Epoch 1302/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4440 - acc: 0.7691 - val_loss: 0.5062 - val_acc: 0.7656\n",
      "Epoch 1303/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4440 - acc: 0.7691 - val_loss: 0.5062 - val_acc: 0.7656\n",
      "Epoch 1304/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4439 - acc: 0.7691 - val_loss: 0.5062 - val_acc: 0.7656\n",
      "Epoch 1305/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4440 - acc: 0.7691 - val_loss: 0.5061 - val_acc: 0.7656\n",
      "Epoch 1306/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4440 - acc: 0.7708 - val_loss: 0.5061 - val_acc: 0.7656\n",
      "Epoch 1307/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4440 - acc: 0.7708 - val_loss: 0.5061 - val_acc: 0.7656\n",
      "Epoch 1308/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4439 - acc: 0.7691 - val_loss: 0.5061 - val_acc: 0.7656\n",
      "Epoch 1309/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4439 - acc: 0.7691 - val_loss: 0.5061 - val_acc: 0.7656\n",
      "Epoch 1310/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4439 - acc: 0.7708 - val_loss: 0.5061 - val_acc: 0.7656\n",
      "Epoch 1311/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4439 - acc: 0.7691 - val_loss: 0.5061 - val_acc: 0.7656\n",
      "Epoch 1312/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 66us/step - loss: 0.4439 - acc: 0.7708 - val_loss: 0.5061 - val_acc: 0.7656\n",
      "Epoch 1313/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4439 - acc: 0.7708 - val_loss: 0.5061 - val_acc: 0.7656\n",
      "Epoch 1314/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4439 - acc: 0.7708 - val_loss: 0.5061 - val_acc: 0.7656\n",
      "Epoch 1315/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4439 - acc: 0.7691 - val_loss: 0.5061 - val_acc: 0.7656\n",
      "Epoch 1316/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4439 - acc: 0.7708 - val_loss: 0.5060 - val_acc: 0.7656\n",
      "Epoch 1317/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4439 - acc: 0.7708 - val_loss: 0.5060 - val_acc: 0.7656\n",
      "Epoch 1318/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4439 - acc: 0.7708 - val_loss: 0.5060 - val_acc: 0.7656\n",
      "Epoch 1319/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4439 - acc: 0.7708 - val_loss: 0.5060 - val_acc: 0.7656\n",
      "Epoch 1320/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4439 - acc: 0.7708 - val_loss: 0.5060 - val_acc: 0.7656\n",
      "Epoch 1321/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4439 - acc: 0.7708 - val_loss: 0.5060 - val_acc: 0.7656\n",
      "Epoch 1322/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4439 - acc: 0.7708 - val_loss: 0.5060 - val_acc: 0.7656\n",
      "Epoch 1323/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4439 - acc: 0.7708 - val_loss: 0.5060 - val_acc: 0.7656\n",
      "Epoch 1324/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4439 - acc: 0.7708 - val_loss: 0.5060 - val_acc: 0.7656\n",
      "Epoch 1325/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4438 - acc: 0.7708 - val_loss: 0.5060 - val_acc: 0.7656\n",
      "Epoch 1326/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4439 - acc: 0.7708 - val_loss: 0.5059 - val_acc: 0.7656\n",
      "Epoch 1327/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4438 - acc: 0.7708 - val_loss: 0.5059 - val_acc: 0.7656\n",
      "Epoch 1328/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4438 - acc: 0.7708 - val_loss: 0.5059 - val_acc: 0.7656\n",
      "Epoch 1329/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4438 - acc: 0.7708 - val_loss: 0.5059 - val_acc: 0.7656\n",
      "Epoch 1330/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4438 - acc: 0.7708 - val_loss: 0.5059 - val_acc: 0.7656\n",
      "Epoch 1331/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4438 - acc: 0.7708 - val_loss: 0.5059 - val_acc: 0.7656\n",
      "Epoch 1332/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4438 - acc: 0.7708 - val_loss: 0.5059 - val_acc: 0.7656\n",
      "Epoch 1333/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4438 - acc: 0.7708 - val_loss: 0.5059 - val_acc: 0.7656\n",
      "Epoch 1334/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4438 - acc: 0.7726 - val_loss: 0.5059 - val_acc: 0.7656\n",
      "Epoch 1335/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4438 - acc: 0.7708 - val_loss: 0.5059 - val_acc: 0.7656\n",
      "Epoch 1336/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4438 - acc: 0.7708 - val_loss: 0.5059 - val_acc: 0.7656\n",
      "Epoch 1337/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4438 - acc: 0.7708 - val_loss: 0.5059 - val_acc: 0.7656\n",
      "Epoch 1338/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4438 - acc: 0.7708 - val_loss: 0.5058 - val_acc: 0.7656\n",
      "Epoch 1339/1500\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.4438 - acc: 0.7708 - val_loss: 0.5058 - val_acc: 0.7656\n",
      "Epoch 1340/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4438 - acc: 0.7708 - val_loss: 0.5058 - val_acc: 0.7656\n",
      "Epoch 1341/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4437 - acc: 0.7708 - val_loss: 0.5058 - val_acc: 0.7656\n",
      "Epoch 1342/1500\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4438 - acc: 0.7708 - val_loss: 0.5058 - val_acc: 0.7656\n",
      "Epoch 1343/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4438 - acc: 0.7708 - val_loss: 0.5058 - val_acc: 0.7656\n",
      "Epoch 1344/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4437 - acc: 0.7708 - val_loss: 0.5058 - val_acc: 0.7656\n",
      "Epoch 1345/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4437 - acc: 0.7708 - val_loss: 0.5058 - val_acc: 0.7656\n",
      "Epoch 1346/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4438 - acc: 0.7708 - val_loss: 0.5058 - val_acc: 0.7656\n",
      "Epoch 1347/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4437 - acc: 0.7708 - val_loss: 0.5058 - val_acc: 0.7656\n",
      "Epoch 1348/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4437 - acc: 0.7708 - val_loss: 0.5058 - val_acc: 0.7656\n",
      "Epoch 1349/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4437 - acc: 0.7708 - val_loss: 0.5058 - val_acc: 0.7656\n",
      "Epoch 1350/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4437 - acc: 0.7708 - val_loss: 0.5058 - val_acc: 0.7656\n",
      "Epoch 1351/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4437 - acc: 0.7708 - val_loss: 0.5058 - val_acc: 0.7656\n",
      "Epoch 1352/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4437 - acc: 0.7708 - val_loss: 0.5058 - val_acc: 0.7656\n",
      "Epoch 1353/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4437 - acc: 0.7708 - val_loss: 0.5058 - val_acc: 0.7656\n",
      "Epoch 1354/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4437 - acc: 0.7708 - val_loss: 0.5058 - val_acc: 0.7656\n",
      "Epoch 1355/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4437 - acc: 0.7708 - val_loss: 0.5057 - val_acc: 0.7656\n",
      "Epoch 1356/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4437 - acc: 0.7708 - val_loss: 0.5057 - val_acc: 0.7656\n",
      "Epoch 1357/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4437 - acc: 0.7708 - val_loss: 0.5057 - val_acc: 0.7656\n",
      "Epoch 1358/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4437 - acc: 0.7708 - val_loss: 0.5057 - val_acc: 0.7656\n",
      "Epoch 1359/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4437 - acc: 0.7708 - val_loss: 0.5057 - val_acc: 0.7656\n",
      "Epoch 1360/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4437 - acc: 0.7708 - val_loss: 0.5057 - val_acc: 0.7656\n",
      "Epoch 1361/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4437 - acc: 0.7708 - val_loss: 0.5057 - val_acc: 0.7656\n",
      "Epoch 1362/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4436 - acc: 0.7708 - val_loss: 0.5057 - val_acc: 0.7656\n",
      "Epoch 1363/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4437 - acc: 0.7726 - val_loss: 0.5057 - val_acc: 0.7656\n",
      "Epoch 1364/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4436 - acc: 0.7708 - val_loss: 0.5057 - val_acc: 0.7656\n",
      "Epoch 1365/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4436 - acc: 0.7726 - val_loss: 0.5057 - val_acc: 0.7656\n",
      "Epoch 1366/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4437 - acc: 0.7708 - val_loss: 0.5057 - val_acc: 0.7656\n",
      "Epoch 1367/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4436 - acc: 0.7708 - val_loss: 0.5057 - val_acc: 0.7656\n",
      "Epoch 1368/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4436 - acc: 0.7708 - val_loss: 0.5057 - val_acc: 0.7656\n",
      "Epoch 1369/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4436 - acc: 0.7708 - val_loss: 0.5057 - val_acc: 0.7656\n",
      "Epoch 1370/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4436 - acc: 0.7726 - val_loss: 0.5057 - val_acc: 0.7656\n",
      "Epoch 1371/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 61us/step - loss: 0.4436 - acc: 0.7708 - val_loss: 0.5057 - val_acc: 0.7656\n",
      "Epoch 1372/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4436 - acc: 0.7708 - val_loss: 0.5056 - val_acc: 0.7656\n",
      "Epoch 1373/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4437 - acc: 0.7708 - val_loss: 0.5056 - val_acc: 0.7656\n",
      "Epoch 1374/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4436 - acc: 0.7708 - val_loss: 0.5056 - val_acc: 0.7656\n",
      "Epoch 1375/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4436 - acc: 0.7708 - val_loss: 0.5056 - val_acc: 0.7656\n",
      "Epoch 1376/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4436 - acc: 0.7708 - val_loss: 0.5056 - val_acc: 0.7656\n",
      "Epoch 1377/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4436 - acc: 0.7708 - val_loss: 0.5056 - val_acc: 0.7656\n",
      "Epoch 1378/1500\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4436 - acc: 0.7708 - val_loss: 0.5056 - val_acc: 0.7656\n",
      "Epoch 1379/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4436 - acc: 0.7708 - val_loss: 0.5056 - val_acc: 0.7656\n",
      "Epoch 1380/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4435 - acc: 0.7708 - val_loss: 0.5056 - val_acc: 0.7656\n",
      "Epoch 1381/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4435 - acc: 0.7708 - val_loss: 0.5056 - val_acc: 0.7656\n",
      "Epoch 1382/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4435 - acc: 0.7708 - val_loss: 0.5056 - val_acc: 0.7656\n",
      "Epoch 1383/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4435 - acc: 0.7708 - val_loss: 0.5056 - val_acc: 0.7656\n",
      "Epoch 1384/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4435 - acc: 0.7708 - val_loss: 0.5056 - val_acc: 0.7656\n",
      "Epoch 1385/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4435 - acc: 0.7708 - val_loss: 0.5056 - val_acc: 0.7656\n",
      "Epoch 1386/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4435 - acc: 0.7726 - val_loss: 0.5056 - val_acc: 0.7656\n",
      "Epoch 1387/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4436 - acc: 0.7726 - val_loss: 0.5056 - val_acc: 0.7656\n",
      "Epoch 1388/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4435 - acc: 0.7708 - val_loss: 0.5056 - val_acc: 0.7656\n",
      "Epoch 1389/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4435 - acc: 0.7708 - val_loss: 0.5056 - val_acc: 0.7656\n",
      "Epoch 1390/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4435 - acc: 0.7708 - val_loss: 0.5056 - val_acc: 0.7656\n",
      "Epoch 1391/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4435 - acc: 0.7726 - val_loss: 0.5056 - val_acc: 0.7656\n",
      "Epoch 1392/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4435 - acc: 0.7708 - val_loss: 0.5056 - val_acc: 0.7656\n",
      "Epoch 1393/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4435 - acc: 0.7726 - val_loss: 0.5055 - val_acc: 0.7656\n",
      "Epoch 1394/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4435 - acc: 0.7708 - val_loss: 0.5055 - val_acc: 0.7656\n",
      "Epoch 1395/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4435 - acc: 0.7691 - val_loss: 0.5055 - val_acc: 0.7656\n",
      "Epoch 1396/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4435 - acc: 0.7691 - val_loss: 0.5055 - val_acc: 0.7656\n",
      "Epoch 1397/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4435 - acc: 0.7726 - val_loss: 0.5055 - val_acc: 0.7656\n",
      "Epoch 1398/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4434 - acc: 0.7708 - val_loss: 0.5055 - val_acc: 0.7656\n",
      "Epoch 1399/1500\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.4435 - acc: 0.7691 - val_loss: 0.5055 - val_acc: 0.7656\n",
      "Epoch 1400/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4435 - acc: 0.7708 - val_loss: 0.5055 - val_acc: 0.7656\n",
      "Epoch 1401/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4434 - acc: 0.7708 - val_loss: 0.5055 - val_acc: 0.7656\n",
      "Epoch 1402/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4435 - acc: 0.7691 - val_loss: 0.5054 - val_acc: 0.7656\n",
      "Epoch 1403/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4434 - acc: 0.7708 - val_loss: 0.5054 - val_acc: 0.7656\n",
      "Epoch 1404/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4434 - acc: 0.7708 - val_loss: 0.5054 - val_acc: 0.7656\n",
      "Epoch 1405/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4434 - acc: 0.7726 - val_loss: 0.5054 - val_acc: 0.7656\n",
      "Epoch 1406/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4434 - acc: 0.7691 - val_loss: 0.5054 - val_acc: 0.7656\n",
      "Epoch 1407/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4434 - acc: 0.7708 - val_loss: 0.5054 - val_acc: 0.7656\n",
      "Epoch 1408/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4434 - acc: 0.7691 - val_loss: 0.5054 - val_acc: 0.7656\n",
      "Epoch 1409/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4434 - acc: 0.7708 - val_loss: 0.5054 - val_acc: 0.7656\n",
      "Epoch 1410/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4434 - acc: 0.7708 - val_loss: 0.5054 - val_acc: 0.7656\n",
      "Epoch 1411/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4434 - acc: 0.7708 - val_loss: 0.5054 - val_acc: 0.7656\n",
      "Epoch 1412/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4433 - acc: 0.7691 - val_loss: 0.5053 - val_acc: 0.7656\n",
      "Epoch 1413/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4434 - acc: 0.7691 - val_loss: 0.5053 - val_acc: 0.7656\n",
      "Epoch 1414/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4434 - acc: 0.7691 - val_loss: 0.5053 - val_acc: 0.7656\n",
      "Epoch 1415/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4433 - acc: 0.7708 - val_loss: 0.5053 - val_acc: 0.7656\n",
      "Epoch 1416/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4434 - acc: 0.7691 - val_loss: 0.5053 - val_acc: 0.7656\n",
      "Epoch 1417/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4434 - acc: 0.7708 - val_loss: 0.5053 - val_acc: 0.7656\n",
      "Epoch 1418/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4433 - acc: 0.7708 - val_loss: 0.5053 - val_acc: 0.7656\n",
      "Epoch 1419/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4433 - acc: 0.7708 - val_loss: 0.5053 - val_acc: 0.7656\n",
      "Epoch 1420/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4433 - acc: 0.7691 - val_loss: 0.5053 - val_acc: 0.7656\n",
      "Epoch 1421/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4433 - acc: 0.7708 - val_loss: 0.5053 - val_acc: 0.7656\n",
      "Epoch 1422/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4433 - acc: 0.7691 - val_loss: 0.5053 - val_acc: 0.7656\n",
      "Epoch 1423/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4433 - acc: 0.7708 - val_loss: 0.5053 - val_acc: 0.7656\n",
      "Epoch 1424/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4433 - acc: 0.7708 - val_loss: 0.5052 - val_acc: 0.7656\n",
      "Epoch 1425/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4433 - acc: 0.7708 - val_loss: 0.5052 - val_acc: 0.7656\n",
      "Epoch 1426/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4433 - acc: 0.7708 - val_loss: 0.5052 - val_acc: 0.7656\n",
      "Epoch 1427/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4433 - acc: 0.7691 - val_loss: 0.5052 - val_acc: 0.7656\n",
      "Epoch 1428/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4433 - acc: 0.7691 - val_loss: 0.5052 - val_acc: 0.7656\n",
      "Epoch 1429/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4433 - acc: 0.7691 - val_loss: 0.5052 - val_acc: 0.7656\n",
      "Epoch 1430/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 57us/step - loss: 0.4432 - acc: 0.7708 - val_loss: 0.5052 - val_acc: 0.7656\n",
      "Epoch 1431/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4433 - acc: 0.7708 - val_loss: 0.5052 - val_acc: 0.7656\n",
      "Epoch 1432/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4433 - acc: 0.7708 - val_loss: 0.5052 - val_acc: 0.7656\n",
      "Epoch 1433/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4432 - acc: 0.7708 - val_loss: 0.5052 - val_acc: 0.7656\n",
      "Epoch 1434/1500\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4432 - acc: 0.7708 - val_loss: 0.5052 - val_acc: 0.7656\n",
      "Epoch 1435/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4432 - acc: 0.7691 - val_loss: 0.5052 - val_acc: 0.7656\n",
      "Epoch 1436/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4433 - acc: 0.7691 - val_loss: 0.5052 - val_acc: 0.7656\n",
      "Epoch 1437/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4432 - acc: 0.7691 - val_loss: 0.5052 - val_acc: 0.7656\n",
      "Epoch 1438/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4432 - acc: 0.7691 - val_loss: 0.5052 - val_acc: 0.7656\n",
      "Epoch 1439/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4432 - acc: 0.7691 - val_loss: 0.5052 - val_acc: 0.7656\n",
      "Epoch 1440/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4432 - acc: 0.7691 - val_loss: 0.5051 - val_acc: 0.7656\n",
      "Epoch 1441/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4432 - acc: 0.7691 - val_loss: 0.5051 - val_acc: 0.7656\n",
      "Epoch 1442/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4432 - acc: 0.7708 - val_loss: 0.5051 - val_acc: 0.7656\n",
      "Epoch 1443/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4432 - acc: 0.7708 - val_loss: 0.5051 - val_acc: 0.7656\n",
      "Epoch 1444/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4432 - acc: 0.7691 - val_loss: 0.5051 - val_acc: 0.7656\n",
      "Epoch 1445/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4432 - acc: 0.7708 - val_loss: 0.5051 - val_acc: 0.7656\n",
      "Epoch 1446/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4432 - acc: 0.7708 - val_loss: 0.5051 - val_acc: 0.7656\n",
      "Epoch 1447/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4431 - acc: 0.7691 - val_loss: 0.5051 - val_acc: 0.7656\n",
      "Epoch 1448/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4431 - acc: 0.7708 - val_loss: 0.5051 - val_acc: 0.7656\n",
      "Epoch 1449/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4432 - acc: 0.7708 - val_loss: 0.5051 - val_acc: 0.7656\n",
      "Epoch 1450/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4432 - acc: 0.7708 - val_loss: 0.5051 - val_acc: 0.7656\n",
      "Epoch 1451/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4431 - acc: 0.7708 - val_loss: 0.5051 - val_acc: 0.7656\n",
      "Epoch 1452/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4432 - acc: 0.7691 - val_loss: 0.5051 - val_acc: 0.7656\n",
      "Epoch 1453/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4431 - acc: 0.7708 - val_loss: 0.5051 - val_acc: 0.7656\n",
      "Epoch 1454/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4431 - acc: 0.7691 - val_loss: 0.5051 - val_acc: 0.7656\n",
      "Epoch 1455/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4431 - acc: 0.7691 - val_loss: 0.5051 - val_acc: 0.7656\n",
      "Epoch 1456/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4431 - acc: 0.7691 - val_loss: 0.5051 - val_acc: 0.7656\n",
      "Epoch 1457/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4431 - acc: 0.7708 - val_loss: 0.5050 - val_acc: 0.7656\n",
      "Epoch 1458/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4431 - acc: 0.7708 - val_loss: 0.5050 - val_acc: 0.7656\n",
      "Epoch 1459/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4431 - acc: 0.7708 - val_loss: 0.5050 - val_acc: 0.7656\n",
      "Epoch 1460/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4431 - acc: 0.7691 - val_loss: 0.5050 - val_acc: 0.7656\n",
      "Epoch 1461/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4431 - acc: 0.7708 - val_loss: 0.5050 - val_acc: 0.7656\n",
      "Epoch 1462/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4431 - acc: 0.7691 - val_loss: 0.5050 - val_acc: 0.7656\n",
      "Epoch 1463/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4431 - acc: 0.7691 - val_loss: 0.5050 - val_acc: 0.7656\n",
      "Epoch 1464/1500\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.4431 - acc: 0.7708 - val_loss: 0.5050 - val_acc: 0.7656\n",
      "Epoch 1465/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4431 - acc: 0.7691 - val_loss: 0.5050 - val_acc: 0.7656\n",
      "Epoch 1466/1500\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4431 - acc: 0.7691 - val_loss: 0.5050 - val_acc: 0.7656\n",
      "Epoch 1467/1500\n",
      "576/576 [==============================] - 0s 48us/step - loss: 0.4431 - acc: 0.7726 - val_loss: 0.5050 - val_acc: 0.7656\n",
      "Epoch 1468/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4430 - acc: 0.7708 - val_loss: 0.5050 - val_acc: 0.7656\n",
      "Epoch 1469/1500\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.4430 - acc: 0.7726 - val_loss: 0.5050 - val_acc: 0.7656\n",
      "Epoch 1470/1500\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.4430 - acc: 0.7726 - val_loss: 0.5049 - val_acc: 0.7656\n",
      "Epoch 1471/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4430 - acc: 0.7708 - val_loss: 0.5049 - val_acc: 0.7656\n",
      "Epoch 1472/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4430 - acc: 0.7708 - val_loss: 0.5049 - val_acc: 0.7656\n",
      "Epoch 1473/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4430 - acc: 0.7726 - val_loss: 0.5049 - val_acc: 0.7656\n",
      "Epoch 1474/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4430 - acc: 0.7726 - val_loss: 0.5049 - val_acc: 0.7656\n",
      "Epoch 1475/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4430 - acc: 0.7708 - val_loss: 0.5049 - val_acc: 0.7656\n",
      "Epoch 1476/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4430 - acc: 0.7708 - val_loss: 0.5049 - val_acc: 0.7656\n",
      "Epoch 1477/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4430 - acc: 0.7726 - val_loss: 0.5049 - val_acc: 0.7656\n",
      "Epoch 1478/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4430 - acc: 0.7708 - val_loss: 0.5049 - val_acc: 0.7656\n",
      "Epoch 1479/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4430 - acc: 0.7726 - val_loss: 0.5049 - val_acc: 0.7656\n",
      "Epoch 1480/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4430 - acc: 0.7708 - val_loss: 0.5049 - val_acc: 0.7656\n",
      "Epoch 1481/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4430 - acc: 0.7708 - val_loss: 0.5048 - val_acc: 0.7656\n",
      "Epoch 1482/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4429 - acc: 0.7726 - val_loss: 0.5048 - val_acc: 0.7656\n",
      "Epoch 1483/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4429 - acc: 0.7726 - val_loss: 0.5048 - val_acc: 0.7656\n",
      "Epoch 1484/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4429 - acc: 0.7726 - val_loss: 0.5048 - val_acc: 0.7656\n",
      "Epoch 1485/1500\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4429 - acc: 0.7708 - val_loss: 0.5048 - val_acc: 0.7656\n",
      "Epoch 1486/1500\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.4429 - acc: 0.7726 - val_loss: 0.5048 - val_acc: 0.7656\n",
      "Epoch 1487/1500\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4429 - acc: 0.7726 - val_loss: 0.5048 - val_acc: 0.7656\n",
      "Epoch 1488/1500\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4429 - acc: 0.7726 - val_loss: 0.5048 - val_acc: 0.7656\n",
      "Epoch 1489/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 62us/step - loss: 0.4429 - acc: 0.7726 - val_loss: 0.5048 - val_acc: 0.7656\n",
      "Epoch 1490/1500\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4429 - acc: 0.7708 - val_loss: 0.5048 - val_acc: 0.7656\n",
      "Epoch 1491/1500\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.4429 - acc: 0.7726 - val_loss: 0.5048 - val_acc: 0.7656\n",
      "Epoch 1492/1500\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4429 - acc: 0.7726 - val_loss: 0.5047 - val_acc: 0.7656\n",
      "Epoch 1493/1500\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4429 - acc: 0.7708 - val_loss: 0.5047 - val_acc: 0.7656\n",
      "Epoch 1494/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4429 - acc: 0.7726 - val_loss: 0.5047 - val_acc: 0.7656\n",
      "Epoch 1495/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4428 - acc: 0.7708 - val_loss: 0.5047 - val_acc: 0.7656\n",
      "Epoch 1496/1500\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.4429 - acc: 0.7708 - val_loss: 0.5047 - val_acc: 0.7656\n",
      "Epoch 1497/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4428 - acc: 0.7708 - val_loss: 0.5047 - val_acc: 0.7656\n",
      "Epoch 1498/1500\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4428 - acc: 0.7708 - val_loss: 0.5047 - val_acc: 0.7656\n",
      "Epoch 1499/1500\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.4428 - acc: 0.7726 - val_loss: 0.5047 - val_acc: 0.7656\n",
      "Epoch 1500/1500\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4428 - acc: 0.7726 - val_loss: 0.5047 - val_acc: 0.7656\n"
     ]
    }
   ],
   "source": [
    "# Type your code here to with layer 1,2 having activation relu and layer 3 with activation sigmoid\n",
    "\n",
    "# Define the Model \n",
    "# Input size is 8-dimensional\n",
    "# 1 hidden layer, 12 hidden nodes, sigmoid activation\n",
    "# Final layer has just one node with a sigmoid activation (standard for binary classification)\n",
    "\n",
    "#model_1 = Sequential([\n",
    "#    Dense(12, input_shape=(8,), activation=\"relu\"),\n",
    "#    Dense(1, activation=\"sigmoid\")\n",
    "#])\n",
    "\n",
    "model_1 = Sequential([\n",
    "    Dense(6, input_shape=(8,), activation=\"relu\"),\n",
    "    Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model_1.compile(SGD(lr = .003), \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "run_hist_1 = model_1.fit(X_train_norm, y_train, validation_data=(X_test_norm, y_test), epochs=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 0.766\n",
      "roc-auc is 0.805\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3Xt8VNW99/HPL0NC5C4BK4otKNgakEukyHhjEOv1CNZ6AaVUq421j7XoYxVsX9ZybBX1Ucqpj2JteWrhSK3WSy02fYpMkWNALiIiiFKFY8RLCIogIISs88feiZPJJJlA5rJnvu/Xa78ys2Zl55edzC8rv7322uacQ0REcktBpgMQEZH2p+QuIpKDlNxFRHKQkruISA5SchcRyUFK7iIiOSip5G5mZ5vZBjPbaGZTE7z+FTNbaGZrzCxqZn3bP1QREUmWtTbP3cxCwJvAN4AqYDkw0Tm3LqbPn4DnnHO/N7PTgSudc99OXdgiItKSZEbuI4GNzrm3nXN7gfnA+Lg+pcBC//GiBK+LiEgadUiiz5HAuzHPq4AT4/q8CnwL+BXwTaCrmZU452qa22mvXr1cv3792hatiEieW7ly5VbnXO/W+iWT3C1BW3wt5ybg12Z2BbAYeA+obbIjs3KgHODLX/4yK1asSOLLi4hIPTPbnEy/ZMoyVcBRMc/7AltiOzjntjjnLnTODQd+4rdtj9+Rc+5h59wI59yI3r1b/cMjIiIHKJnkvhwYaGb9zawImAA8G9vBzHqZWf2+pgG/a98wRUSkLVpN7s65WuA6oAJYDzzunHvdzKab2Ti/WwTYYGZvAl8CfpGieEVEJAmtToVMlREjRjjV3EVSa9++fVRVVbFnz55MhyJtVFxcTN++fSksLGzUbmYrnXMjWvv8ZE6oikhAVVVV0bVrV/r164dZorkRko2cc9TU1FBVVUX//v0PaB9afkAkh+3Zs4eSkhIl9oAxM0pKSg7qP67AJffKSrjzTu+jiLROiT2YDvbnFqiyTGUljBkDe/dCcTEsXAjhcKajEhHJPoEauUejXmJ3zvsYjWY6IhFpSU1NDcOGDWPYsGEcfvjhHHnkkQ3P9+7dm9Q+rrzySjZs2JD013zkkUeYMmXKgYacMwI1co9EoEMH2LcPCgu95yKSvUpKSli9ejUAt99+O126dOGmm25q1Mc5h3OOgoLEY805c+akPM5cFKiRezgMt97qPf7971WSEUmJNJzY2rhxI4MHD+b73/8+ZWVlvP/++5SXlzNixAgGDRrE9OnTG/qecsoprF69mtraWnr06MHUqVMZOnQo4XCYjz76KOmvOXfuXI4//ngGDx7MrX4iqa2t5dvf/nZD+6xZswC4//77KS0tZejQoUyaNKl9v/k0CdTIHbxaO8Du3ZmNQyRwpkwBfxTdrO3bYc0aqKuDggIYMgS6d2++/7BhMHPmAYWzbt065syZw0MPPQTAXXfdRc+ePamtrWXMmDFcdNFFlJaWxoW3ndGjR3PXXXdx44038rvf/Y6pU5vcYqKJqqoqfvrTn7JixQq6d+/OGWecwXPPPUfv3r3ZunUrr732GgCffPIJAHfffTebN2+mqKiooS1oAjVyr6yE22/3Hl9zjWbMiLS77du9xA7ex+1NlohqN8cccwxf//rXG54/9thjlJWVUVZWxvr161m3bl2TzznkkEM455xzADjhhBPYtGlTUl9r2bJlnH766fTq1YvCwkIuu+wyFi9ezIABA9iwYQM/+tGPqKiooLv/h2zQoEFMmjSJefPmNbmIKCgCNXKPRr16O3gfo1GVZkSSlswIu7ISxo71ZiwUFcG8eSl7k3Xu3Lnh8VtvvcWvfvUrXn75ZXr06MGkSZMSzvEuKipqeBwKhaitbbL4bELNXYlfUlLCmjVreP7555k1axZPPvkkDz/8MBUVFfzzn//kmWee4Y477mDt2rWEQqE2foeZFaiReyTi/b6Bd2JVJ1RF2lk47M0x/vd/T+tc408//ZSuXbvSrVs33n//fSoqKtp1/6NGjWLRokXU1NRQW1vL/PnzGT16NNXV1TjnuPjii/n5z3/OqlWr2L9/P1VVVZx++uncc889VFdXs2vXrnaNJx0CNXIPh+EPf4CLL4abb9aoXSQlwuG0v7nKysooLS1l8ODBHH300Zx88skHtb/f/va3PPHEEw3PV6xYwfTp04lEIjjnOP/88znvvPNYtWoVV111Fc45zIwZM2ZQW1vLZZddxo4dO6irq+OWW26ha9euB/stpl3gFg776CP40pdg3DiYOlUJXqQl69ev57jjjst0GHKAEv38kl04LFBlGYC1a72Pf/mLVxrUSVURkaYCl9yXLvU+6ipVEZHmBS65jxnjfTTzTq7qpKqISFOBS+7hMHTtCkcc4c3sUs1dRKSpwCX3ykrYuRPee8+74E41dxGRpgKX3KNRr94OqrmLiDQncMk9EvGWvADV3EWyXSQSaXJB0syZM/nBD37Q4ud16dIFgC1btnDRRRc1u+/WplPPnDmz0QVI5557brusFXP77bdz7733HvR+UilwyT0chhNOgKOO0s06RLLdxIkTmT9/fqO2+fPnM3HixKQ+/4gjjmh0MVJbxSf3BQsW0KNHjwPeX5AELrkDHHKIV5IRkfbXniv+XnTRRTz33HN8/vnnAGzatIktW7ZwyimnsHPnTsaOHUtZWRnHH388zzzzTJPP37RpE4MHDwZg9+7dTJgwgSFDhnDppZeyO2Zp2GuvvbZhueCf/exnAMyaNYstW7YwZswYxvjT7Pr168fWrVsBuO+++xg8eDCDBw9mpr/uzqZNmzjuuOP43ve+x6BBgzjzzDMbfZ3WJNrnZ599xnnnncfQoUMZPHgwf/zjHwGYOnUqpaWlDBkypMka9+0hUMsPAFQ+/Br/9eIg9jtj7FjT6F0kSZlY8bekpISRI0fyt7/9jfHjxzN//nwuvfRSzIzi4mKeeuopunXrxtatWxk1ahTjxo1r9t6hDz74IJ06dWLNmjWsWbOGsrKyhtd+8Ytf0LNnT/bv38/YsWNZs2YN119/Pffddx+LFi2iV69ejfa1cuVK5syZw7Jly3DOceKJJzJ69GgOPfRQ3nrrLR577DF+85vfcMkll/Dkk08mtaZ7c/t8++23OeKII/jrX//qH+PtbNu2jaeeeoo33ngDM0vJssLBGrlXVhL9X49T5wCMvZ87nVAVaUepWPE3tjQTW5JxznHrrbcyZMgQzjjjDN577z0+/PDDZvezePHihiQ7ZMgQhgwZ0vDa448/TllZGcOHD+f1119PuFxwrCVLlvDNb36Tzp0706VLFy688EJefPFFAPr378+wYcOAti0r3Nw+jz/+eP7xj39wyy238OKLL9K9e3e6detGcXExV199NX/+85/p1KlTUl+jLYI1co9GidQuJMRt1GIUhWqJRIK51rJIumVqxd8LLriAG2+8kVWrVrF79+6GEfe8efOorq5m5cqVFBYW0q9fv4TL/MZKNKp/5513uPfee1m+fDmHHnooV1xxRav7aWlNrY4dOzY8DoVCSZdlmtvnsccey8qVK1mwYAHTpk3jzDPP5LbbbuPll19m4cKFzJ8/n1//+te88MILSX2dZAVr5B6JEC5cwVn8DTDuv+G/VZIRaUepWPG3S5cuRCIRvvvd7zY6kbp9+3YOO+wwCgsLWbRoEZs3b25xP6eddhrz5s0DYO3ataxZswbwlgvu3Lkz3bt358MPP+T5559v+JyuXbuyY8eOhPt6+umn2bVrF5999hlPPfUUp5566kF9n83tc8uWLXTq1IlJkyZx0003sWrVKnbu3Mn27ds599xzmTlzZsN9ZttTsEbu4TCVkx7g73POAuCG/ziGIReo5i7SnlKx4u/EiRO58MILG82cufzyyzn//PMZMWIEw4YN42tf+1qL+7j22mu58sorGTJkCMOGDWPkyJEADB06lOHDhzNo0KAmywWXl5dzzjnn0KdPHxYtWtTQXlZWxhVXXNGwj6uvvprhw4cnXYIBuOOOOxpOmoJ3K79E+6yoqODHP/4xBQUFFBYW8uCDD7Jjxw7Gjx/Pnj17cM5x//33J/11kxW4JX/vPDvKTypOxREiFPJGGNOmpSBAkRygJX+DLX+W/K2sJPLCzyjEu7VWYahOFzGJiCQQrOQejRLev4RfcCsA5w54M8MBiYhkp2Ald/8mqp35DICn139VN+wQaUWmSq9ycA725xas5B4Ow7x5vIFXg6pzpsXDRFpQXFxMTU2NEnzAOOeoqamhuLj4gPcRrNkyAJ06MZp/MosfUcB+ijpAJBLKdFQiWalv375UVVVRXV2d6VCkjYqLi+nbt+8Bf37wkvvy5USIAjCQN7nxnHcIh8/NbEwiWaqwsJD+/ftnOgzJgGCVZQDGjuV1SgF4k68y5fmzVHMXEYkTvOR+0kksKToDcDgK2FsbUs1dRCRO8JI7EOm83H/kCIV0ww4RkXhJJXczO9vMNpjZRjObmuD1L5vZIjN7xczWmFnqiuCVlfDxxxje2X9zdSn7UiIiQdVqcjezEPAAcA5QCkw0s9K4bj8FHnfODQcmAP+3vQNtEI0SZTQOA4zaWi37KyISL5mR+0hgo3PubefcXmA+MD6ujwO6+Y+7A1vaL8Q4kQgRW0wBdYDTfVRFRBJIJrkfCbwb87zKb4t1OzDJzKqABcAPE+3IzMrNbIWZrTjgebfhMOHjdzLEXqNL0T5mzgppVUgRkTjJJPdE97yKv9xtIvD/nHN9gXOBP5hZk3075x52zo1wzo3o3bt326MFqKykcm1XXnOD2bm3kCnX79dUSBGROMkk9yrgqJjnfWladrkKeBzAOVcJFAO9SIVolGjdadRRAGj5ARGRRJJJ7suBgWbW38yK8E6YPhvX57+BsQBmdhxeck/N9c6RCJHQiw0191AH1dxFROK1mtydc7XAdUAFsB5vVszrZjbdzMb53f438D0zexV4DLjCpWqlonAYRp3oT4U0rEDryoiIxEtqbRnn3AK8E6WxbbfFPF4HnBz/eSlRWUm0spj9/t+l2n2OaNR0UlVEJEbwrlCNRonUvdBwNyaoo6QkoxGJiGSd4CX3SIRw0Uqu4z8AqKsrYMoU3bBDRCRW8JJ7OAzf/z77KATAYez9XFepiojECl5yB9i3jzEsAsCoI2T7NWNGRCRGMJP7qafSjU8B72oqKwjmtyEikirBzIqRCMs4ES+1F1BbV6CyjIhIjGAm9zfeYIx/qz2oI1SgsoyISKxgJnd/akzDhUx1urO7iEisYCb3MWOIEvliTXenW+2JiMQKZnIHIvwTq19fJlSnsoyISIxgJvdoFPxxO6CyjIhInGAm90iEqI1RWUZEpBnBTO7hMJEB7/nL/kKog6ksIyISI5jJvbIS/vUvf7YM4OoyG4+ISJYJZnJvdDcm2F+ruzGJiMQKZnKPRIgULKYD+7znhpb9FRGJEczkDoQLlnED9wFQ50zL/oqIxAhmco9GYf9+aikCwDnj889VmhERqRfM5B6JQGEhX2az3+Coq1NpRkSkXjCTezgMU6awky74i/5SUOCoqcl0YCIi2SGpG2RnpX37GEMlXnJ3dLA6IpHgfjsiIu0pmCN3gDPOAPCXIDDdsENEJEZwM2LXrv7KkADGvlrTCVUREV9wk/uSJZSwFW/s7qhzOqEqIlIvuEXqkhJq+JT6W+0VWB01NdbaZ4mI5IXgjtxraogQJcR+ADoUaE13EZF6wU3ukQhgDWu6oxOqIiINAp0RoxZpWDxsX63x6KMZDkhEJEsEN7lHo0RclBC1ADgHc+ZofRkREQhyco9ECIde5mL+5DcY+/ZpfRkREQhycgcwYxRL/SdaX0ZEpF5wk3s0CnV17KAr9evLmKH1ZURECHJyj0QgFKIXW/0Gh3NOI3cREYKc3MNhuPBCaujlNxiGVoYUEYEgJ3eAE0/0lyAAcDhMI3cREYKe3Hfv9kfufs1dI3cRESDJ5G5mZ5vZBjPbaGZTE7x+v5mt9rc3zeyT9g81gTFjYhYPQyN3ERFfqwuHmVkIeAD4BlAFLDezZ51z6+r7OOduiOn/Q2B4CmJNqIbeGHU4Qhjwyivp+soiItkrmZH7SGCjc+5t59xeYD4wvoX+E4HH2iO4VkWjRFhEh/qrVNFVqiIikFxyPxJ4N+Z5ld/WhJl9BegPvHDwoSWhpIQwS/kWT9RHoKtURURILrknWiTdNdN3AvCEc25/wh2ZlZvZCjNbUV1dnWyMzfPPnp5E/VBdV6mKiEByyb0KOCrmeV9gSzN9J9BCScY597BzboRzbkTv3r2Tj7I5fhbfqatURUQaSSa5LwcGmll/MyvCS+DPxncys68ChwLpq3jX1IBZ47nuut2eiEjryd05VwtcB1QA64HHnXOvm9l0MxsX03UiMN8511zJpv1FItChQ+OrVE1z3UVEkrqHqnNuAbAgru22uOe3t19YSQqH4YILKPmTRu4iIrGCfYUqwEkn6SpVEZE4wU/uu3Y1uUr1k/RcHysikrWCn9x79fJH7nUNTfffrwuZRCS/BT+519QQIUqHhuRu1NbqQiYRyW/BT+7+VapTuA+v7q6TqiIiwU/u/tnTntQX2nUhk4hI8JO7P0TXhUwiIl8IfnL3r1J9hTK/wZs1o6V/RSSfBT+5+1epNpa+i2RFRLJR8JN7OAwTJjCcVY2au3XLUDwiIlkg+MkdYq5S1Vx3ERHIleT+6qua6y4iEiM3kjsQZik38H/QXHcRkVxJ7sO9+3HvoLvfoBkzIpLfciO564olEZFGciO5+/WXL2bMeFMh/QG9iEjeyY3k7o/cG1/I5FSWEZG8lRvJPeGFTPDBB+kPRUQkG+RGcg+H4YYbmMyjdGBfQ/Nf/6q57iKSn3IjuQPs2EGYpZzHAupvubdvHzz6aKYDExFJv9xJ7j6LuUoVVJoRkfyUO8ndnxpzOB82aj788EwEIyKSWbmT3P2pMZoOKSKSS8ndF7+u+/PPZy4WEZFMyZ3kPnlywnXdn31WM2ZEJP/kTnIPh+G885jMoxSwn/oZM3V1mjEjIvknd5I7gBlhlnIKSxo1a8aMiOSb3Eru/tSYUtYnahYRyRu5ldz9qTGaMSMi+S63krs/HVIzZkQk3+VWck9IM2ZEJP/kVnKfPBkKCjRjRkTyXm4l93AYTjlFM2ZEJO/lVnIHKC0FoCfbMhyIiEjm5F5yb2ZqzDblehHJI7mX3P0ZM/GrQy5ZopOqIpI/kkruZna2mW0ws41mNrWZPpeY2Toze93M/rN9w2y7xidV0UlVEckrrSZ3MwsBDwDnAKXARDMrjeszEJgGnOycGwRMSUGsyfFnzOikqojks2RG7iOBjc65t51ze4H5wPi4Pt8DHnDOfQzgnPuofcNsA3/GDOikqojkr2SS+5HAuzHPq/y2WMcCx5rZf5nZUjM7u70CPCA9eyZs1klVEckXySR3S9Dm4p53AAYCEWAi8IiZ9WiyI7NyM1thZiuqq6vbGmub6aSqiOSrZJJ7FXBUzPO+wJYEfZ5xzu1zzr0DbMBL9o045x52zo1wzo3o3bv3gcbcOn8ZSJ1UFZF8lUxyXw4MNLP+ZlYETACejevzNDAGwMx64ZVp3m7PQNvEvyuTTqqKSL5qNbk752qB64AKYD3wuHPudTObbmbj/G4VQI2ZrQMWAT92ztWkKuhW+XdlSmTTpvSGIiKSCfE3HU3IObcAWBDXdlvMYwfc6G/ZwbxTBXsobtT86qte3T0czkRQIiLpkXtXqMa5it/6j7y6u3Oqu4tI7sv55F7OIwzjlUZt69ZlKBgRkTTJ3eQec+PUfmxq9NKLL2pKpIjkttxN7pMnNzyMn++u0oyI5LrcTe7hMAwbBnjz3S1mvjtoSqSI5LbcTe4A/foBEGYpQ1nT6CVNiRSRXJbbyT2m7l7E3kYvrV6turuI5K7cTu7+8r/QdEokwN13ZyAmEZE0yO3kHrP8bzmPcHjckjivvJLok0REgi+3kzs0Wv73WN5q9NLmzSrNiEhuyv3kHqOU9U3aVJoRkVyU+8k95qTqZB4F6oitu6s0IyK5KPeT++TJDYuIhVnKMF5t9LJKMyKSi3I/uYfDcOqpDU9HsaxJF5VmRCTX5H5yBygtbXiYqDSzeHH6QxIRSaX8SO4x68yEWUo/Njd6eds2ePjhdAclIpI6+ZHcY9aZAZjGnU26/PKX6QxIRCS18iO5Q8M6M+Bd0NSTrcSWZnRiVURySf4k95gpkQCn8WKTLjqxKiK5In+Se0zdHeBm7iF25A6wcGEa4xERSaH8Se7hcKPSTJil9Ct4t1GXHTvgllvSHJeISArkT3KHRidVAabV3UH86H3WrDTGIyKSIvmV3G++udHTch6hZ8HHjdr27IFJk9IZlIhI+8uv5B4ONzmxemfx9Cbd5s3TzBkRCbb8Su4Ao0Y1elq+61f07Ly7SbdLLklXQCIi7S//kntcaQbgzo4/b9JWVaXyjIgEV/4l9wSlmfJtMxjQd1eTrvPmaVkCEQmm/Evu0KQ0A/Do0U1r7wDXXKP6u4gET34m9wSlmfDa3yRqBuD005XgRSRY8jO5x13QBMC2bcw45mFGjmzafc8eOOkkXeAkIsGRn8kdYNq0pm2//CXLlsFXvpL4U+6+G846K7VhiYi0h/xN7uXl0LNn4zZ/achNm5pP8H//e9NBv4hItsnf5A5w2mlN26ZOBWgxwW/eDKGQyjQikr3yO7knOoO6eHHD2dNNm+C44xJ/al2dV6YpLFSSF5Hsk9/JPdGJVWgYvQOsWwdnntn8LmprvSRvBgMHalZNJk2aBB06eD8LbQe+deqkAUsuyO/kDolPrMaM3gEqKmD2bK8U05KNG71ZNXqDJKeyEo49tv2S0rx5sH9/pr+r4Nu9+4sBS75uoRD06RPwixidc61uwNnABmAjMDXB61cA1cBqf7u6tX2ecMIJLmscfrhz0HgbODBh1+OOa9o12c3MuQEDnHvppTR/fwdo9mznevY88O9XmzZtTbeDzQPACueSyNutdoAQ8C/gaKAIeBUojetzBfDrZL5g/ZZVyX327MQ/hdmzm+3eqVPmf0m0adMW3K2g4MASfLLJPZmyzEhgo3PubefcXmA+ML5d/m3IFuXlMGBA0/abbmq2+2efeaWa+NmUIiLJqKuDaDR1+08muR8JxN6Prspvi/ctM1tjZk+Y2VHtEl06Pfpo07YdO1q8aqm8HGpqvL/Dl1/eek1eUisUgqFD4aWXMj0mC+6mAUv6FBRAJJLC/SfRxxK0ubjnfwH6OeeGAP8Afp9wR2blZrbCzFZUV1e3LdJUC4e9DB3v739P6szo3LnezBnnvOQycGAKYsxBHTp4h709ElNtLaxe7f0o5cDEDljyebv5ZiguTs0xNvMKBUuWpPZ31bwSTkuBWBi43Tl3lv98GoBz7s5m+oeAbc657i3td8SIEW7FihUHFHRKde8On37atH32bO83/yBMmgTz5wdrRschh8APfwgzZmQ6EhEBMLOVzrkRrfVLZuS+HBhoZv3NrAiYADwb98X6xDwdB6xvS7BZ5Z57Ere3w9q/saP7oGy7dimxiwRRq8ndOVcLXAdU4CXtx51zr5vZdDMb53e73sxeN7NXgevxZs8EU3l581cttXQ1k4hIFmm1LJMqWVuWqXfiifDyy03bDzkEFi5UYVdEMqI9yzL5admyxAvL7N7tXYYa6EvXRCTXKbm3ZN26JvdbbXDNNVpfQESylpJ7a95/v/mJv3ff7ZVvRESyjJJ7Mmpqmh/Bv/wydOyoMo2IZBUl92S9/37zd+/Yu9cr0xx2mNb8FZGsoOTeFi3dvQOguto72dq9u0byIpJRSu5ttW5d4mUKYn36qTeSLyjQHbVFJCOU3A/E3LneAjI9erTczzlvbZr61f+V6EUkTZTcD1Q4DB9/7K0wVJDEYayr+yLRK9mLSIopuR+sGTO8lcBaK9XEi0/2Zt4fia5dNX9eRA6aknt7mTv3i7VCO3c+sH04Bzt3Nn8Dy5ISnagVkaQoube3GTO8BO2ct9CYJVoO/wBt2+adqG3urr6Fhd66wiKS95TcU6miwiu/pCLRJ1JbC/PmtX5r906dVPoRyXFK7ukSm+jTleybs3t386Wf+K2gwLutlC7OEgkUJfdMiU/2zmXnDSydg40bvYuzkvljkGjr2FHlIpE0U3LPJi3dwDKVN3VMtb17kysXHcym8w0ijSi5B8WMGV45pbn74eX7XbmTPd9wsJvKVBIQSu65IhyGN99s/aaos2d7K1wmc+GVNNUeZap0bKEQ9OmjqbN5TO/wfFNe7q1wuX9/cnfIvvxyL1FIsNTVwQcftDx1Nt+3jh1h9Oic/S9MyV1aNneuV/JI5g+BykUSJHv3wuLF6f8vLE2lPSV3Sa1ky0UHs730EgwbplKTBEN9ae+UU1Ka4PVukOALh+GVV5IvNR3Mdvnl3swckYNVVwfRaMp2r+Qu0hZz53r/zqf6j8jBbkGeOpsvCgogEknd7lO2ZxHJnNamzub7lslZY2YwYAAsWeL915kiHVK2ZxGRbFVe7m05TCN3EZEcpOQuIpKDlNxFRHKQkruISA5SchcRyUFK7iIiOcicc5n5wmbVwOYD/PRewNZ2DCcVFOPBy/b4IPtjzPb4QDG21Vecc71b65Sx5H4wzGyFc25EpuNoiWI8eNkeH2R/jNkeHyjGVFFZRkQkBym5i4jkoKAm9yDcXkYxHrxsjw+yP8Zsjw8UY0oEsuYuIiItC+rIXUREWhC45G5mZ5vZBjPbaGZTMxTDUWa2yMzWm9nrZvYjv72nmf1/M3vL/3io325mNsuPeY2ZlaUx1pCZvWJmz/nP+5vZMj/GP5pZkd/e0X++0X+9Xxpi62FmT5jZG/6xDGfbMTSzG/yf8Voze8zMijN9DM3sd2b2kZmtjWlr83Ezs+/4/d8ys++kOL57/J/zGjN7ysx6xLw2zY9vg5mdFdOesvd6ohhjXrvJzJyZ9fKfp/0YtgvnXGA2IAT8CzgaKAJeBUozEEcfoMx/3BV4EygF7gam+u1TgRn+43OB5wEDRgHL0hjrjcB/As/5zx8HJviPHwKu9R//AHjIfzwB+GMaYvs9cLX/uAjokU3HEDgSeAdH01fyAAADz0lEQVQ4JObYXZHpYwicBpQBa2Pa2nTcgJ7A2/7HQ/3Hh6YwvjOBDv7jGTHxlfrv445Af//9HUr1ez1RjH77UUAF3jU4vTJ1DNvle8x0AG38gYSBipjn04BpWRDXM8A3gA1AH7+tD7DBfzwbmBjTv6FfiuPqCywETgee8385t8a8yRqOp/8LHfYfd/D7WQpj6+YnTotrz5pjiJfc3/XfvB38Y3hWNhxDoF9c8mzTcQMmArNj2hv1a+/44l77JjDPf9zoPVx/DNPxXk8UI/AEMBTYxBfJPSPH8GC3oJVl6t9s9ar8tozx//UeDiwDvuScex/A/3iY3y1Tcc8Ebgbq/OclwCfOudoEcTTE6L++3e+fKkcD1cAcv2z0iJl1JouOoXPuPeBe4L+B9/GOyUqy5xjGautxy+R76bt4I2FaiCPt8ZnZOOA959yrcS9lTYxtEbTkbgnaMjbdx8y6AE8CU5xzn7bUNUFbSuM2s38DPnLOrUwyjnTH2AHv3+IHnXPDgc/wygnNycQxPBQYj1cuOALoDJzTQhxZ9fvpay6mjMRqZj8BaoF59U3NxJHW+MysE/AT4LZELzcTSzb+vBsELblX4dXE6vUFtmQiEDMrxEvs85xzf/abPzSzPv7rfYCP/PZMxH0yMM7MNgHz8UozM4EeZlZ/e8XYOBpi9F/vDmxLYXxVQJVzbpn//Am8ZJ9Nx/AM4B3nXLVzbh/wZ+AksucYxmrrcUv78fRPOP4bcLnz6xhZFN8xeH/EX/XfM32BVWZ2eBbF2CZBS+7LgYH+bIUivJNWz6Y7CDMz4LfAeufcfTEvPQvUnzH/Dl4tvr59sn/WfRSwvf5f6FRxzk1zzvV1zvXDO04vOOcuBxYBFzUTY33sF/n9UzYKcc59ALxrZl/1m8YC68iiY4hXjhllZp38n3l9jFlxDOO09bhVAGea2aH+fyhn+m0pYWZnA7cA45xzu+LinuDPNOoPDAReJs3vdefca865w5xz/fz3TBXepIkPyJJj2GaZLvofwEmQc/Fmp/wL+EmGYjgF79+vNcBqfzsXr766EHjL/9jT72/AA37MrwEj0hxvhC9myxyN9+bZCPwJ6Oi3F/vPN/qvH52GuIYBK/zj+DTejIOsOobAz4E3gLXAH/BmdWT0GAKP4Z0D2IeXhK46kOOGV/ve6G9Xpji+jXj16fr3y0Mx/X/ix7cBOCemPWXv9UQxxr2+iS9OqKb9GLbHpitURURyUNDKMiIikgQldxGRHKTkLiKSg5TcRURykJK7iEgOUnIXEclBSu4iIjlIyV1EJAf9D5jId+g4ePH/AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Type your code here to plot the loss accuracy and ROC curve\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(run_hist_1.history[\"loss\"],'r', marker='.', label=\"Train Loss\")\n",
    "ax.plot(run_hist_1.history[\"val_loss\"],'b', marker='.', label=\"Validation Loss\")\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "# Print model performance and plot the roc curve\n",
    "print('accuracy is {:.3f}'.format(accuracy_score(y_test,y_pred_class_nn_1)))\n",
    "print('roc-auc is {:.3f}'.format(roc_auc_score(y_test,y_pred_prob_nn_1)))\n",
    "\n",
    "plot_roc(y_test, y_pred_prob_nn_1, 'NN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py3 OpenCV3 (Forge)",
   "language": "python",
   "name": "opencv-forge"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
